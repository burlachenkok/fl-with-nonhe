#!/usr/bin/env bash

# 5.1 Synthetic Experiments (CASE 1 and CASE 2 from paper)

python run.py --rounds "3000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "1" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp64" --gpu "0" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "456" --group-name "" --comment "" --hostname "kw60797" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "2_job_id_1683134845" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,stepsize_multiplier:1.0,th_stepsize_cvx" --logfile "../logs/2_log_1683134845.txt" --client-compressor "randk:20%" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "fl_pytorch_simulation" --loglevel "debug" --logfilter ".*" --out "2_job_id_1683134845.bin"

python run.py --rounds "3000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "1" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp32" --gpu "0" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "456" --group-name "" --comment "" --hostname "kw60797" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "3_job_id_1683136146" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,stepsize_multiplier:1.0,th_stepsize_cvx" --logfile "../logs/3_log_1683136146.txt" --client-compressor "randk:20%" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "fl_pytorch_simulation" --loglevel "debug" --logfilter ".*" --out "3_job_id_1683136146.bin"

python run.py --rounds "3000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "1" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp16" --gpu "0" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "456" --group-name "" --comment "" --hostname "kw60797" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "4_job_id_1683137466" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,stepsize_multiplier:1.0,th_stepsize_cvx" --logfile "../logs/4_log_1683137466.txt" --client-compressor "randk:20%" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "fl_pytorch_simulation" --loglevel "debug" --logfilter ".*" --out "4_job_id_1683137466.bin"

python run.py --rounds "3000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "1" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp64" --gpu "0" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "456" --group-name "" --comment "" --hostname "kw60797" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "5_job_id_1683138783" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,stepsize_multiplier:1.0,th_stepsize_cvx,aes:1" --logfile "../logs/5_log_1683138783.txt" --client-compressor "randk:20%" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "fl_pytorch_simulation" --loglevel "debug" --logfilter ".*" --out "5_job_id_1683138783.bin"

python run.py --rounds "3000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "1" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp32" --gpu "0" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "456" --group-name "" --comment "" --hostname "kw60797" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "6_job_id_1683140866" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,stepsize_multiplier:1.0,th_stepsize_cvx,aes:1" --logfile "../logs/6_log_1683140866.txt" --client-compressor "randk:20%" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "fl_pytorch_simulation" --loglevel "debug" --logfilter ".*" --out "6_job_id_1683140866.bin"

python run.py --rounds "3000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "1" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp16" --gpu "0" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "456" --group-name "" --comment "" --hostname "kw60797" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "7_job_id_1683142948" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,stepsize_multiplier:1.0,th_stepsize_cvx,aes:1" --logfile "../logs/7_log_1683142948.txt" --client-compressor "randk:20%" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "fl_pytorch_simulation" --loglevel "debug" --logfilter ".*" --out "7_job_id_1683142948.bin"

python run.py --rounds "3000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "1" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp16" --gpu "0" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "456" --group-name "" --comment "" --hostname "kw60797" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "8_job_id_1683145039" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,stepsize_multiplier:1.0,th_stepsize_cvx,he:1" --logfile "../logs/8_log_1683145039.txt" --client-compressor "randk:20%" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "fl_pytorch_simulation" --loglevel "debug" --logfilter ".*" --out "8_job_id_1683145039.bin"

python run.py --rounds "3000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "1" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp32" --gpu "0" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "456" --group-name "" --comment "" --hostname "kw60797" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "9_job_id_1683152065" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,stepsize_multiplier:1.0,th_stepsize_cvx,he:1" --logfile "../logs/9_log_1683152065.txt" --client-compressor "randk:20%" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "fl_pytorch_simulation" --loglevel "debug" --logfilter ".*" --out "9_job_id_1683152065.bin"

python run.py --rounds "3000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "1" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp64" --gpu "0" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "456" --group-name "" --comment "" --hostname "kw60797" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "10_job_id_1683159117" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,stepsize_multiplier:1.0,th_stepsize_cvx,he:1" --logfile "../logs/10_log_1683159117.txt" --client-compressor "randk:20%" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "fl_pytorch_simulation" --loglevel "debug" --logfilter ".*" --out "10_job_id_1683159117.bin"

python run.py --rounds "3000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "1" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp64" --gpu "0" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "456" --group-name "" --comment "" --hostname "kw60797" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "11_job_id_1683166184" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,stepsize_multiplier:1.0,th_stepsize_cvx" --logfile "../logs/11_log_1683166184.txt" --client-compressor "ident" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "fl_pytorch_simulation" --loglevel "debug" --logfilter ".*" --out "11_job_id_1683166184.bin"

python run.py --rounds "3000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "1" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp32" --gpu "0" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "456" --group-name "" --comment "" --hostname "kw60797" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "12_job_id_1683167460" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,stepsize_multiplier:1.0,th_stepsize_cvx" --logfile "../logs/12_log_1683167460.txt" --client-compressor "ident" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "fl_pytorch_simulation" --loglevel "debug" --logfilter ".*" --out "12_job_id_1683167460.bin"

python run.py --rounds "3000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "1" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp16" --gpu "0" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "456" --group-name "" --comment "" --hostname "kw60797" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "13_job_id_1683168742" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,stepsize_multiplier:1.0,th_stepsize_cvx" --logfile "../logs/13_log_1683168742.txt" --client-compressor "ident" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "fl_pytorch_simulation" --loglevel "debug" --logfilter ".*" --out "13_job_id_1683168742.bin"

python run.py --rounds "3000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "1" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp16" --gpu "0" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "456" --group-name "" --comment "" --hostname "kw60797" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "14_job_id_1683170027" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,stepsize_multiplier:1.0,th_stepsize_cvx,aes:1" --logfile "../logs/14_log_1683170027.txt" --client-compressor "ident" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "fl_pytorch_simulation" --loglevel "debug" --logfilter ".*" --out "14_job_id_1683170027.bin"

python run.py --rounds "3000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "1" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp32" --gpu "0" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "456" --group-name "" --comment "" --hostname "kw60797" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "15_job_id_1683172191" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,stepsize_multiplier:1.0,th_stepsize_cvx,aes:1" --logfile "../logs/15_log_1683172191.txt" --client-compressor "ident" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "fl_pytorch_simulation" --loglevel "debug" --logfilter ".*" --out "15_job_id_1683172191.bin"

python run.py --rounds "3000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "1" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp64" --gpu "0" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "456" --group-name "" --comment "" --hostname "kw60797" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "16_job_id_1683174351" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,stepsize_multiplier:1.0,th_stepsize_cvx,aes:1" --logfile "../logs/16_log_1683174351.txt" --client-compressor "ident" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "fl_pytorch_simulation" --loglevel "debug" --logfilter ".*" --out "16_job_id_1683174351.bin"

python run.py --rounds "3000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "1" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp64" --gpu "0" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "456" --group-name "" --comment "" --hostname "kw60797" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "17_job_id_1683176514" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,stepsize_multiplier:1.0,th_stepsize_cvx,he:1" --logfile "../logs/17_log_1683176514.txt" --client-compressor "ident" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "fl_pytorch_simulation" --loglevel "debug" --logfilter ".*" --out "17_job_id_1683176514.bin"

python run.py --rounds "3000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "1" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp32" --gpu "0" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "456" --group-name "" --comment "" --hostname "kw60797" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "18_job_id_1683183720" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,stepsize_multiplier:1.0,th_stepsize_cvx,he:1" --logfile "../logs/18_log_1683183720.txt" --client-compressor "ident" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "fl_pytorch_simulation" --loglevel "debug" --logfilter ".*" --out "18_job_id_1683183720.bin"

python run.py --rounds "3000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "1" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp16" --gpu "0" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "456" --group-name "" --comment "" --hostname "kw60797" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "19_job_id_1683190941" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,stepsize_multiplier:1.0,th_stepsize_cvx,he:1" --logfile "../logs/19_log_1683190941.txt" --client-compressor "ident" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "fl_pytorch_simulation" --loglevel "debug" --logfilter ".*" --out "19_job_id_1683190941.bin"
