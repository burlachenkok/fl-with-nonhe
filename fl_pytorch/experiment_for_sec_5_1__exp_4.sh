#!/usr/bin/env bash

# 5.1 Synthetic Experiments (comparison of DCGD/PermK/AES and GD/CKKS)

# Command line for experiment with job_id=DCGD [fp64], AES-128, PermK, gamma=0.005
python run.py --rounds "30000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "0.005" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp64" --gpu "-1" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "9700" --group-name "0.005 group" --comment "" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "zxxxv_current_0.005_launch_5" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,__stepsize_multiplier__:1.0,___th_stepsize_cvx__,aes:1" --logfile "../logs/1_log_1683248554.txt" --client-compressor "permk" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "" --loglevel "debug" --logfilter ".*" --out "qqcurrent_0.005_launch_5.bin" --out "x1_dgd [fp64] permk, gamma=0.005.bin" --out "DCGD [fp64], AES-128, PermK, gamma=0.005.bin"

# Command line for experiment with job_id=DCGD [fp64], AES-128, PermK, gamma=0.007
python run.py --rounds "30000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "0.007" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp64" --gpu "-1" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "9117" --group-name "0.007 group" --comment "" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "zxcxxa_current_0.007_launch_5" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,__stepsize_multiplier__:1.0,___th_stepsize_cvx__,aes:1" --logfile "../logs/1_log_1683248318.txt" --client-compressor "permk" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "" --loglevel "debug" --logfilter ".*" --out "qq2current_0.007_launch_5.bin" --out "x2_dgd [fp64] permk, gamma=0.007.bin" --out "DCGD [fp64], AES-128, PermK, gamma=0.007.bin"

# Command line for experiment with job_id=GD [fp64] AES-128
python run.py --rounds "3000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "1" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp64" --gpu "0" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "456" --group-name "" --comment "" --hostname "kw60797" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "16_job_id_1683174351" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,stepsize_multiplier:1.0,th_stepsize_cvx,aes:1" --logfile "../logs/16_log_1683174351.txt" --client-compressor "ident" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "fl_pytorch_simulation" --loglevel "debug" --logfilter ".*" --out "GD [fp64] AES-128.bin"

# Command line for experiment with job_id=DCGD [fp64] PermK, gamma=0.005
python run.py --rounds "30000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "0.005" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp64" --gpu "-1" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "9700" --group-name "0.005 group" --comment "" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "current_0.005_launch_5" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,__stepsize_multiplier__:1.0,___th_stepsize_cvx__" --logfile "../logs/1_log_1683248554.txt" --client-compressor "permk" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "" --loglevel "debug" --logfilter ".*" --out "current_0.005_launch_5.bin" --out "DCGD [fp64] PermK, gamma=0.005.bin"

# Command line for experiment with job_id=DCGD [fp64] PermK, gamma=0.007
python run.py --rounds "30000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "0.007" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp64" --gpu "-1" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "9117" --group-name "0.007 group" --comment "" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "current_0.007_launch_5" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,__stepsize_multiplier__:1.0,___th_stepsize_cvx__" --logfile "../logs/1_log_1683248318.txt" --client-compressor "permk" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "" --loglevel "debug" --logfilter ".*" --out "current_0.007_launch_5.bin" --out "DCGD [fp64] PermK, gamma=0.007.bin"

# Command line for experiment with job_id=GD [fp64]
python run.py --rounds "3000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "1" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp64" --gpu "0" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "456" --group-name "" --comment "" --hostname "kw60797" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "11_job_id_1683166184" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,stepsize_multiplier:1.0,th_stepsize_cvx" --logfile "../logs/11_log_1683166184.txt" --client-compressor "ident" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "fl_pytorch_simulation" --loglevel "debug" --logfilter ".*" --out "GD [fp64].bin"

# Command line for experiment with job_id=GD [fp64] HE/CKKS
python run.py --rounds "3000" --client-sampling-type "uniform" --num-clients-per-round "50" --global-lr "1" --global-optimiser "sgd" --global-weight-decay "0.0" --number-of-local-iters "1" --batch-size "24" --local-lr "1" --local-optimiser "sgd" --local-weight-decay "0.0" --dataset "generated_for_quadratic_minimization" --dataset-generation-spec "homogeneous:0,mu:1.0,l:10.0,samples_per_client:12,clients:50,variables:1000" --loss "mse" --model "linear" --metric "loss" --global-regulizer "none" --global-regulizer-alpha "0.0" --checkpoint-dir "../check_points" --do-not-save-eval-checkpoints --data-path "../data/" --compute-type "fp64" --gpu "0" --num-workers-train "0" --num-workers-test "0" --deterministic --manual-init-seed "123" --manual-runtime-seed "456" --group-name "" --comment "" --hostname "kw60797" --eval-every "100" --eval-async-threads "0" --save-async-threads "0" --threadpool-for-local-opt "50" --run-id "17_job_id_1683176514" --algorithm "dcgd" --algorithm-options "internal_sgd:full-gradient,stepsize_multiplier:1.0,th_stepsize_cvx,he:1" --logfile "../logs/17_log_1683176514.txt" --client-compressor "ident" --extra-track "full_gradient_norm_train,full_objective_value_train" --initialize-shifts-policy "zero" --wandb-key "" --wandb-project-name "fl_pytorch_simulation" --loglevel "debug" --logfilter ".*" --out "GD [fp64] HE/CKKS.bin"
