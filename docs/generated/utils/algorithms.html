<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>utils.algorithms API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>utils.algorithms</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python3

import random
import time
import copy
import math

# Import PyTorch root package import torch                        
import torch

import numpy as np

from utils import execution_context
from utils import  model_funcs
from utils import  compressors
from models import mutils

import utils
import argparse
from utils.logger import Logger

# ======================================================================================================================
def evaluateGradient(client_state, model, dataloader, criterion, is_rnn, update_statistics = True, evaluate_function = False, device = None, args = None):
    &#34;&#34;&#34;
    Evalute gradient for model at current point and optionally update statistics and return loss value at current point.

    Args:
        client_state(dict): information about client. used information - statistics, used device
        model(torch.nn.Module): used model for which trainable variables we will evaluate full gradient
        dataloader: used dataloader for fetch records for evaluate local loss during training
        criterion: used criteria with setuped reduction as sum. After evalute reduction correct scaling is a part of evaluation
        is_rnn(bool): flag which specofy that what we evaluate is rnn
        update_statistics(bool): update the following statistics - full_gradient_oracles, samples_gradient_oracles, dataload_duration, inference_duration, backprop_duration
        evaluate_function(bool): if true then returned value is CPU scalar which describes loss function value

    Returns:
        If evaluate_function is True then sclar with local ERM value
    &#34;&#34;&#34;
    model.train(True)
    if model.do_not_use_bn_and_dropout:
        mutils.turn_off_batch_normalization_and_dropout(model)

    if update_statistics:
        client_state[&#39;stats&#39;][&#39;full_gradient_oracles&#39;] += 1
        client_state[&#39;stats&#39;][&#39;samples_gradient_oracles&#39;] += len(dataloader.dataset)

    # Zero out previous gradient
    for p in model.parameters():
        p.grad = None

    if device is None:
        device = client_state[&#34;device&#34;]

    total_number_of_samples = len(dataloader.dataset)
    function_value = None

    if evaluate_function:
        function_value = torch.Tensor([0.0]).to(device = device, dtype = model.fl_dtype)

    for i, (data, label) in enumerate(dataloader):
        start_ts = time.time()
        batch_size = data.shape[0]
        if str(data.device) != device or str(label.device) != device or data.dtype != model.fl_dtype or label.dtype != model.fl_dtype:
            data, label = data.to(device = device, dtype = model.fl_dtype), label.to(device = device, dtype = model.fl_dtype)

        if update_statistics:
            client_state[&#34;stats&#34;][&#34;dataload_duration&#34;] += (time.time() - start_ts)

        input, label = model_funcs.get_train_inputs(data, label, model, batch_size, device, is_rnn)

        start_ts = time.time()
        outputs = model(*input)

        if not is_rnn:
            hidden = None
            output = outputs
        else:
            output, hidden = outputs

        loss = model_funcs.compute_loss(model, criterion, output, label)
        loss = loss * (1.0/total_number_of_samples)

        if evaluate_function:
            function_value += loss

        if update_statistics:
            client_state[&#34;stats&#34;][&#34;inference_duration&#34;] += (time.time() - start_ts)

        start_ts = time.time()
        loss.backward()

        if update_statistics:
            client_state[&#34;stats&#34;][&#34;backprop_duration&#34;] += (time.time() - start_ts)

    if args is None:
        args = client_state[&#34;H&#34;][&#34;args&#34;]

    regulizer_global = model_funcs.get_global_regulizer(args.global_regulizer)
    R = regulizer_global(model, args.global_regulizer_alpha)

    Rvalue = 0.0
    if R is not None:
        R.backward()
        Rvalue = R.item()

    if evaluate_function:
        return function_value.item() + Rvalue
    else:
        return None
# ======================================================================================================================
def evaluateSgd(client_state, model, dataloader, criterion, is_rnn, update_statistics = True, evaluate_function = False, device = None, args = None):
    &#34;&#34;&#34;
    Evalute gradient estimator with using global context for model at current point and optionally update statistics and return loss value at current point.

    Args:
        client_state(dict): information about client. used information - statistics, used device
        model(torch.nn.Module): used model for which trainable variables we will evaluate full gradient
        dataloader: used dataloader for fetch records for evaluate local loss during training
        criterion: used criteria with setuped reduction as sum. After evalute reduction correct scaling is a part of evaluation
        is_rnn(bool): flag which specofy that what we evaluate is rnn
        update_statistics(bool): update the following statistics - full_gradient_oracles, samples_gradient_oracles, dataload_duration, inference_duration, backprop_duration
        evaluate_function(bool): if true then returned value is CPU scalar which describes loss function value

    Returns:
        If evaluate_function is True then sclar with local ERM value
    &#34;&#34;&#34;
    exec_ctx = client_state[&#39;H&#39;][&#34;execution_context&#34;]

    if &#34;internal_sgd&#34; not in exec_ctx.experimental_options:
        return evaluateGradient(client_state, model, dataloader, criterion, is_rnn, update_statistics=update_statistics, evaluate_function=evaluate_function, device=device)

    internal_sgd = exec_ctx.experimental_options[&#39;internal_sgd&#39;]
    if internal_sgd == &#39;full-gradient&#39;:
        return evaluateGradient(client_state, model, dataloader, criterion, is_rnn, update_statistics=update_statistics, evaluate_function=evaluate_function, device=device)

    model.train(True)
    if model.do_not_use_bn_and_dropout:
        mutils.turn_off_batch_normalization_and_dropout(model)

    # Zero out previous gradient
    for p in model.parameters():
        p.grad = None

    if device is None:
        device = client_state[&#34;device&#34;]

    total_number_of_samples = len(dataloader.dataset)
    function_value = None

    if evaluate_function:
        function_value = torch.Tensor([0.0]).to(device = device, dtype = model.fl_dtype)

    # ==================================================================================================================
    indicies = None
    if internal_sgd == &#34;sgd-nice&#34; or internal_sgd == &#39;sgd-us&#39; or internal_sgd == &#39;iterated-minibatch&#39; or internal_sgd == &#39;sgd-multi&#39;:
        indicies = client_state[&#39;iterated-minibatch-indicies&#39;]
        indicies = torch.from_numpy(indicies)
    # ==================================================================================================================
    batch_size_ds = dataloader.batch_size
    iterations = math.ceil(len(indicies) / float(batch_size_ds))

    sampled_samples = len(indicies)

    for i in range(iterations):
        data = []
        label = []
        for j in range(batch_size_ds):
            index = i * batch_size_ds + j
            if index &gt;= sampled_samples:
                break

            d, t = dataloader.dataset[indicies[index]]

            data.append(d.unsqueeze(0))

            if not torch.is_tensor(t):
                if type(criterion) is torch.nn.MSELoss:
                    label.append(torch.Tensor([t]))
                else:
                    label.append(torch.LongTensor([t]))
            else:
                label.append(t.unsqueeze(0))

        data = torch.cat(data)
        label = torch.cat(label)

        start_ts = time.time()
        batch_size = data.shape[0]
        if str(data.device) != device or str(label.device) != device or data.dtype != model.fl_dtype or label.dtype != model.fl_dtype:
            data, label = data.to(device = device, dtype = model.fl_dtype), label.to(device = device, dtype = model.fl_dtype)

        if update_statistics:
            client_state[&#34;stats&#34;][&#34;dataload_duration&#34;] += (time.time() - start_ts)

        if update_statistics:
            client_state[&#39;stats&#39;][&#39;full_gradient_oracles&#39;] += float(batch_size) / total_number_of_samples
            client_state[&#39;stats&#39;][&#39;samples_gradient_oracles&#39;] += batch_size

        input, label = model_funcs.get_train_inputs(data, label, model, batch_size, device, is_rnn)

        start_ts = time.time()
        outputs = model(*input)

        if not is_rnn:
            hidden = None
            output = outputs
        else:
            output, hidden = outputs

        loss = model_funcs.compute_loss(model, criterion, output, label)
        loss = loss * (1.0/sampled_samples)

        if evaluate_function:
            function_value += loss

        if update_statistics:
            client_state[&#34;stats&#34;][&#34;inference_duration&#34;] += (time.time() - start_ts)

        start_ts = time.time()
        loss.backward()

        if update_statistics:
            client_state[&#34;stats&#34;][&#34;backprop_duration&#34;] += (time.time() - start_ts)

    if args is None:
        args = client_state[&#34;H&#34;][&#34;args&#34;]

    regulizer_global = model_funcs.get_global_regulizer(args.global_regulizer)
    R = regulizer_global(model, args.global_regulizer_alpha)

    Rvalue = 0.0
    if R is not None:
        R.backward()
        Rvalue = R.item()

    if evaluate_function:
        return function_value.item() + Rvalue
    else:
        return None
# ======================================================================================================================
def evaluateFunction(client_state, model, dataloader, criterion, is_rnn, update_statistics = True, device = None, args = None):
    &#34;&#34;&#34;
    Evalute gradient for model at current point and optionally update statistics and return loss value at current point.

    Args:
        client_state(dict): information about client. used information - statistics, used device
        model(torch.nn.Module): used model for which trainable variables we will evaluate full gradient
        dataloader: used dataloader for fetch records for evaluate local loss during training
        criterion: used criteria with setuped reduction as sum. After evalute reduction correct scaling is a part of evaluation
        is_rnn(bool): flag which specofy that what we evaluate is rnn
        update_statistics(bool): update the following statistics - dataload_duration, inference_duration

    Returns:
        Scalar with local ERM value
    &#34;&#34;&#34;
    model.train(False)

    if device is None:
        device = client_state[&#34;device&#34;]

    total_number_of_samples = len(dataloader.dataset)
    total_loss = torch.Tensor([0.0]).to(device = device, dtype = model.fl_dtype)

    # code wrap that stops autograd from tracking tensor 
    with torch.no_grad():
        for i, (data, label) in enumerate(dataloader):
            start_ts = time.time()
            batch_size = data.shape[0]
            if str(data.device) != device or str(label.device) != device or data.dtype != model.fl_dtype or label.dtype != model.fl_dtype:
                data, label = data.to(device=device, dtype=model.fl_dtype), label.to(device=device,dtype=model.fl_dtype)

            if update_statistics:
                client_state[&#34;stats&#34;][&#34;dataload_duration&#34;] += (time.time() - start_ts)

            input, label = model_funcs.get_train_inputs(data, label, model, batch_size, device, is_rnn)

            start_ts = time.time()
            outputs = model(*input)

            if not is_rnn:
                hidden = None
                output = outputs
            else:
                output, hidden = outputs

            loss = model_funcs.compute_loss(model, criterion, output, label)
            loss = loss * (1.0/total_number_of_samples)

            if update_statistics:
                client_state[&#34;stats&#34;][&#34;inference_duration&#34;] += (time.time() - start_ts)
            total_loss += loss

    if args is None:
        args = client_state[&#34;H&#34;][&#34;args&#34;]

    regulizer_global = model_funcs.get_global_regulizer(args.global_regulizer)
    R = regulizer_global(model, args.global_regulizer_alpha)

    Rvalue = 0.0
    if R is not None:
        Rvalue = R.item()

    return total_loss.item() + Rvalue

def findRecentRecord(H, client_id, field):
    &#34;&#34;&#34;
    Find in history records recent information about query record in client_states.

    Args:
        H(dict): information about client. used information - statistics, used device
        client_id(int): integer number for client
        field(str): name of the field which we are trying to find history

    Returns:
        Return requested object if it found or None if object is not found
    &#34;&#34;&#34;
    history = H[&#39;history&#39;]

    history_keys = [k for k in history.keys()]
    history_keys.sort(reverse=True)

    for r in history_keys:
        clients_history = history[r]
        if client_id in clients_history[&#39;client_states&#39;]:
            client_prev_state = clients_history[&#39;client_states&#39;][client_id][&#39;client_state&#39;]
            if field in client_prev_state:
                return_object = client_prev_state[field]
                return return_object

            else:
                # Assumption -- if client has been sampled then field have to be setuped
                return None
    return None

def findRecentRecordAndRemoveFromHistory(H, client_id, field):
    &#34;&#34;&#34;
    Find in history records recent information about query record in client_states.
    If record has been found return it, but before that remove itself from history.

    Args:
        H(dict): information about client. used information - statistics, used device
        client_id(int): integer number for client
        field(str): name of the field which we are trying to find history

    Returns:
        Return requested object if it found or None if object is not found
    &#34;&#34;&#34;
    history = H[&#39;history&#39;]
    history_keys = [k for k in history.keys()]
    history_keys.sort(reverse=True)

    for r in history_keys:
        clients_history = history[r]
        if client_id in clients_history[&#39;client_states&#39;]:
            client_prev_state = clients_history[&#39;client_states&#39;][client_id][&#39;client_state&#39;]
            if field in client_prev_state:
                return_object = client_prev_state[field]
                client_prev_state[field] = None
                return return_object
            else:
                # Assumption -- if client has been sampled then field have to be setuped
                return None
    return None

# ======================================================================================================================
def get_logger(H):
    &#34;&#34;&#34;
    Help function to get logger.

    Args:
        H(dict): server state

    Returns:
        Reference to logger
    &#34;&#34;&#34;

    my_logger = Logger.get(H[&#34;args&#34;].run_id)
    return my_logger

def has_experiment_option(H, name):
    &#34;&#34;&#34;
    Check that experimental option is presented

    Args:
        H(dict): server state
        name(str): variable name

    Returns:
        True if option is present
    &#34;&#34;&#34;
    return name in H[&#34;execution_context&#34;].experimental_options

def get_experiment_option_f(H, name):
    &#34;&#34;&#34;
    Get experimental option to carry experiments with algorithms

    Args:
        H(dict): server state
        name(str): variable name

    Returns:
        Value of requested value converted to float
    &#34;&#34;&#34;
    return float(H[&#34;execution_context&#34;].experimental_options[name])

def get_experiment_option_int(H, name):
    &#34;&#34;&#34;
    Get experimental option to carry experiments with algorithms

    Args:
        H(dict): server state
        name(str): variable name

    Returns:
        Value of requested value converted to int
    &#34;&#34;&#34;
    return int(H[&#34;execution_context&#34;].experimental_options[name])

def get_experiment_option_str(H, name):
    &#34;&#34;&#34;
    Get experimental option to carry experiments with algorithms

    Args:
        H(dict): server state
        name(str): variable name

    Returns:
        Value of requested value converted to string
    &#34;&#34;&#34;
    return str(H[&#34;execution_context&#34;].experimental_options[name])

def get_initial_shift(args:argparse.Namespace, D:int, grad_start:torch.Tensor):
    &#34;&#34;&#34;Help method to get initial shifts&#34;&#34;&#34;
    if args.initialize_shifts_policy == &#34;full_gradient_at_start&#34;:
        return grad_start.detach().clone().to(device = args.device)
    else:
        return torch.zeros(D).to(device = args.device, dtype = grad_start.dtype)
# ======================================================================================================================
class MarinaAlgorithm:
    &#39;&#39;&#39;
    MARINA Algoritm [Gorbunov et al., 2021]: https://arxiv.org/abs/2102.07845
    &#39;&#39;&#39;
    @staticmethod
    def initializeServerState(args:argparse.Namespace, model:torch.nn.Module, D:int, total_clients:int, grad_start:torch.Tensor)-&gt;dict:
        state = {&#34;x_prev&#34; : mutils.get_params(model),             # previous iterate
                 &#34;test_ber_rv&#34; : 0.0                              # test_ber_rv = 0.0 will force fisrt iteration be a full gradient evaluation
                 }
        return state

    @staticmethod
    def clientState(H:dict, clientId:int, client_data_samples:int, device:str)-&gt;dict:
        logger = Logger.get(H[&#34;run_id&#34;])

        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)

        p = 1.0 / (1.0 + compressor.getW())

        state = {}
        if H[&#34;test_ber_rv&#34;] &lt;= p:
            state.update({&#34;p&#34; : p, &#34;ck&#34;: 1, &#34;client_compressor&#34; : compressor})
        else:
            state.update({&#34;p&#34; : p, &#34;ck&#34;: 0, &#34;client_compressor&#34;: compressor})

        return state

    @staticmethod
    def localGradientEvaluation(client_state: dict,
                                model: torch.nn.Module,
                                dataloader: torch.utils.data.dataloader.DataLoader,
                                criterion: torch.nn.modules.loss._Loss,
                                is_rnn: bool,
                                local_iteration_number: tuple) -&gt; torch.Tensor:
        if client_state[&#34;ck&#34;] == 1:
            fApprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
            grad_cur = mutils.get_gradient(model)
            client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += grad_cur.numel()
            return fApprox, grad_cur
        else:
            client_id = client_state[&#34;client_id&#34;]
            fApprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
            grad_cur = mutils.get_gradient(model)

            reconstruct_params = mutils.get_params(model)
            mutils.set_params(model, client_state[&#34;H&#34;][&#34;x_prev&#34;])
            evaluateSgd(client_state, model, dataloader, criterion, is_rnn)
            grad_prev = mutils.get_gradient(model)
            mutils.set_params(model, reconstruct_params)

            g_prev = client_state[&#34;H&#34;][&#34;g_prev&#34;].to(device = client_state[&#34;device&#34;], dtype = model.fl_dtype)
            g_next = g_prev + client_state[&#34;client_compressor&#34;].compressVector(grad_cur - grad_prev)

            # Comments: server knows g_prev
            client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance
            return fApprox, g_next

    @staticmethod
    def serverGradient(clients_responses: utils.buffer.Buffer, clients: int, model: torch.nn.Module, params_current: torch.Tensor, H: dict) -&gt; torch.Tensor:
        clients_responses.waitForItem()
        obtained_model = clients_responses.get(0)
        wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
        gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        gs = wi * gi
        w_total = wi

        for i in range(1, clients):
            clients_responses.waitForItem()
            client_model = clients_responses.get(i)
            gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
            wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

            w_total += wi
            gs += wi * gi
        gs = gs / w_total
        return gs

    @staticmethod
    def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
        H[&#34;g_prev&#34;] = grad_server
        H[&#34;x_prev&#34;] = mutils.get_params(model)

        H[&#34;test_ber_rv&#34;] = H[&#39;execution_context&#39;].np_random.random()
        return H
# ======================================================================================================================
def getLismoothForClients(H, clients_responses):
    Li = np.array(H[&#39;Li_all_clients&#39;])

    if H[&#34;args&#34;].global_regulizer == &#34;none&#34;:
        pass
    elif H[&#34;args&#34;].global_regulizer == &#34;cvx_l2norm_square_div_2&#34;:
        Li = Li + (1.0 * H[&#34;args&#34;].global_regulizer_alpha)
    elif H[&#34;args&#34;].global_regulizer == &#34;noncvx_robust_linear_regression&#34;:
        Li = Li + (2.0 * H[&#34;args&#34;].global_regulizer_alpha)

    return Li

def getLsmoothGlobal(H, clients_responses):
    L = H[&#39;L_compute&#39;]

    if H[&#34;args&#34;].global_regulizer == &#34;none&#34;:
        pass
    elif H[&#34;args&#34;].global_regulizer == &#34;cvx_l2norm_square_div_2&#34;:
        L = L + (1.0 * H[&#34;args&#34;].global_regulizer_alpha)
    elif H[&#34;args&#34;].global_regulizer == &#34;noncvx_robust_linear_regression&#34;:
        L = L + (2.0 * H[&#34;args&#34;].global_regulizer_alpha)

    return L

# ======================================================================================================================
class MarinaAlgorithmPP:
    &#39;&#39;&#39;
    MARINA Algoritm [Gorbunov et al., 2021]: https://arxiv.org/abs/2102.07845
    &#39;&#39;&#39;
    @staticmethod
    def algorithmDescription():
        return { &#34;paper&#34; : &#34;https://arxiv.org/abs/2102.07845&#34; }

    @staticmethod
    def theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case):
        # Step size for non-convex case
        m = 1.0
        workers_per_round = clients_in_round
        workers = H[&#39;total_clients&#39;]
        Li_all_clients = getLismoothForClients(H, clients_responses)

        # Ltask = (np.mean( (Li_all_clients) **2) )**0.5
        Ltask = (np.mean( max(Li_all_clients) **2) )**0.5 # Maybe hack by /2
        w = H[&#34;w&#34;]
        p = (workers_per_round/workers)*1.0/(1+w)            # For RAND-K compressor
        r = workers_per_round

        step_1 = ((1 + 4*(1-p)*(1+w)/(p*workers))**0.5 - 1) / (2* (1-p) * (1+w)/(p*workers) * Ltask)
        step_2 = (-(1 + 4*(1-p)*(1+w)/(p*workers))**0.5 - 1) / (2* (1-p) * (1+w)/(p*workers) * Ltask)
        step_3 = 1.0 / (Ltask * (1 + ((1 - p) * (1 + w) / (p * workers_per_round)) ** 0.5))  # Theorem 4.1, p.37

        return step_3

    @staticmethod
    def initializeServerState(args:argparse.Namespace, model:torch.nn.Module, D:int, total_clients:int, grad_start:torch.Tensor)-&gt;dict:
        compressor = compressors.initCompressor(args.client_compressor, D)

        state = {&#34;x_prev&#34; : mutils.get_params(model),             # previous iterate
                 &#34;test_ber_rv&#34; : 0.0,                             # test_ber_rv = 0.0 will force fisrt iteration be a full gradient evaluation
                 &#34;num_clients_per_round&#34; : args.num_clients_per_round,
                 &#34;total_clients&#34; : total_clients,
                 &#34;w&#34; : compressor.getW()
                 }

        p = 1.0 / (1.0 + compressor.getW())
        p = p * args.num_clients_per_round / total_clients
        state.update({&#34;p&#34; : p})

        if state[&#34;test_ber_rv&#34;] &lt;= p:
            state[&#34;ck&#34;] = 1
            state[&#34;request_use_full_list_of_clients&#34;] = True
        else:
            state[&#34;ck&#34;] = 0
            state[&#34;request_use_full_list_of_clients&#34;] = False

        return state

    @staticmethod
    def clientState(H:dict, clientId:int, client_data_samples:int, device:str)-&gt;dict:
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)
        state = {&#34;p&#34; : H[&#34;p&#34;], &#34;ck&#34;: H[&#34;ck&#34;], &#34;client_compressor&#34; : compressor}
        return state

    @staticmethod
    def localGradientEvaluation(client_state: dict,
                                model: torch.nn.Module,
                                dataloader: torch.utils.data.dataloader.DataLoader,
                                criterion: torch.nn.modules.loss._Loss,
                                is_rnn: bool,
                                local_iteration_number: tuple) -&gt; torch.Tensor:
        if client_state[&#34;ck&#34;] == 1:
            fApprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
            grad_cur = mutils.get_gradient(model)
            client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += grad_cur.numel()
            return fApprox, grad_cur
        else:
            client_id = client_state[&#34;client_id&#34;]
            fApprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
            grad_cur = mutils.get_gradient(model)

            reconstruct_params = mutils.get_params(model)
            mutils.set_params(model, client_state[&#34;H&#34;][&#34;x_prev&#34;])
            evaluateSgd(client_state, model, dataloader, criterion, is_rnn)
            grad_prev = mutils.get_gradient(model)
            mutils.set_params(model, reconstruct_params)

            g_prev = client_state[&#34;H&#34;][&#34;g_prev&#34;].to(device = client_state[&#34;device&#34;], dtype = model.fl_dtype)
            g_next = g_prev + client_state[&#34;client_compressor&#34;].compressVector(grad_cur - grad_prev)

            # Comments: server knows g_prev
            client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance
            return fApprox, g_next

    @staticmethod
    def serverGradient(clients_responses: utils.buffer.Buffer, clients: int, model: torch.nn.Module, params_current: torch.Tensor, H: dict) -&gt; torch.Tensor:
        clients_responses.waitForItem()
        obtained_model = clients_responses.get(0)
        wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
        gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        gs = wi * gi
        w_total = wi

        for i in range(1, clients):
            clients_responses.waitForItem()
            client_model = clients_responses.get(i)
            gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
            wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

            w_total += wi
            gs += wi * gi
        gs = gs / w_total
        return gs

    @staticmethod
    def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
        H[&#34;g_prev&#34;] = grad_server
        H[&#34;x_prev&#34;] = mutils.get_params(model)

        H[&#34;test_ber_rv&#34;] = H[&#39;execution_context&#39;].np_random.random()
        if H[&#34;test_ber_rv&#34;] &lt;= H[&#34;p&#34;]:
            H[&#34;ck&#34;] = 1
            H[&#34;request_use_full_list_of_clients&#34;] = True
        else:
            H[&#34;ck&#34;] = 0
            H[&#34;request_use_full_list_of_clients&#34;] = False

        return H
# ======================================================================================================================
class SCAFFOLD:
    &#39;&#39;&#39;
    SCAFFOLD Algoritm [Karimireddy et al., 2020]: https://arxiv.org/abs/1910.06378
    &#39;&#39;&#39;
    @staticmethod
    def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D:int, total_clients:int, grad_start:torch.Tensor) -&gt; dict:
        state = {&#34;c&#34;  : torch.zeros(D).to(device = args.device, dtype = model.fl_dtype),
                 &#34;c0&#34; : torch.zeros(D).to(device = args.device, dtype = model.fl_dtype)}

        return state

    @staticmethod
    def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
        # Compressors are not part of SCAFFOLD
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)
        last_ci = findRecentRecordAndRemoveFromHistory(H, clientId, &#39;ci&#39;)

        if last_ci is None:
            return {&#34;ci&#34; : H[&#39;c0&#39;].detach().clone().to(device = device, dtype = H[&#34;fl_dtype&#34;]),
                    &#34;client_compressor&#34;: compressor}
        else:
            return {&#34;ci&#34; : last_ci.to(device = device, dtype = H[&#34;fl_dtype&#34;]),                  #last_ci.detach().clone().to(device, dtype = H[&#34;fl_dtype&#34;]),
                    &#34;client_compressor&#34;: compressor}

    @staticmethod
    def localGradientEvaluation(client_state: dict,
                                model: torch.nn.Module,
                                dataloader: torch.utils.data.dataloader.DataLoader,
                                criterion: torch.nn.modules.loss._Loss,
                                is_rnn: bool,
                                local_iteration_number: tuple) -&gt; torch.Tensor:
        c  = client_state[&#34;H&#34;][&#39;c&#39;].to(device = client_state[&#34;device&#34;], dtype = model.fl_dtype)
        ci = client_state[&#39;ci&#39;]

        if local_iteration_number[0] == 0:
            evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = False)
            c_plus = mutils.get_gradient(model)
            client_state[&#39;delta_c&#39;] = client_state[&#34;client_compressor&#34;].compressVector(c_plus - c)

            # send delta_c and delta_x for model which has the same dimension
            client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#39;delta_c&#39;].numel() # send change iterates
            client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance

        fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
        grad_cur = mutils.get_gradient(model)
        dy = grad_cur - ci + c

        return fAprox, dy

    @staticmethod
    def serverGradient(clients_responses: utils.buffer.Buffer,
                       clients: int,
                       model: torch.nn.Module,
                       params_current: torch.Tensor,
                       H: dict) -&gt; torch.Tensor:
        clients_responses.waitForItem()
        obtained_model = clients_responses.get(0)
        wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
        gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        gs = wi * gi
        w_total = wi

        for i in range(1, clients):
            clients_responses.waitForItem()
            client_model = clients_responses.get(i)
            gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
            wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

            w_total += wi
            gs += wi * gi
        gs = gs / w_total

        return gs

    @staticmethod
    def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
        # x is updates as a part of general logic
        # here we will update c = c + sum(dc) * |S|/N

        obtained_model = clients_responses.get(0)
        dc = obtained_model[&#39;client_state&#39;][&#39;delta_c&#39;].to(device = paramsPrev.device, dtype = H[&#34;fl_dtype&#34;])
        clients_num_in_round = len(clients_responses)

        for i in range(1, clients_num_in_round):
            client_model = clients_responses.get(i)
            dc += client_model[&#39;client_state&#39;][&#39;delta_c&#39;].to(device = paramsPrev.device, dtype = H[&#34;fl_dtype&#34;])

        # Make dc is average of detla_c
        dc = dc / clients_num_in_round

        # Construct final delta step for update &#34;c&#34;
        dc = dc * float(clients_num_in_round)/float(H[&#34;total_clients&#34;])
        H[&#34;c&#34;] += dc
        return H
# ======================================================================================================================
class FRECON:
    &#39;&#39;&#39;
    FRECON Algoritm [Haoyu Zhao et al., 2021]: https://arxiv.org/abs/2112.13097
    &#39;&#39;&#39;
    @staticmethod
    def theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case):
        # FRECON in non-convex case
        S = clients_in_round
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        w = compressor.getW()
        a = 1 / (1.0 + w)
        n = H[&#39;total_clients&#39;]
        Li_all_clients = getLismoothForClients(H, clients_responses)
        Lmax = max(Li_all_clients)
        step_size = 1.0/(Lmax * (1 + (10*(1+w)*(1+w)*n/S/S)**0.5))
        return step_size

    @staticmethod
    def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D:int, total_clients:int, grad_start) -&gt; dict:

        state = {&#34;h0&#34;: get_initial_shift(args, D, grad_start),
                 &#34;g0&#34;: grad_start.detach().clone().to(device = args.device, dtype = model.fl_dtype),
                 &#34;h_prev&#34;: get_initial_shift(args, D, grad_start),
                 &#34;g_server_prev&#34;: grad_start.detach().clone().to(device = args.device, dtype = model.fl_dtype),
                 &#34;x_prev&#34;: mutils.get_params(model)
                 }

        return state

    @staticmethod
    def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)
        last_hi = findRecentRecordAndRemoveFromHistory(H, clientId, &#39;hi&#39;)

        #last_qi = findRecentRecordAndRemoveFromHistory(H, clientId, &#39;qi&#39;)
        # Drop qi
        #last_qi = None

        w = compressor.getW()
        alpha = 1 / (1.0 + w)

        if last_hi is None:
            return {&#34;client_compressor&#34; : compressor, &#34;alpha&#34; : alpha, &#34;hi&#34; : H[&#39;h0&#39;].detach().clone().to(device = device, dtype = H[&#34;fl_dtype&#34;])}
        else:
            return {&#34;client_compressor&#34; : compressor, &#34;alpha&#34; : alpha,
                    &#34;hi&#34; : last_hi.to(device = device, dtype = H[&#34;fl_dtype&#34;])  #last_hi.detach().clone().to(device = device, dtype = H[&#34;fl_dtype&#34;])
                   }

    @staticmethod
    def localGradientEvaluation(client_state: dict,
                                model: torch.nn.Module,
                                dataloader: torch.utils.data.dataloader.DataLoader,
                                criterion: torch.nn.modules.loss._Loss,
                                is_rnn: bool,
                                local_iteration_number: tuple) -&gt; torch.Tensor:

        client_id = client_state[&#34;client_id&#34;]
        fApprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function=True)

        # Please select in GUI SGD-US or another estimator
        grad_cur = mutils.get_gradient(model)
        ui = client_state[&#34;client_compressor&#34;].compressVector(grad_cur - client_state[&#39;hi&#39;])
        #if client_state[&#34;H&#34;][&#34;current_round&#34;] != 0:
        client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance
        client_state[&#39;hi&#39;] = client_state[&#39;hi&#39;] + client_state[&#39;alpha&#39;] * ui

        # ==============================================================================================================
        reconstruct_params = mutils.get_params(model)
        mutils.set_params(model, client_state[&#34;H&#34;][&#34;x_prev&#34;])
        evaluateSgd(client_state, model, dataloader, criterion, is_rnn)
        grad_prev = mutils.get_gradient(model)
        mutils.set_params(model, reconstruct_params)
        # ==============================================================================================================
        qi = client_state[&#34;client_compressor&#34;].compressVector(grad_cur - grad_prev)
        #if client_state[&#34;H&#34;][&#34;current_round&#34;] != 0:
        client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance
        client_state[&#39;qi&#39;] = qi
        # =============================================================================================================
        return fApprox, ui

    @staticmethod
    def serverGradient(clients_responses: utils.buffer.Buffer,
                       clients: int,
                       model: torch.nn.Module,
                       params_current: torch.Tensor,
                       H: dict) -&gt; torch.Tensor:
        clients_responses.waitForItem()
        obtained_model = clients_responses.get(0)
        wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
        gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        alpha = obtained_model[&#34;client_state&#34;][&#39;alpha&#39;]

        gs = wi * gi
        q_avg = wi * obtained_model[&#39;client_state&#39;][&#39;qi&#39;]
        w_total = wi

        del obtained_model[&#39;client_state&#39;][&#39;qi&#39;]

        for i in range(1, clients):
            clients_responses.waitForItem()
            client_model = clients_responses.get(i)
            gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
            wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]
            q_avg += wi * client_model[&#39;client_state&#39;][&#39;qi&#39;]

            w_total += wi
            gs += wi * gi

            del client_model[&#39;client_state&#39;][&#39;qi&#39;]

        gs = gs / w_total
        q_avg = q_avg / w_total
        u = gs

        h_prev = H[&#39;h_prev&#39;]

        # ===============================================================================================================
        if has_experiment_option(H, &#34;lambda_&#34;):
            lambda_ = get_experiment_option_f(H, &#34;lambda_&#34;)
        elif has_experiment_option(H, &#34;th_stepsize_noncvx&#34;) or has_experiment_option(H, &#34;th_stepsize_cvx&#34;):
            S = clients
            compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
            w = compressor.getW()
            n = H[&#39;total_clients&#39;]
            H[&#34;lambda_th&#34;] = S / (2 * (1 + w) * n)
            lambda_ = S/( 2*(1+w) * n )
            get_logger(H).info(f&#34;Used lambda is {lambda_}&#34;)
        # ==============================================================================================================
        result = q_avg + (1.0 - lambda_) * H[&#34;g_server_prev&#34;] + lambda_ * (u + h_prev)

        multipler_alpha = alpha * (clients/H[&#39;total_clients&#39;])
        H[&#39;u_avg_update&#39;] =  u
        H[&#39;alpha_update&#39;] = multipler_alpha
        return result

    @staticmethod
    def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
        H[&#39;h_prev&#39;] = H[&#39;h_prev&#39;] + H[&#34;alpha_update&#34;] * H[&#39;u_avg_update&#39;]
        H[&#34;x_prev&#34;] = paramsPrev
        H[&#34;g_server_prev&#34;] = grad_server
        return H
# ======================================================================================================================
class COFIG:
    &#39;&#39;&#39;
    COFIG Algoritm [Haoyu Zhao et al., 2021]: https://arxiv.org/abs/2112.13097
    Assumption: \widetilda{S} = S, i.e. they are the same sets
    &#39;&#39;&#39;

    @staticmethod
    def theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case):
        # Step size for non-convex case
        S = clients_in_round
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])

        w = compressor.getW()
        a = 1 / (1.0 + w)

        if use_steps_size_for_non_convex_case:
            Li_all_clients = getLismoothForClients(H, clients_responses)
            Lmax = max(Li_all_clients)
            step_size_1 = 1.0/( Lmax * 2 )
            step_size_2 = S / ( 5 * Lmax * (1 + w) * (H[&#39;total_clients&#39;] ** (2.0/3.0)) )
            step_size_3 = S / ( 5 * Lmax * ((1 + w)**3.0/2.0) * (H[&#39;total_clients&#39;] ** (0.5)) )

            return min(step_size_1, step_size_2, step_size_3)

        else:
            Li_all_clients = getLismoothForClients(H, clients_responses)
            Lmax = max(Li_all_clients)

            step_size_1 = 1.0/( Lmax * (2 + 8 * (1 + w) / S) )
            step_size_2 = S / ( (1 + w) * (H[&#39;total_clients&#39;] ** 0.5) )

            return min(step_size_1, step_size_2)

    @staticmethod
    def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D:int, total_clients:int, grad_start) -&gt; dict:
        cm = compressors.Compressor()
        cm.makeIdenticalCompressor()

        state = {&#34;compressor_master&#34; : cm,
                 &#34;h0&#34;: get_initial_shift(args, D, grad_start),
                 &#34;h_prev&#34;: get_initial_shift(args, D, grad_start),
                 }

        return state

    @staticmethod
    def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)
        last_hi = findRecentRecordAndRemoveFromHistory(H, clientId, &#39;hi&#39;)
        alpha = 1.0 / (1.0 + compressor.getW())

        if last_hi is None:
            return {&#34;client_compressor&#34; : compressor, &#34;alpha&#34; : alpha, &#34;hi&#34; : H[&#39;h0&#39;].detach().clone().to(device = device, dtype = H[&#34;fl_dtype&#34;])}
        else:
            return {&#34;client_compressor&#34; : compressor, &#34;alpha&#34; : alpha,
                    &#34;hi&#34; : last_hi.to(device = device, dtype = H[&#34;fl_dtype&#34;]) #last_hi.detach().clone().to(device = device)
                   }

    @staticmethod
    def localGradientEvaluation(client_state: dict,
                                model: torch.nn.Module,
                                dataloader: torch.utils.data.dataloader.DataLoader,
                                criterion: torch.nn.modules.loss._Loss,
                                is_rnn: bool,
                                local_iteration_number: tuple) -&gt; torch.Tensor:

        client_id = client_state[&#34;client_id&#34;]
        fApprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function=True)

        # Please select in GUI SGD-US or another estimator
        grad_cur = mutils.get_gradient(model)

        ui = client_state[&#34;client_compressor&#34;].compressVector(grad_cur - client_state[&#39;hi&#39;])
        client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance

        # Update hi (Experiment!)
        client_state[&#39;hi&#39;] = client_state[&#39;hi&#39;] + client_state[&#39;alpha&#39;] * ui
        return fApprox, ui

    @staticmethod
    def serverGradient(clients_responses: utils.buffer.Buffer,
                       clients: int,
                       model: torch.nn.Module,
                       params_current: torch.Tensor,
                       H: dict) -&gt; torch.Tensor:
        clients_responses.waitForItem()
        obtained_model = clients_responses.get(0)
        wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
        gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])

        alpha = obtained_model[&#34;client_state&#34;][&#39;alpha&#39;]

        gs = wi * gi

        w_total = wi

        for i in range(1, clients):
            clients_responses.waitForItem()
            client_model = clients_responses.get(i)
            gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
            wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

            w_total += wi
            gs += wi * gi

        gs = gs / w_total
        u = gs

        h_prev = H[&#39;h_prev&#39;]
        result =  u + h_prev

        multipler_alpha = alpha * (clients/H[&#39;total_clients&#39;])
        H[&#39;u_avg_update&#39;] =  gs
        H[&#39;alpha_update&#39;] = multipler_alpha
        return result

    @staticmethod
    def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
        H[&#39;h_prev&#39;] = H[&#39;h_prev&#39;] + H[&#34;alpha_update&#34;] * H[&#39;u_avg_update&#39;]
        return H
# ======================================================================================================================
class DIANA:
    &#39;&#39;&#39;
    DIANA Algoritm [Mishchenko et al., 2019]: https://arxiv.org/abs/1901.09269, https://arxiv.org/pdf/1904.05115.pdf
    &#39;&#39;&#39;
    @staticmethod
    def theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case):
        # DIANA for non-convex case
        if use_steps_size_for_non_convex_case:
            # For non-convex case
            m = 1.0
            workers_per_round = clients_in_round
            workers = H[&#39;total_clients&#39;]
            Ltask = getLsmoothGlobal(H, clients_responses)
            step_size = 1.0/(10*Ltask*(1 + H[&#34;w&#34;]/workers)**0.5 * (m**(2.0/3.0) + H[&#34;w&#34;] + 1))  # Th.4 of https://arxiv.org/pdf/1904.05115.pdf
            return step_size
        else:
            # For convex case
            compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
            w = compressor.getW()
            a = 1 / (1.0 + w)
            Li_all_clients = getLismoothForClients(H, clients_responses)
            Lmax = max(Li_all_clients)
            step_size = 1.0 / (Lmax * (1 + 4*w/clients_in_round))                    # SGD-CTRL analysis for strongly-covnex case
            return step_size

    @staticmethod
    def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D:int, total_clients:int, grad_start:torch.Tensor) -&gt; dict:
        c = compressors.initCompressor(args.client_compressor, D)
        w = c.getW()
        alpha = 1.0 / (1.0 + w)

        state = {&#34;h0&#34;    : get_initial_shift(args, D, grad_start),
                 &#34;h&#34;     : get_initial_shift(args, D, grad_start),
                 &#34;alpha&#34; : alpha,
                 &#34;w&#34;     :w
                }
        return state

    @staticmethod
    def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)

        last_hi = findRecentRecordAndRemoveFromHistory(H, clientId, &#39;hi&#39;)

        if last_hi is None:
            return {&#34;client_compressor&#34; : compressor,
                    &#34;hi&#34; : H[&#39;h0&#39;].detach().clone().to(device = device, dtype = H[&#34;fl_dtype&#34;])}
        else:
            return {&#34;client_compressor&#34; : compressor,
                    &#34;hi&#34; : last_hi.to(device = device, dtype = H[&#34;fl_dtype&#34;])          #last_hi.detach().clone().to(device = device, dtype = H[&#34;fl_dtype&#34;])
                   }

    @staticmethod
    def localGradientEvaluation(client_state: dict,
                                model: torch.nn.Module,
                                dataloader: torch.utils.data.dataloader.DataLoader,
                                criterion: torch.nn.modules.loss._Loss,
                                is_rnn: bool,
                                local_iteration_number: tuple) -&gt; torch.Tensor:
        # In theory it&#39;s possible to perform compute without accessing &#34;h&#34; from master
        h = client_state[&#39;hi&#39;]
        fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
        grad_cur = mutils.get_gradient(model)
        m_i = client_state[&#34;client_compressor&#34;].compressVector(grad_cur - h)

        # Comments: server needs only obtain m_i
        client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance

        client_state[&#39;hi&#39;] = client_state[&#39;hi&#39;] + client_state[&#39;H&#39;][&#39;alpha&#39;] * m_i
        return fAprox, m_i

    @staticmethod
    def serverGradient(clients_responses: utils.buffer.Buffer,
                       clients: int,
                       model: torch.nn.Module,
                       params_current: torch.Tensor,
                       H: dict) -&gt; torch.Tensor:
        clients_responses.waitForItem()
        obtained_model = clients_responses.get(0)
        wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
        gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])

        gs = wi * gi
        w_total = wi

        for i in range(1, clients):
            clients_responses.waitForItem()
            client_model = clients_responses.get(i)
            gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
            wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

            w_total += wi
            gs += wi * gi
        gs = gs / w_total


        # Here gs is final gradient estimator without shift
        H[&#39;m&#39;] = gs
        h = H[&#39;h&#39;]
        return h + gs

    @staticmethod
    def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
        mk = H[&#39;m&#39;]
        H[&#39;h&#39;] = H[&#39;h&#39;] + H[&#39;alpha&#39;] * mk
        return H
# ======================================================================================================================
class EF21:
    &#39;&#39;&#39;
    EF21 Algoritm: &#34;EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback&#34;, https://arxiv.org/abs/2106.05203
    &#39;&#39;&#39;
    @staticmethod
    def theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case):
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        if compressor.isContractionCompressor():
            a = compressor.getAlphaContraction() # use alpha for contraction compressor
        elif compressor.isUnbiasedCompressor():
            a = 1/(1.0 + compressor.getW())      # use w for scaled unbiased compressor

        Li = getLismoothForClients(H, clients_responses)
        Ltask = getLsmoothGlobal(H, clients_responses)
        Ltilda = np.mean(Li**2)**0.5

        theta = 1 - (1 - a)**0.5
        beta = (1.0 - a) / (1 - (1 - a)**0.5)
        gamma = 1.0 / (Ltask + Ltilda * (beta/theta)**0.5)

        if has_experiment_option(H, &#39;stepsize_multiplier&#39;):
            gamma = gamma * get_experiment_option_f(H, &#39;stepsize_multiplier&#39;)

        return gamma # Th.1, p.40 from EF21

    @staticmethod
    def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D:int, total_clients:int, grad_start:torch.Tensor) -&gt; dict:
        cm = compressors.Compressor()
        cm.makeIdenticalCompressor()
        state = {&#34;compressor_master&#34; : cm,
                 &#34;x0&#34; : mutils.get_params(model),
                 &#34;request_use_full_list_of_clients&#34;: True
                 }
        return state

    @staticmethod
    def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)
        last_g_prev = findRecentRecordAndRemoveFromHistory(H, clientId, &#39;g_prev&#39;)

        if last_g_prev is None:
            return {&#34;client_compressor&#34; : compressor,
                    &#34;g_prev&#34; : None
                    }
        else:
            return {&#34;client_compressor&#34; : compressor,
                    &#34;g_prev&#34; : last_g_prev.to(device = device, dtype = H[&#34;fl_dtype&#34;]) #last_g_prev.detach().clone().to(device = device)
                    }

    @staticmethod
    def localGradientEvaluation(client_state: dict,
                                model: torch.nn.Module,
                                dataloader: torch.utils.data.dataloader.DataLoader,
                                criterion: torch.nn.modules.loss._Loss,
                                is_rnn: bool,
                                local_iteration_number: tuple) -&gt; torch.Tensor:
        # Compute g0 for a first iteration
        g_prev = client_state[&#39;g_prev&#39;]
        if g_prev is None:
            fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function=True)
            grad_cur = mutils.get_gradient(model)
            client_state[&#39;g_prev&#39;] = grad_cur
            # Not take into account communication at first round
            return fAprox, grad_cur
        else:
            # In theory it&#39;s possible to perform compute without accessing &#34;h&#34; from master
            fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
            grad_cur = mutils.get_gradient(model)

            g_prev = client_state[&#39;g_prev&#39;]

            compressor_multiplier = 1.0
            if not client_state[&#34;client_compressor&#34;].isContractionCompressor():
                compressor_multiplier = 1.0/(1.0 + client_state[&#34;client_compressor&#34;].getW())

            g_next = g_prev + client_state[&#34;client_compressor&#34;].compressVector(grad_cur - g_prev) * compressor_multiplier
            # In algorithm really we need only to send compressed difference between new gradient and previous gradient estimator
            client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance
            client_state[&#39;g_prev&#39;] = g_next
            return fAprox, g_next

    @staticmethod
    def serverGradient(clients_responses: utils.buffer.Buffer,
                       clients: int,
                       model: torch.nn.Module,
                       params_current: torch.Tensor,
                       H: dict) -&gt; torch.Tensor:
        # We compute it straightford.
        # In the paper the master uses g^t on server side and combine that with avg. of c_i^t

        clients_responses.waitForItem()
        obtained_model = clients_responses.get(0)
        wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
        gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        gs = wi * gi
        w_total = wi

        for i in range(1, clients):
            clients_responses.waitForItem()
            client_model = clients_responses.get(i)
            gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
            wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

            w_total += wi
            gs += wi * gi
        gs = gs / w_total
        grad_compress = H[&#34;compressor_master&#34;].compressVector(gs)
        return grad_compress

    @staticmethod
    def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
        compressor = H[&#34;compressor_master&#34;]
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, paramsPrev.device, -1, H)
        H[&#34;request_use_full_list_of_clients&#34;] = False
        return H
# ======================================================================================================================
class EF21PP:
    &#39;&#39;&#39;
    EF21PP: &#34;EF21 with Bells &amp; Whistles: Practical Algorithmic Extensions of Modern Error Feedback&#34; https://arxiv.org/abs/2110.03294, PP with Poisson sampling
    &#39;&#39;&#39;
    @staticmethod
    def theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case):
        # Theoretical steps size for non-convex case
        p = H[&#39;args&#39;].client_sampling_poisson
        assert p &gt; 0.0

        pmax = p
        pmin = p

        rho = 1e-3
        s = 1e-3

        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        if compressor.isContractionCompressor():
            a = compressor.getAlphaContraction() # use alpha for contraction compressor
        elif compressor.isUnbiasedCompressor():
            a = 1/(1.0 + compressor.getW())      # use w for scaled unbiased compressor

        theta = 1 - (1 + s)*(1 - a)
        beta = (1.0 + 1.0/s) * (1 - a)
        thetap = rho * pmin + theta * pmax - rho - (pmax - pmin)
        Li = getLismoothForClients(H, clients_responses)

        B = (beta * p + (1 + 1.0/rho) * (1 - p)) * (np.mean(Li**2) )

        Ltask = getLsmoothGlobal(H, clients_responses)

        return 1.0/(Ltask + (B/thetap)**0.5) # Th.7, p.47 from EF21-PP

    @staticmethod
    def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D:int, total_clients:int, grad_start:torch.Tensor) -&gt; dict:
        cm = compressors.Compressor()
        cm.makeIdenticalCompressor()
        state = {&#34;compressor_master&#34; : cm,
                 &#34;x0&#34; : mutils.get_params(model),
                 &#34;request_use_full_list_of_clients&#34;: True
                 }
        return state

    @staticmethod
    def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)
        last_g_prev = findRecentRecordAndRemoveFromHistory(H, clientId, &#39;g_prev&#39;)

        if last_g_prev is None:
            return {&#34;client_compressor&#34; : compressor,
                    &#34;g_prev&#34; : None
                    }
        else:
            return {&#34;client_compressor&#34; : compressor,
                    &#34;g_prev&#34; : last_g_prev.to(device = device, dtype = H[&#34;fl_dtype&#34;]) #last_g_prev.detach().clone().to(device = device, dtype = H[&#34;fl_dtype&#34;])
                    }

    @staticmethod
    def localGradientEvaluation(client_state: dict,
                                model: torch.nn.Module,
                                dataloader: torch.utils.data.dataloader.DataLoader,
                                criterion: torch.nn.modules.loss._Loss,
                                is_rnn: bool,
                                local_iteration_number: tuple) -&gt; torch.Tensor:
        # Compute g0 for a first iteration
        g_prev = client_state[&#39;g_prev&#39;]
        if g_prev is None:
            fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function=True)
            grad_cur = mutils.get_gradient(model)
            client_state[&#39;g_prev&#39;] = grad_cur
            # Not take into account communication at first round
            return fAprox, grad_cur
        else:
            # In theory it&#39;s possible to perform compute without accessing &#34;h&#34; from master
            fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
            grad_cur = mutils.get_gradient(model)

            g_prev = client_state[&#39;g_prev&#39;]

            compressor_multiplier = 1.0
            if not client_state[&#34;client_compressor&#34;].isContractionCompressor():
                compressor_multiplier = 1.0/(1.0 + client_state[&#34;client_compressor&#34;].getW())

            g_next = g_prev + client_state[&#34;client_compressor&#34;].compressVector(grad_cur - g_prev) * compressor_multiplier
            client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance
            client_state[&#39;g_prev&#39;] = g_next
            return fAprox, g_next

    @staticmethod
    def serverGradient(clients_responses: utils.buffer.Buffer,
                       clients: int,
                       model: torch.nn.Module,
                       params_current: torch.Tensor,
                       H: dict) -&gt; torch.Tensor:
        # We compute it straightford.
        # In the paper the master uses g^t on server side and combine that with avg. of c_i^t

        clients_responses.waitForItem()
        obtained_model = clients_responses.get(0)
        wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
        gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        gs = wi * gi
        w_total = wi

        for i in range(1, clients):
            clients_responses.waitForItem()
            client_model = clients_responses.get(i)
            gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
            wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

            w_total += wi
            gs += wi * gi
        gs = gs / w_total
        grad_compress = H[&#34;compressor_master&#34;].compressVector(gs)
        return grad_compress

    @staticmethod
    def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
        compressor = H[&#34;compressor_master&#34;]
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, paramsPrev.device, -1, H)
        H[&#34;request_use_full_list_of_clients&#34;] = False
        return H
# ======================================================================================================================
class DCGD:
    &#39;&#39;&#39;
    Distributed Compressed Gradient Descent Algoritm [Alistarh et al., 2017, Khirirat et al., 2018, Horvath et al., 2019]: https://arxiv.org/abs/1610.02132, https://arxiv.org/abs/1806.06573, https://arxiv.org/abs/1905.10988
    &#39;&#39;&#39;
    @staticmethod
    def theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case):
        # Step size for convex case
        workers = H[&#39;total_clients&#39;]
        Li_all_clients = getLismoothForClients(H, clients_responses)
        L = getLsmoothGlobal(H, clients_responses)
        w = H[&#34;w&#34;]
        wM = H[&#34;compressor_master&#34;].getW()

        A = L + 2 * (wM + 1) * max(Li_all_clients * w/workers) + L * wM
        step_size = 1.0/A
        return step_size

    @staticmethod
    def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D: int, total_clients:int, grad_start:torch.Tensor) -&gt; dict:
        cm = compressors.Compressor()
        cm.makeIdenticalCompressor()

        c = compressors.initCompressor(args.client_compressor, D)
        state = {&#34;compressor_master&#34; : cm}

        if c.isUnbiasedCompressor():
            state[&#34;w&#34;] = c.getW()

        elif c.isContractionCompressor():
            # Not applicable for DCGD
            state[&#34;alpha&#34;] = c.getAlphaContraction()

        return state

    @staticmethod
    def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)
        return {&#34;client_compressor&#34; : compressor}

    @staticmethod
    def localGradientEvaluation(client_state: dict,
                                model: torch.nn.Module,
                                dataloader: torch.utils.data.dataloader.DataLoader,
                                criterion: torch.nn.modules.loss._Loss,
                                is_rnn: bool,
                                local_iteration_number: tuple) -&gt; torch.Tensor:
        fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
        grad_cur = mutils.get_gradient(model)
        grad_compress = client_state[&#34;client_compressor&#34;].compressVector(grad_cur)
        client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance
        return fAprox, grad_compress

    @staticmethod
    def serverGradient(clients_responses: utils.buffer.Buffer,
                       clients: int,
                       model: torch.nn.Module,
                       params_current: torch.Tensor,
                       H: dict) -&gt; torch.Tensor:
        clients_responses.waitForItem()
        obtained_model = clients_responses.get(0)
        wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
        gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        gs = wi * gi
        w_total = wi

        for i in range(1, clients):
            clients_responses.waitForItem()
            client_model = clients_responses.get(i)
            gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
            wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

            w_total += wi
            gs += wi * gi
        gs = gs / w_total
        grad_compress = H[&#34;compressor_master&#34;].compressVector(gs)
        return grad_compress

    @staticmethod
    def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
        compressor = H[&#34;compressor_master&#34;]
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, paramsPrev.device, -1, H)
        return H

# ======================================================================================================================
class FedAvg:
    &#39;&#39;&#39;
    Algorithm FedAVG [McMahan et al., 2017]: https://arxiv.org/abs/1602.05629 
    &#39;&#39;&#39;
    @staticmethod
    def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D:int, total_clients:int, grad_start:torch.Tensor) -&gt; dict:
        state = {}
        return state

    @staticmethod
    def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
        return {}

    @staticmethod
    def localGradientEvaluation(client_state: dict,
                                model: torch.nn.Module,
                                dataloader: torch.utils.data.dataloader.DataLoader,
                                criterion: torch.nn.modules.loss._Loss,
                                is_rnn: bool,
                                local_iteration_number: tuple) -&gt; torch.Tensor:
        fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
        grad_cur = mutils.get_gradient(model)
        client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += grad_cur.numel()

        return fAprox, grad_cur

    @staticmethod
    def serverGradient(clients_responses: utils.buffer.Buffer,
                       clients: int,
                       model: torch.nn.Module,
                       params_current: torch.Tensor,
                       H: dict) -&gt; torch.Tensor:
        clients_responses.waitForItem()
        obtained_model = clients_responses.get(0)
        wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
        gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        gs = wi * gi
        w_total = wi

        for i in range(1, clients):
            clients_responses.waitForItem()
            client_model = clients_responses.get(i)
            gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
            wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

            w_total += wi
            gs += wi * gi
        gs = gs / w_total

        return gs

    @staticmethod
    def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
        return H
# ======================================================================================================================
class FedProx:
    &#39;&#39;&#39;
    Algorithm FedProx  [Li et al., 2018]: https://arxiv.org/abs/1812.06127
    &#39;&#39;&#39;
    @staticmethod
    def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D:int, total_clients:int, grad_start:torch.Tensor) -&gt; dict:
        state = {&#39;wt&#39;: mutils.get_params(model)}
        return state

    @staticmethod
    def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)
        return {&#34;client_compressor&#34;: compressor}

    @staticmethod
    def localGradientEvaluation(client_state: dict,
                                model: torch.nn.Module,
                                dataloader: torch.utils.data.dataloader.DataLoader,
                                criterion: torch.nn.modules.loss._Loss,
                                is_rnn: bool,
                                local_iteration_number: tuple) -&gt; torch.Tensor:
        fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
        grad_cur = mutils.get_gradient(model)

        opts = client_state[&#39;H&#39;][&#39;execution_context&#39;].experimental_options
        mu_prox = 1.0
        if &#34;mu_prox&#34; in opts:
            mu_prox = float(opts[&#39;mu_prox&#39;])

        x_cur = mutils.get_params(model)
        wt = client_state[&#39;H&#39;][&#39;wt&#39;].to(device = client_state[&#34;device&#34;], dtype = model.fl_dtype)

        grad_cur += mu_prox * (x_cur - wt)

        client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += grad_cur.numel()
        # assume sending &#39;wt&#39; from master to clients is for free

        grad_cur = client_state[&#34;client_compressor&#34;].compressVector(grad_cur)

        return fAprox, grad_cur

    @staticmethod
    def serverGradient(clients_responses: utils.buffer.Buffer,
                       clients: int,
                       model: torch.nn.Module,
                       params_current: torch.Tensor,
                       H: dict) -&gt; torch.Tensor:
        clients_responses.waitForItem()
        obtained_model = clients_responses.get(0)
        wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
        gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        gs = wi * gi
        w_total = wi

        for i in range(1, clients):
            clients_responses.waitForItem()
            client_model = clients_responses.get(i)
            gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
            wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

            w_total += wi
            gs += wi * gi
        gs = gs / w_total

        return gs

    @staticmethod
    def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
        H[&#39;wt&#39;] = mutils.get_params(model)
        return H
# ======================================================================================================================
def getImplClassForAlgo(algorithm):
    &#34;&#34;&#34;
    Get imlementation class for specific algorithm

    Args:
        algorithm(str): Algorithm implementation

    Returns:
        class type with need interface methods
    &#34;&#34;&#34;
    classImpl = None
    if algorithm == &#34;marina&#34;:
        classImpl = MarinaAlgorithm
    elif algorithm == &#34;dcgd&#34;:
        classImpl = DCGD
    elif algorithm == &#34;fedavg&#34;:
        classImpl = FedAvg
    elif algorithm == &#34;diana&#34;:
        classImpl = DIANA
    elif algorithm == &#34;scaffold&#34;:
        classImpl = SCAFFOLD
    elif algorithm == &#34;fedprox&#34;:
        classImpl = FedProx
    elif algorithm == &#34;ef21&#34;:
        classImpl = EF21
    elif algorithm == &#34;cofig&#34;:
        classImpl = COFIG
    elif algorithm == &#34;frecon&#34;:
        classImpl = FRECON
    elif algorithm == &#34;ef21-pp&#34;:
        classImpl = EF21PP
    elif algorithm == &#34;pp-marina&#34;:
        classImpl = MarinaAlgorithmPP
    else:
        raise ValueError(f&#34;Please check algorithm. There is no implementation for &#39;{algorithm}&#39;&#34;)

    return classImpl

def getAlgorithmsList():
    &#34;&#34;&#34;
    Get list of algorithms in order in which sorting happens in GUI
    &#34;&#34;&#34;
    algoList = [&#34;marina&#34;, &#34;dcgd&#34;, &#34;cofig&#34;, &#34;frecon&#34;, &#34;fedavg&#34;, &#34;diana&#34;, &#34;scaffold&#34;,
                &#34;fedprox&#34;, &#34;ef21&#34;, &#34;ef21-pp&#34;, &#34;pp-marina&#34;]

    for a in algoList:
        assert getImplClassForAlgo(a) is not None

    return algoList

def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, total_clients:int, grad_start: torch.Tensor, exec_ctx) -&gt; dict:
    &#34;&#34;&#34;
    Initialize server state.

    Server state is a main source of information with various information, including:
     - &#39;x0&#39; : start iterate. Be default it&#39;s a current position where model is
     - &#39;algorithm&#39;: string represenation of the used algorithms
     - &#39;history&#39;: history by rounds

    Args:
        args (argparse): Parsed command line for the python shell process
        model (nn.Module): Model under which server operate
        total_clients (int): Total number of clients in the experiment
        grad_start (torch.Tensor): Full gradient at starting point

    Returns:
        Returns initialize server state for specific algorithms args.algorithm
    &#34;&#34;&#34;
    serverState = {}
    classImpl = getImplClassForAlgo(args.algorithm)

    D = mutils.number_of_params(model)
    serverState = classImpl.initializeServerState(args, model, D, total_clients, grad_start)

    serverState.update( {&#39;algorithm&#39; : args.algorithm,
                         &#39;history&#39; : {},
                         &#39;D&#39; : D,
                         &#39;fl_dtype&#39; : model.fl_dtype,
                         &#39;D_include_frozen&#39; : mutils.number_of_params(model, skipFrozen = False),
                         &#39;current_round&#39; : 0,
                         &#39;client_compressor&#39; : args.client_compressor,
                         &#39;run_id&#39; : args.run_id,
                         &#39;start_time&#39;: time.strftime(&#34;%d %b %Y %H:%M:%S UTC%z&#34;, time.localtime()),
                         &#39;server_state_update_time&#39;: time.strftime(&#34;%d %b %Y %H:%M:%S UTC%z&#34;, time.localtime())
                         } )

    if &#39;x0&#39; not in serverState:
        serverState.update( {&#39;x0&#39; : mutils.get_params(model) } )

    return serverState

def clientState(H:dict, clientId:int, client_data_samples:int, round:int, device:str)-&gt;dict:
    &#34;&#34;&#34;
    Initialize client state.

    Clientstate is initialized from the scratch at each round for each selected client which is participates in optimization.
    If you want to initilize client via using it&#39;s previous state please use findRecentRecord() to find need information in server state.

     clientstate is a second source of information which helps to operate for clients
     - &#39;H&#39; : referece to server state
     - &#39;algorithm&#39;: string represenation of the used algorithms
     - &#39;client_id&#39;: id of the client
     - &#39;device&#39;: target device for the client
     - &#39;round&#39;: number of round in which this state has been used
     - &#39;weight&#39;: weight used in the aggregation in serverGradient
     - &#39;stats&#39;: different statistics for a single client
     - &#39;seed&#39;: custom seed for pseudo-random generator for a client

    Args:
        H (dict): Server state
        clientId(int): Id of the client
        client_data_samples(int): Number of data points for client
        round(int): Number of the round
        device(str): Target device which should be used by the client for computations and store client state

    Returns:
        Initialized client state
    &#34;&#34;&#34;
    classImpl = getImplClassForAlgo(H[&#34;algorithm&#34;])

    clientState = classImpl.clientState(H, clientId, client_data_samples, device)
    if &#39;weight&#39; not in clientState:
        clientState.update( {&#39;weight&#39; : 1.0} )


    clientState.update( {&#39;H&#39;         : H,
                         &#39;algorithm&#39; : H[&#34;algorithm&#34;],
                         &#39;client_id&#39; : clientId,
                         &#39;device&#39;    : device,
                         &#39;weight&#39;    : 1.0,
                         &#39;round&#39;     : round,
                         &#39;approximate_f_value&#39; : [],
                         &#39;seed&#39; :   H[&#39;execution_context&#39;].np_random.randint(2**31)
                         }
                      )

    stats = {&#34;dataload_duration&#34;  : 0.0,
             &#34;inference_duration&#34; : 0.0,
             &#34;backprop_duration&#34;  : 0.0,
             &#34;full_gradient_oracles&#34;    : 0,
             &#34;samples_gradient_oracles&#34; : 0,
             &#34;send_scalars_to_master&#34;   : 0
             }

    clientState.update({&#34;stats&#34; : stats})

    return clientState

def localGradientEvaluation(client_state:dict, model:torch.nn.Module, dataloader:torch.utils.data.dataloader.DataLoader, criterion:torch.nn.modules.loss._Loss, is_rnn:bool, local_iteration_number:tuple)-&gt;torch.Tensor:
    &#34;&#34;&#34;
    Evalute local gradient for client.

    This API should implement optimization schema specific SGD estimator.

    Args:
        client_state (dict): state of the client which evaluate local gradient
        model(nn.Module): Initialized computation graph(model) locating currently in the interesting &#34;x&#34; position
        dataloader(torch.utils.data.dataloader.DataLoader): DataLoader mechanism to fectch data
        criterion(class): Loss function for minimization, defined as a &#34;summ&#34; of loss over train samples.
        is_rnn(bool): boolean flag which say that model is RNN

    Returns:
        Flat 1D SGD vector
    &#34;&#34;&#34;
    classImpl = getImplClassForAlgo(client_state[&#34;algorithm&#34;])
    return classImpl.localGradientEvaluation(client_state, model, dataloader, criterion, is_rnn, local_iteration_number)

def serverGradient(clients_responses : utils.buffer.Buffer,
                   clients : int,
                   model   : torch.nn.Module,
                   params_current : torch.Tensor,
                   H : dict)-&gt;torch.Tensor:
    &#34;&#34;&#34;
    Evalute server gradient via analyzing local shifts from the clients.

    Args:
            clients_responses (Buffer): client responses. Each client transfers at least:
            &#39;model&#39; field in their response with last iterate for local model
            &#39;optimiser&#39; state of the local optimizer for optimizers with state
            &#39;client_id&#39; id of the client
            &#39;client_state&#39; state of the client

        clients(int): number of clients in that communication round. Invariant len(clients_responses) == clients
        model(torch.nn.Module): model which is locating currently in need server position.
        params_current(torch.Tensor): position where currently model is locating
        H(dict): server state

    Returns:
        Flat 1D SGD vector
    &#34;&#34;&#34;
    if clients == 0:
        return torch.zeros_like(params_current)

    classImpl = getImplClassForAlgo(H[&#34;algorithm&#34;])
    gs = classImpl.serverGradient(clients_responses, clients, model, params_current, H)

    # Need change for global optimizer for some time was gs=-gs. Currently no need.

    return gs

def theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses: utils.buffer.Buffer, use_steps_size_for_non_convex_case:bool):
    &#34;&#34;&#34;
    Experimental method to evaluate theoretical step-size for non-convex L-smooth case

    Args:
        x_cur(torch.Tensor): current iterate
        grad_server(torch.Tensor): global gradient for which we&#39;re finding theoretical step-size
        H(dict): server state
        clients_in_round(int): number of clients particiapted in that round
        clients_responses: responses from clients

    Returns:
        Step size
    &#34;&#34;&#34;
    classImpl = getImplClassForAlgo(H[&#34;algorithm&#34;])
    step_size = classImpl.theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case)

    logger = Logger.get(H[&#34;args&#34;].run_id)
    logger.info(f&#39;Computed step size for H[&#34;algorithm&#34;] is {step_size}&#39;)

    return step_size

def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, model:torch.nn.Module, paramsPrev:torch.Tensor, round:int, grad_server:torch.Tensor, H:dict, numClients:int, sampled_clients_per_next_round)-&gt;dict:
    &#34;&#34;&#34;
    Server global state update.

    Default update - include any states from previous round, but excluding model parameters, which maybe huge.

    Args:
        clients_responses (Buffer): client responses. Each client transfers at least:
        model(torch.nn.Module): model for sever, initialized with last position.
        paramsPrev(torch.Tensor): previous model parameters at the begininng of round
        round(int): number of the round
        grad_server(torch.Tensor): server&#39;s gradient.
        H(dict): server state
        numClients(int): number of clients in that round
        sampled_clients_per_next_round: future clients in a next round

    Returns:
        New server state.
    &#34;&#34;&#34;
    clients = {}

    while len(clients_responses) &lt; numClients:
        pass

    fvalues = []

    # Prune after communication round
    for item in clients_responses.items:
        assert item is not None
        # if item is None:
        #    continue

        item[&#39;client_state&#39;].update({&#34;optimiser&#34; : item[&#39;optimiser&#39;]})
        item[&#39;client_state&#39;].update({&#34;buffers&#34; : item[&#39;buffers&#39;]})

        del item[&#39;optimiser&#39;]
        del item[&#39;buffers&#39;]

        del item[&#39;model&#39;]

        if &#39;H&#39; in item[&#39;client_state&#39;]:
            del item[&#39;client_state&#39;][&#39;H&#39;]

        if &#39;client_compressor&#39; in item[&#39;client_state&#39;]:
            del item[&#39;client_state&#39;][&#39;client_compressor&#39;]

        clients[item[&#39;client_id&#39;]] = item
        fvalues += item[&#39;client_state&#39;][&#39;approximate_f_value&#39;]

        # Remove sampled indicies in case of experiments with another SGD estimators inside optimization algorithms
        if &#39;iterated-minibatch-indicies&#39; in item[&#39;client_state&#39;]:
            del item[&#39;client_state&#39;][&#39;iterated-minibatch-indicies&#39;]

        #if &#39;iterated-minibatch-indicies-full&#39; in item[&#39;client_state&#39;]:
        #    del item[&#39;client_state&#39;][&#39;iterated-minibatch-indicies-full&#39;]

    # This place and serverGlobalStateUpdate() are only place where H[&#39;history&#39;] can be updated
    assert round not in H[&#39;history&#39;]

    fRunAvg = np.nan
    if len(fvalues) &gt; 0:
        fRunAvg = np.mean(fvalues)

    H[&#39;history&#39;][round] = {}
    H[&#39;history&#39;][round].update({ &#34;client_states&#34; : clients,
                                  &#34;grad_sgd_server_l2&#34; : mutils.l2_norm_of_vec(grad_server),
                                  &#34;approximate_f_avg_value&#34; : fRunAvg,
                                  &#34;x_before_round&#34; : mutils.l2_norm_of_vec(paramsPrev)
                               })

    if has_experiment_option(H, &#34;track_distance_to_solution&#34;):
        xSolutionFileName = H[&#34;execution_context&#34;].experimental_options[&#34;x_solution&#34;]
        if &#34;used_x_solution&#34; not in H:
            with open(xSolutionFileName, &#34;rb&#34;) as f:
                import pickle
                obj = pickle.load(f)
                assert len(obj) == 1
                Hsol = obj[0][1]
                used_x_solution = Hsol[&#39;xfinal&#39;]
                H[&#34;used_x_solution&#34;] = used_x_solution.to(dtype = paramsPrev.dtype)

        H[&#39;history&#39;][round][&#34;distance_to_solution&#34;] = mutils.l2_norm_of_vec(paramsPrev - H[&#34;used_x_solution&#34;].to(paramsPrev.device))

    classImpl = getImplClassForAlgo(H[&#34;algorithm&#34;])
    Hnew = classImpl.serverGlobalStateUpdate(clients_responses, clients, model, paramsPrev, grad_server, H)

    # Update server state
    Hnew.update( {&#39;server_state_update_time&#39; : time.strftime(&#34;%d %b %Y %H:%M:%S UTC%z&#34;, time.localtime())} )

    # Move various shifts client information into CPU
    move_client_state_to_host = Hnew[&#39;args&#39;].store_client_state_in_cpu

    if move_client_state_to_host:
        client_states = Hnew[&#39;history&#39;][round][&#34;client_states&#34;]
        for client_id in client_states:
            # Client will be sampled in a next round. It&#39;s not worthwhile to copy state to CPU, because in next round we will need back it to GPU (TODO: Maybe still better to move to CPU)
            # if client_id in sampled_clients_per_next_round:
            #    continue

            cs = client_states[client_id][&#39;client_state&#39;]
            for k,v in cs.items():
                if torch.is_tensor(v):
                    if v.device != &#39;cpu&#39;:
                        v = v.cpu()
                    client_states[client_id][&#39;client_state&#39;][k] = v

    return  Hnew
# ======================================================================================================================</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="utils.algorithms.clientState"><code class="name flex">
<span>def <span class="ident">clientState</span></span>(<span>H: dict, clientId: int, client_data_samples: int, round: int, device: str) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize client state.</p>
<p>Clientstate is initialized from the scratch at each round for each selected client which is participates in optimization.
If you want to initilize client via using it's previous state please use findRecentRecord() to find need information in server state.</p>
<p>clientstate is a second source of information which helps to operate for clients
- 'H' : referece to server state
- 'algorithm': string represenation of the used algorithms
- 'client_id': id of the client
- 'device': target device for the client
- 'round': number of round in which this state has been used
- 'weight': weight used in the aggregation in serverGradient
- 'stats': different statistics for a single client
- 'seed': custom seed for pseudo-random generator for a client</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>H</code></strong> :&ensp;<code>dict</code></dt>
<dd>Server state</dd>
</dl>
<p>clientId(int): Id of the client
client_data_samples(int): Number of data points for client
round(int): Number of the round
device(str): Target device which should be used by the client for computations and store client state</p>
<h2 id="returns">Returns</h2>
<p>Initialized client state</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clientState(H:dict, clientId:int, client_data_samples:int, round:int, device:str)-&gt;dict:
    &#34;&#34;&#34;
    Initialize client state.

    Clientstate is initialized from the scratch at each round for each selected client which is participates in optimization.
    If you want to initilize client via using it&#39;s previous state please use findRecentRecord() to find need information in server state.

     clientstate is a second source of information which helps to operate for clients
     - &#39;H&#39; : referece to server state
     - &#39;algorithm&#39;: string represenation of the used algorithms
     - &#39;client_id&#39;: id of the client
     - &#39;device&#39;: target device for the client
     - &#39;round&#39;: number of round in which this state has been used
     - &#39;weight&#39;: weight used in the aggregation in serverGradient
     - &#39;stats&#39;: different statistics for a single client
     - &#39;seed&#39;: custom seed for pseudo-random generator for a client

    Args:
        H (dict): Server state
        clientId(int): Id of the client
        client_data_samples(int): Number of data points for client
        round(int): Number of the round
        device(str): Target device which should be used by the client for computations and store client state

    Returns:
        Initialized client state
    &#34;&#34;&#34;
    classImpl = getImplClassForAlgo(H[&#34;algorithm&#34;])

    clientState = classImpl.clientState(H, clientId, client_data_samples, device)
    if &#39;weight&#39; not in clientState:
        clientState.update( {&#39;weight&#39; : 1.0} )


    clientState.update( {&#39;H&#39;         : H,
                         &#39;algorithm&#39; : H[&#34;algorithm&#34;],
                         &#39;client_id&#39; : clientId,
                         &#39;device&#39;    : device,
                         &#39;weight&#39;    : 1.0,
                         &#39;round&#39;     : round,
                         &#39;approximate_f_value&#39; : [],
                         &#39;seed&#39; :   H[&#39;execution_context&#39;].np_random.randint(2**31)
                         }
                      )

    stats = {&#34;dataload_duration&#34;  : 0.0,
             &#34;inference_duration&#34; : 0.0,
             &#34;backprop_duration&#34;  : 0.0,
             &#34;full_gradient_oracles&#34;    : 0,
             &#34;samples_gradient_oracles&#34; : 0,
             &#34;send_scalars_to_master&#34;   : 0
             }

    clientState.update({&#34;stats&#34; : stats})

    return clientState</code></pre>
</details>
</dd>
<dt id="utils.algorithms.evaluateFunction"><code class="name flex">
<span>def <span class="ident">evaluateFunction</span></span>(<span>client_state, model, dataloader, criterion, is_rnn, update_statistics=True, device=None, args=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Evalute gradient for model at current point and optionally update statistics and return loss value at current point.</p>
<h2 id="args">Args</h2>
<dl>
<dt>client_state(dict): information about client. used information - statistics, used device</dt>
<dt>model(torch.nn.Module): used model for which trainable variables we will evaluate full gradient</dt>
<dt><strong><code>dataloader</code></strong></dt>
<dd>used dataloader for fetch records for evaluate local loss during training</dd>
<dt><strong><code>criterion</code></strong></dt>
<dd>used criteria with setuped reduction as sum. After evalute reduction correct scaling is a part of evaluation</dd>
</dl>
<p>is_rnn(bool): flag which specofy that what we evaluate is rnn
update_statistics(bool): update the following statistics - dataload_duration, inference_duration</p>
<h2 id="returns">Returns</h2>
<p>Scalar with local ERM value</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluateFunction(client_state, model, dataloader, criterion, is_rnn, update_statistics = True, device = None, args = None):
    &#34;&#34;&#34;
    Evalute gradient for model at current point and optionally update statistics and return loss value at current point.

    Args:
        client_state(dict): information about client. used information - statistics, used device
        model(torch.nn.Module): used model for which trainable variables we will evaluate full gradient
        dataloader: used dataloader for fetch records for evaluate local loss during training
        criterion: used criteria with setuped reduction as sum. After evalute reduction correct scaling is a part of evaluation
        is_rnn(bool): flag which specofy that what we evaluate is rnn
        update_statistics(bool): update the following statistics - dataload_duration, inference_duration

    Returns:
        Scalar with local ERM value
    &#34;&#34;&#34;
    model.train(False)

    if device is None:
        device = client_state[&#34;device&#34;]

    total_number_of_samples = len(dataloader.dataset)
    total_loss = torch.Tensor([0.0]).to(device = device, dtype = model.fl_dtype)

    # code wrap that stops autograd from tracking tensor 
    with torch.no_grad():
        for i, (data, label) in enumerate(dataloader):
            start_ts = time.time()
            batch_size = data.shape[0]
            if str(data.device) != device or str(label.device) != device or data.dtype != model.fl_dtype or label.dtype != model.fl_dtype:
                data, label = data.to(device=device, dtype=model.fl_dtype), label.to(device=device,dtype=model.fl_dtype)

            if update_statistics:
                client_state[&#34;stats&#34;][&#34;dataload_duration&#34;] += (time.time() - start_ts)

            input, label = model_funcs.get_train_inputs(data, label, model, batch_size, device, is_rnn)

            start_ts = time.time()
            outputs = model(*input)

            if not is_rnn:
                hidden = None
                output = outputs
            else:
                output, hidden = outputs

            loss = model_funcs.compute_loss(model, criterion, output, label)
            loss = loss * (1.0/total_number_of_samples)

            if update_statistics:
                client_state[&#34;stats&#34;][&#34;inference_duration&#34;] += (time.time() - start_ts)
            total_loss += loss

    if args is None:
        args = client_state[&#34;H&#34;][&#34;args&#34;]

    regulizer_global = model_funcs.get_global_regulizer(args.global_regulizer)
    R = regulizer_global(model, args.global_regulizer_alpha)

    Rvalue = 0.0
    if R is not None:
        Rvalue = R.item()

    return total_loss.item() + Rvalue</code></pre>
</details>
</dd>
<dt id="utils.algorithms.evaluateGradient"><code class="name flex">
<span>def <span class="ident">evaluateGradient</span></span>(<span>client_state, model, dataloader, criterion, is_rnn, update_statistics=True, evaluate_function=False, device=None, args=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Evalute gradient for model at current point and optionally update statistics and return loss value at current point.</p>
<h2 id="args">Args</h2>
<dl>
<dt>client_state(dict): information about client. used information - statistics, used device</dt>
<dt>model(torch.nn.Module): used model for which trainable variables we will evaluate full gradient</dt>
<dt><strong><code>dataloader</code></strong></dt>
<dd>used dataloader for fetch records for evaluate local loss during training</dd>
<dt><strong><code>criterion</code></strong></dt>
<dd>used criteria with setuped reduction as sum. After evalute reduction correct scaling is a part of evaluation</dd>
</dl>
<p>is_rnn(bool): flag which specofy that what we evaluate is rnn
update_statistics(bool): update the following statistics - full_gradient_oracles, samples_gradient_oracles, dataload_duration, inference_duration, backprop_duration
evaluate_function(bool): if true then returned value is CPU scalar which describes loss function value</p>
<h2 id="returns">Returns</h2>
<p>If evaluate_function is True then sclar with local ERM value</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluateGradient(client_state, model, dataloader, criterion, is_rnn, update_statistics = True, evaluate_function = False, device = None, args = None):
    &#34;&#34;&#34;
    Evalute gradient for model at current point and optionally update statistics and return loss value at current point.

    Args:
        client_state(dict): information about client. used information - statistics, used device
        model(torch.nn.Module): used model for which trainable variables we will evaluate full gradient
        dataloader: used dataloader for fetch records for evaluate local loss during training
        criterion: used criteria with setuped reduction as sum. After evalute reduction correct scaling is a part of evaluation
        is_rnn(bool): flag which specofy that what we evaluate is rnn
        update_statistics(bool): update the following statistics - full_gradient_oracles, samples_gradient_oracles, dataload_duration, inference_duration, backprop_duration
        evaluate_function(bool): if true then returned value is CPU scalar which describes loss function value

    Returns:
        If evaluate_function is True then sclar with local ERM value
    &#34;&#34;&#34;
    model.train(True)
    if model.do_not_use_bn_and_dropout:
        mutils.turn_off_batch_normalization_and_dropout(model)

    if update_statistics:
        client_state[&#39;stats&#39;][&#39;full_gradient_oracles&#39;] += 1
        client_state[&#39;stats&#39;][&#39;samples_gradient_oracles&#39;] += len(dataloader.dataset)

    # Zero out previous gradient
    for p in model.parameters():
        p.grad = None

    if device is None:
        device = client_state[&#34;device&#34;]

    total_number_of_samples = len(dataloader.dataset)
    function_value = None

    if evaluate_function:
        function_value = torch.Tensor([0.0]).to(device = device, dtype = model.fl_dtype)

    for i, (data, label) in enumerate(dataloader):
        start_ts = time.time()
        batch_size = data.shape[0]
        if str(data.device) != device or str(label.device) != device or data.dtype != model.fl_dtype or label.dtype != model.fl_dtype:
            data, label = data.to(device = device, dtype = model.fl_dtype), label.to(device = device, dtype = model.fl_dtype)

        if update_statistics:
            client_state[&#34;stats&#34;][&#34;dataload_duration&#34;] += (time.time() - start_ts)

        input, label = model_funcs.get_train_inputs(data, label, model, batch_size, device, is_rnn)

        start_ts = time.time()
        outputs = model(*input)

        if not is_rnn:
            hidden = None
            output = outputs
        else:
            output, hidden = outputs

        loss = model_funcs.compute_loss(model, criterion, output, label)
        loss = loss * (1.0/total_number_of_samples)

        if evaluate_function:
            function_value += loss

        if update_statistics:
            client_state[&#34;stats&#34;][&#34;inference_duration&#34;] += (time.time() - start_ts)

        start_ts = time.time()
        loss.backward()

        if update_statistics:
            client_state[&#34;stats&#34;][&#34;backprop_duration&#34;] += (time.time() - start_ts)

    if args is None:
        args = client_state[&#34;H&#34;][&#34;args&#34;]

    regulizer_global = model_funcs.get_global_regulizer(args.global_regulizer)
    R = regulizer_global(model, args.global_regulizer_alpha)

    Rvalue = 0.0
    if R is not None:
        R.backward()
        Rvalue = R.item()

    if evaluate_function:
        return function_value.item() + Rvalue
    else:
        return None</code></pre>
</details>
</dd>
<dt id="utils.algorithms.evaluateSgd"><code class="name flex">
<span>def <span class="ident">evaluateSgd</span></span>(<span>client_state, model, dataloader, criterion, is_rnn, update_statistics=True, evaluate_function=False, device=None, args=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Evalute gradient estimator with using global context for model at current point and optionally update statistics and return loss value at current point.</p>
<h2 id="args">Args</h2>
<dl>
<dt>client_state(dict): information about client. used information - statistics, used device</dt>
<dt>model(torch.nn.Module): used model for which trainable variables we will evaluate full gradient</dt>
<dt><strong><code>dataloader</code></strong></dt>
<dd>used dataloader for fetch records for evaluate local loss during training</dd>
<dt><strong><code>criterion</code></strong></dt>
<dd>used criteria with setuped reduction as sum. After evalute reduction correct scaling is a part of evaluation</dd>
</dl>
<p>is_rnn(bool): flag which specofy that what we evaluate is rnn
update_statistics(bool): update the following statistics - full_gradient_oracles, samples_gradient_oracles, dataload_duration, inference_duration, backprop_duration
evaluate_function(bool): if true then returned value is CPU scalar which describes loss function value</p>
<h2 id="returns">Returns</h2>
<p>If evaluate_function is True then sclar with local ERM value</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluateSgd(client_state, model, dataloader, criterion, is_rnn, update_statistics = True, evaluate_function = False, device = None, args = None):
    &#34;&#34;&#34;
    Evalute gradient estimator with using global context for model at current point and optionally update statistics and return loss value at current point.

    Args:
        client_state(dict): information about client. used information - statistics, used device
        model(torch.nn.Module): used model for which trainable variables we will evaluate full gradient
        dataloader: used dataloader for fetch records for evaluate local loss during training
        criterion: used criteria with setuped reduction as sum. After evalute reduction correct scaling is a part of evaluation
        is_rnn(bool): flag which specofy that what we evaluate is rnn
        update_statistics(bool): update the following statistics - full_gradient_oracles, samples_gradient_oracles, dataload_duration, inference_duration, backprop_duration
        evaluate_function(bool): if true then returned value is CPU scalar which describes loss function value

    Returns:
        If evaluate_function is True then sclar with local ERM value
    &#34;&#34;&#34;
    exec_ctx = client_state[&#39;H&#39;][&#34;execution_context&#34;]

    if &#34;internal_sgd&#34; not in exec_ctx.experimental_options:
        return evaluateGradient(client_state, model, dataloader, criterion, is_rnn, update_statistics=update_statistics, evaluate_function=evaluate_function, device=device)

    internal_sgd = exec_ctx.experimental_options[&#39;internal_sgd&#39;]
    if internal_sgd == &#39;full-gradient&#39;:
        return evaluateGradient(client_state, model, dataloader, criterion, is_rnn, update_statistics=update_statistics, evaluate_function=evaluate_function, device=device)

    model.train(True)
    if model.do_not_use_bn_and_dropout:
        mutils.turn_off_batch_normalization_and_dropout(model)

    # Zero out previous gradient
    for p in model.parameters():
        p.grad = None

    if device is None:
        device = client_state[&#34;device&#34;]

    total_number_of_samples = len(dataloader.dataset)
    function_value = None

    if evaluate_function:
        function_value = torch.Tensor([0.0]).to(device = device, dtype = model.fl_dtype)

    # ==================================================================================================================
    indicies = None
    if internal_sgd == &#34;sgd-nice&#34; or internal_sgd == &#39;sgd-us&#39; or internal_sgd == &#39;iterated-minibatch&#39; or internal_sgd == &#39;sgd-multi&#39;:
        indicies = client_state[&#39;iterated-minibatch-indicies&#39;]
        indicies = torch.from_numpy(indicies)
    # ==================================================================================================================
    batch_size_ds = dataloader.batch_size
    iterations = math.ceil(len(indicies) / float(batch_size_ds))

    sampled_samples = len(indicies)

    for i in range(iterations):
        data = []
        label = []
        for j in range(batch_size_ds):
            index = i * batch_size_ds + j
            if index &gt;= sampled_samples:
                break

            d, t = dataloader.dataset[indicies[index]]

            data.append(d.unsqueeze(0))

            if not torch.is_tensor(t):
                if type(criterion) is torch.nn.MSELoss:
                    label.append(torch.Tensor([t]))
                else:
                    label.append(torch.LongTensor([t]))
            else:
                label.append(t.unsqueeze(0))

        data = torch.cat(data)
        label = torch.cat(label)

        start_ts = time.time()
        batch_size = data.shape[0]
        if str(data.device) != device or str(label.device) != device or data.dtype != model.fl_dtype or label.dtype != model.fl_dtype:
            data, label = data.to(device = device, dtype = model.fl_dtype), label.to(device = device, dtype = model.fl_dtype)

        if update_statistics:
            client_state[&#34;stats&#34;][&#34;dataload_duration&#34;] += (time.time() - start_ts)

        if update_statistics:
            client_state[&#39;stats&#39;][&#39;full_gradient_oracles&#39;] += float(batch_size) / total_number_of_samples
            client_state[&#39;stats&#39;][&#39;samples_gradient_oracles&#39;] += batch_size

        input, label = model_funcs.get_train_inputs(data, label, model, batch_size, device, is_rnn)

        start_ts = time.time()
        outputs = model(*input)

        if not is_rnn:
            hidden = None
            output = outputs
        else:
            output, hidden = outputs

        loss = model_funcs.compute_loss(model, criterion, output, label)
        loss = loss * (1.0/sampled_samples)

        if evaluate_function:
            function_value += loss

        if update_statistics:
            client_state[&#34;stats&#34;][&#34;inference_duration&#34;] += (time.time() - start_ts)

        start_ts = time.time()
        loss.backward()

        if update_statistics:
            client_state[&#34;stats&#34;][&#34;backprop_duration&#34;] += (time.time() - start_ts)

    if args is None:
        args = client_state[&#34;H&#34;][&#34;args&#34;]

    regulizer_global = model_funcs.get_global_regulizer(args.global_regulizer)
    R = regulizer_global(model, args.global_regulizer_alpha)

    Rvalue = 0.0
    if R is not None:
        R.backward()
        Rvalue = R.item()

    if evaluate_function:
        return function_value.item() + Rvalue
    else:
        return None</code></pre>
</details>
</dd>
<dt id="utils.algorithms.findRecentRecord"><code class="name flex">
<span>def <span class="ident">findRecentRecord</span></span>(<span>H, client_id, field)</span>
</code></dt>
<dd>
<div class="desc"><p>Find in history records recent information about query record in client_states.</p>
<h2 id="args">Args</h2>
<p>H(dict): information about client. used information - statistics, used device
client_id(int): integer number for client
field(str): name of the field which we are trying to find history</p>
<h2 id="returns">Returns</h2>
<p>Return requested object if it found or None if object is not found</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def findRecentRecord(H, client_id, field):
    &#34;&#34;&#34;
    Find in history records recent information about query record in client_states.

    Args:
        H(dict): information about client. used information - statistics, used device
        client_id(int): integer number for client
        field(str): name of the field which we are trying to find history

    Returns:
        Return requested object if it found or None if object is not found
    &#34;&#34;&#34;
    history = H[&#39;history&#39;]

    history_keys = [k for k in history.keys()]
    history_keys.sort(reverse=True)

    for r in history_keys:
        clients_history = history[r]
        if client_id in clients_history[&#39;client_states&#39;]:
            client_prev_state = clients_history[&#39;client_states&#39;][client_id][&#39;client_state&#39;]
            if field in client_prev_state:
                return_object = client_prev_state[field]
                return return_object

            else:
                # Assumption -- if client has been sampled then field have to be setuped
                return None
    return None</code></pre>
</details>
</dd>
<dt id="utils.algorithms.findRecentRecordAndRemoveFromHistory"><code class="name flex">
<span>def <span class="ident">findRecentRecordAndRemoveFromHistory</span></span>(<span>H, client_id, field)</span>
</code></dt>
<dd>
<div class="desc"><p>Find in history records recent information about query record in client_states.
If record has been found return it, but before that remove itself from history.</p>
<h2 id="args">Args</h2>
<p>H(dict): information about client. used information - statistics, used device
client_id(int): integer number for client
field(str): name of the field which we are trying to find history</p>
<h2 id="returns">Returns</h2>
<p>Return requested object if it found or None if object is not found</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def findRecentRecordAndRemoveFromHistory(H, client_id, field):
    &#34;&#34;&#34;
    Find in history records recent information about query record in client_states.
    If record has been found return it, but before that remove itself from history.

    Args:
        H(dict): information about client. used information - statistics, used device
        client_id(int): integer number for client
        field(str): name of the field which we are trying to find history

    Returns:
        Return requested object if it found or None if object is not found
    &#34;&#34;&#34;
    history = H[&#39;history&#39;]
    history_keys = [k for k in history.keys()]
    history_keys.sort(reverse=True)

    for r in history_keys:
        clients_history = history[r]
        if client_id in clients_history[&#39;client_states&#39;]:
            client_prev_state = clients_history[&#39;client_states&#39;][client_id][&#39;client_state&#39;]
            if field in client_prev_state:
                return_object = client_prev_state[field]
                client_prev_state[field] = None
                return return_object
            else:
                # Assumption -- if client has been sampled then field have to be setuped
                return None
    return None</code></pre>
</details>
</dd>
<dt id="utils.algorithms.getAlgorithmsList"><code class="name flex">
<span>def <span class="ident">getAlgorithmsList</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Get list of algorithms in order in which sorting happens in GUI</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def getAlgorithmsList():
    &#34;&#34;&#34;
    Get list of algorithms in order in which sorting happens in GUI
    &#34;&#34;&#34;
    algoList = [&#34;marina&#34;, &#34;dcgd&#34;, &#34;cofig&#34;, &#34;frecon&#34;, &#34;fedavg&#34;, &#34;diana&#34;, &#34;scaffold&#34;,
                &#34;fedprox&#34;, &#34;ef21&#34;, &#34;ef21-pp&#34;, &#34;pp-marina&#34;]

    for a in algoList:
        assert getImplClassForAlgo(a) is not None

    return algoList</code></pre>
</details>
</dd>
<dt id="utils.algorithms.getImplClassForAlgo"><code class="name flex">
<span>def <span class="ident">getImplClassForAlgo</span></span>(<span>algorithm)</span>
</code></dt>
<dd>
<div class="desc"><p>Get imlementation class for specific algorithm</p>
<h2 id="args">Args</h2>
<p>algorithm(str): Algorithm implementation</p>
<h2 id="returns">Returns</h2>
<p>class type with need interface methods</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def getImplClassForAlgo(algorithm):
    &#34;&#34;&#34;
    Get imlementation class for specific algorithm

    Args:
        algorithm(str): Algorithm implementation

    Returns:
        class type with need interface methods
    &#34;&#34;&#34;
    classImpl = None
    if algorithm == &#34;marina&#34;:
        classImpl = MarinaAlgorithm
    elif algorithm == &#34;dcgd&#34;:
        classImpl = DCGD
    elif algorithm == &#34;fedavg&#34;:
        classImpl = FedAvg
    elif algorithm == &#34;diana&#34;:
        classImpl = DIANA
    elif algorithm == &#34;scaffold&#34;:
        classImpl = SCAFFOLD
    elif algorithm == &#34;fedprox&#34;:
        classImpl = FedProx
    elif algorithm == &#34;ef21&#34;:
        classImpl = EF21
    elif algorithm == &#34;cofig&#34;:
        classImpl = COFIG
    elif algorithm == &#34;frecon&#34;:
        classImpl = FRECON
    elif algorithm == &#34;ef21-pp&#34;:
        classImpl = EF21PP
    elif algorithm == &#34;pp-marina&#34;:
        classImpl = MarinaAlgorithmPP
    else:
        raise ValueError(f&#34;Please check algorithm. There is no implementation for &#39;{algorithm}&#39;&#34;)

    return classImpl</code></pre>
</details>
</dd>
<dt id="utils.algorithms.getLismoothForClients"><code class="name flex">
<span>def <span class="ident">getLismoothForClients</span></span>(<span>H, clients_responses)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def getLismoothForClients(H, clients_responses):
    Li = np.array(H[&#39;Li_all_clients&#39;])

    if H[&#34;args&#34;].global_regulizer == &#34;none&#34;:
        pass
    elif H[&#34;args&#34;].global_regulizer == &#34;cvx_l2norm_square_div_2&#34;:
        Li = Li + (1.0 * H[&#34;args&#34;].global_regulizer_alpha)
    elif H[&#34;args&#34;].global_regulizer == &#34;noncvx_robust_linear_regression&#34;:
        Li = Li + (2.0 * H[&#34;args&#34;].global_regulizer_alpha)

    return Li</code></pre>
</details>
</dd>
<dt id="utils.algorithms.getLsmoothGlobal"><code class="name flex">
<span>def <span class="ident">getLsmoothGlobal</span></span>(<span>H, clients_responses)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def getLsmoothGlobal(H, clients_responses):
    L = H[&#39;L_compute&#39;]

    if H[&#34;args&#34;].global_regulizer == &#34;none&#34;:
        pass
    elif H[&#34;args&#34;].global_regulizer == &#34;cvx_l2norm_square_div_2&#34;:
        L = L + (1.0 * H[&#34;args&#34;].global_regulizer_alpha)
    elif H[&#34;args&#34;].global_regulizer == &#34;noncvx_robust_linear_regression&#34;:
        L = L + (2.0 * H[&#34;args&#34;].global_regulizer_alpha)

    return L</code></pre>
</details>
</dd>
<dt id="utils.algorithms.get_experiment_option_f"><code class="name flex">
<span>def <span class="ident">get_experiment_option_f</span></span>(<span>H, name)</span>
</code></dt>
<dd>
<div class="desc"><p>Get experimental option to carry experiments with algorithms</p>
<h2 id="args">Args</h2>
<p>H(dict): server state
name(str): variable name</p>
<h2 id="returns">Returns</h2>
<p>Value of requested value converted to float</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_experiment_option_f(H, name):
    &#34;&#34;&#34;
    Get experimental option to carry experiments with algorithms

    Args:
        H(dict): server state
        name(str): variable name

    Returns:
        Value of requested value converted to float
    &#34;&#34;&#34;
    return float(H[&#34;execution_context&#34;].experimental_options[name])</code></pre>
</details>
</dd>
<dt id="utils.algorithms.get_experiment_option_int"><code class="name flex">
<span>def <span class="ident">get_experiment_option_int</span></span>(<span>H, name)</span>
</code></dt>
<dd>
<div class="desc"><p>Get experimental option to carry experiments with algorithms</p>
<h2 id="args">Args</h2>
<p>H(dict): server state
name(str): variable name</p>
<h2 id="returns">Returns</h2>
<p>Value of requested value converted to int</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_experiment_option_int(H, name):
    &#34;&#34;&#34;
    Get experimental option to carry experiments with algorithms

    Args:
        H(dict): server state
        name(str): variable name

    Returns:
        Value of requested value converted to int
    &#34;&#34;&#34;
    return int(H[&#34;execution_context&#34;].experimental_options[name])</code></pre>
</details>
</dd>
<dt id="utils.algorithms.get_experiment_option_str"><code class="name flex">
<span>def <span class="ident">get_experiment_option_str</span></span>(<span>H, name)</span>
</code></dt>
<dd>
<div class="desc"><p>Get experimental option to carry experiments with algorithms</p>
<h2 id="args">Args</h2>
<p>H(dict): server state
name(str): variable name</p>
<h2 id="returns">Returns</h2>
<p>Value of requested value converted to string</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_experiment_option_str(H, name):
    &#34;&#34;&#34;
    Get experimental option to carry experiments with algorithms

    Args:
        H(dict): server state
        name(str): variable name

    Returns:
        Value of requested value converted to string
    &#34;&#34;&#34;
    return str(H[&#34;execution_context&#34;].experimental_options[name])</code></pre>
</details>
</dd>
<dt id="utils.algorithms.get_initial_shift"><code class="name flex">
<span>def <span class="ident">get_initial_shift</span></span>(<span>args: argparse.Namespace, D: int, grad_start: torch.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Help method to get initial shifts</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_initial_shift(args:argparse.Namespace, D:int, grad_start:torch.Tensor):
    &#34;&#34;&#34;Help method to get initial shifts&#34;&#34;&#34;
    if args.initialize_shifts_policy == &#34;full_gradient_at_start&#34;:
        return grad_start.detach().clone().to(device = args.device)
    else:
        return torch.zeros(D).to(device = args.device, dtype = grad_start.dtype)</code></pre>
</details>
</dd>
<dt id="utils.algorithms.get_logger"><code class="name flex">
<span>def <span class="ident">get_logger</span></span>(<span>H)</span>
</code></dt>
<dd>
<div class="desc"><p>Help function to get logger.</p>
<h2 id="args">Args</h2>
<p>H(dict): server state</p>
<h2 id="returns">Returns</h2>
<p>Reference to logger</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_logger(H):
    &#34;&#34;&#34;
    Help function to get logger.

    Args:
        H(dict): server state

    Returns:
        Reference to logger
    &#34;&#34;&#34;

    my_logger = Logger.get(H[&#34;args&#34;].run_id)
    return my_logger</code></pre>
</details>
</dd>
<dt id="utils.algorithms.has_experiment_option"><code class="name flex">
<span>def <span class="ident">has_experiment_option</span></span>(<span>H, name)</span>
</code></dt>
<dd>
<div class="desc"><p>Check that experimental option is presented</p>
<h2 id="args">Args</h2>
<p>H(dict): server state
name(str): variable name</p>
<h2 id="returns">Returns</h2>
<p>True if option is present</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def has_experiment_option(H, name):
    &#34;&#34;&#34;
    Check that experimental option is presented

    Args:
        H(dict): server state
        name(str): variable name

    Returns:
        True if option is present
    &#34;&#34;&#34;
    return name in H[&#34;execution_context&#34;].experimental_options</code></pre>
</details>
</dd>
<dt id="utils.algorithms.initializeServerState"><code class="name flex">
<span>def <span class="ident">initializeServerState</span></span>(<span>args: argparse.Namespace, model: torch.nn.modules.module.Module, total_clients: int, grad_start: torch.Tensor, exec_ctx) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize server state.</p>
<p>Server state is a main source of information with various information, including:
- 'x0' : start iterate. Be default it's a current position where model is
- 'algorithm': string represenation of the used algorithms
- 'history': history by rounds</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>args</code></strong> :&ensp;<code>argparse</code></dt>
<dd>Parsed command line for the python shell process</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>nn.Module</code></dt>
<dd>Model under which server operate</dd>
<dt><strong><code>total_clients</code></strong> :&ensp;<code>int</code></dt>
<dd>Total number of clients in the experiment</dd>
<dt><strong><code>grad_start</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Full gradient at starting point</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Returns initialize server state for specific algorithms args.algorithm</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, total_clients:int, grad_start: torch.Tensor, exec_ctx) -&gt; dict:
    &#34;&#34;&#34;
    Initialize server state.

    Server state is a main source of information with various information, including:
     - &#39;x0&#39; : start iterate. Be default it&#39;s a current position where model is
     - &#39;algorithm&#39;: string represenation of the used algorithms
     - &#39;history&#39;: history by rounds

    Args:
        args (argparse): Parsed command line for the python shell process
        model (nn.Module): Model under which server operate
        total_clients (int): Total number of clients in the experiment
        grad_start (torch.Tensor): Full gradient at starting point

    Returns:
        Returns initialize server state for specific algorithms args.algorithm
    &#34;&#34;&#34;
    serverState = {}
    classImpl = getImplClassForAlgo(args.algorithm)

    D = mutils.number_of_params(model)
    serverState = classImpl.initializeServerState(args, model, D, total_clients, grad_start)

    serverState.update( {&#39;algorithm&#39; : args.algorithm,
                         &#39;history&#39; : {},
                         &#39;D&#39; : D,
                         &#39;fl_dtype&#39; : model.fl_dtype,
                         &#39;D_include_frozen&#39; : mutils.number_of_params(model, skipFrozen = False),
                         &#39;current_round&#39; : 0,
                         &#39;client_compressor&#39; : args.client_compressor,
                         &#39;run_id&#39; : args.run_id,
                         &#39;start_time&#39;: time.strftime(&#34;%d %b %Y %H:%M:%S UTC%z&#34;, time.localtime()),
                         &#39;server_state_update_time&#39;: time.strftime(&#34;%d %b %Y %H:%M:%S UTC%z&#34;, time.localtime())
                         } )

    if &#39;x0&#39; not in serverState:
        serverState.update( {&#39;x0&#39; : mutils.get_params(model) } )

    return serverState</code></pre>
</details>
</dd>
<dt id="utils.algorithms.localGradientEvaluation"><code class="name flex">
<span>def <span class="ident">localGradientEvaluation</span></span>(<span>client_state: dict, model: torch.nn.modules.module.Module, dataloader: torch.utils.data.dataloader.DataLoader, criterion: torch.nn.modules.loss._Loss, is_rnn: bool, local_iteration_number: tuple) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Evalute local gradient for client.</p>
<p>This API should implement optimization schema specific SGD estimator.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>client_state</code></strong> :&ensp;<code>dict</code></dt>
<dd>state of the client which evaluate local gradient</dd>
</dl>
<p>model(nn.Module): Initialized computation graph(model) locating currently in the interesting "x" position
dataloader(torch.utils.data.dataloader.DataLoader): DataLoader mechanism to fectch data
criterion(class): Loss function for minimization, defined as a "summ" of loss over train samples.
is_rnn(bool): boolean flag which say that model is RNN</p>
<h2 id="returns">Returns</h2>
<p>Flat 1D SGD vector</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def localGradientEvaluation(client_state:dict, model:torch.nn.Module, dataloader:torch.utils.data.dataloader.DataLoader, criterion:torch.nn.modules.loss._Loss, is_rnn:bool, local_iteration_number:tuple)-&gt;torch.Tensor:
    &#34;&#34;&#34;
    Evalute local gradient for client.

    This API should implement optimization schema specific SGD estimator.

    Args:
        client_state (dict): state of the client which evaluate local gradient
        model(nn.Module): Initialized computation graph(model) locating currently in the interesting &#34;x&#34; position
        dataloader(torch.utils.data.dataloader.DataLoader): DataLoader mechanism to fectch data
        criterion(class): Loss function for minimization, defined as a &#34;summ&#34; of loss over train samples.
        is_rnn(bool): boolean flag which say that model is RNN

    Returns:
        Flat 1D SGD vector
    &#34;&#34;&#34;
    classImpl = getImplClassForAlgo(client_state[&#34;algorithm&#34;])
    return classImpl.localGradientEvaluation(client_state, model, dataloader, criterion, is_rnn, local_iteration_number)</code></pre>
</details>
</dd>
<dt id="utils.algorithms.serverGlobalStateUpdate"><code class="name flex">
<span>def <span class="ident">serverGlobalStateUpdate</span></span>(<span>clients_responses: utils.buffer.Buffer, model: torch.nn.modules.module.Module, paramsPrev: torch.Tensor, round: int, grad_server: torch.Tensor, H: dict, numClients: int, sampled_clients_per_next_round) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Server global state update.</p>
<p>Default update - include any states from previous round, but excluding model parameters, which maybe huge.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>clients_responses</code></strong> :&ensp;<code>Buffer</code></dt>
<dd>client responses. Each client transfers at least:</dd>
<dt>model(torch.nn.Module): model for sever, initialized with last position.</dt>
<dt>paramsPrev(torch.Tensor): previous model parameters at the begininng of round</dt>
<dt>round(int): number of the round</dt>
<dt>grad_server(torch.Tensor): server's gradient.</dt>
<dt>H(dict): server state</dt>
<dt>numClients(int): number of clients in that round</dt>
<dt><strong><code>sampled_clients_per_next_round</code></strong></dt>
<dd>future clients in a next round</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>New server state.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, model:torch.nn.Module, paramsPrev:torch.Tensor, round:int, grad_server:torch.Tensor, H:dict, numClients:int, sampled_clients_per_next_round)-&gt;dict:
    &#34;&#34;&#34;
    Server global state update.

    Default update - include any states from previous round, but excluding model parameters, which maybe huge.

    Args:
        clients_responses (Buffer): client responses. Each client transfers at least:
        model(torch.nn.Module): model for sever, initialized with last position.
        paramsPrev(torch.Tensor): previous model parameters at the begininng of round
        round(int): number of the round
        grad_server(torch.Tensor): server&#39;s gradient.
        H(dict): server state
        numClients(int): number of clients in that round
        sampled_clients_per_next_round: future clients in a next round

    Returns:
        New server state.
    &#34;&#34;&#34;
    clients = {}

    while len(clients_responses) &lt; numClients:
        pass

    fvalues = []

    # Prune after communication round
    for item in clients_responses.items:
        assert item is not None
        # if item is None:
        #    continue

        item[&#39;client_state&#39;].update({&#34;optimiser&#34; : item[&#39;optimiser&#39;]})
        item[&#39;client_state&#39;].update({&#34;buffers&#34; : item[&#39;buffers&#39;]})

        del item[&#39;optimiser&#39;]
        del item[&#39;buffers&#39;]

        del item[&#39;model&#39;]

        if &#39;H&#39; in item[&#39;client_state&#39;]:
            del item[&#39;client_state&#39;][&#39;H&#39;]

        if &#39;client_compressor&#39; in item[&#39;client_state&#39;]:
            del item[&#39;client_state&#39;][&#39;client_compressor&#39;]

        clients[item[&#39;client_id&#39;]] = item
        fvalues += item[&#39;client_state&#39;][&#39;approximate_f_value&#39;]

        # Remove sampled indicies in case of experiments with another SGD estimators inside optimization algorithms
        if &#39;iterated-minibatch-indicies&#39; in item[&#39;client_state&#39;]:
            del item[&#39;client_state&#39;][&#39;iterated-minibatch-indicies&#39;]

        #if &#39;iterated-minibatch-indicies-full&#39; in item[&#39;client_state&#39;]:
        #    del item[&#39;client_state&#39;][&#39;iterated-minibatch-indicies-full&#39;]

    # This place and serverGlobalStateUpdate() are only place where H[&#39;history&#39;] can be updated
    assert round not in H[&#39;history&#39;]

    fRunAvg = np.nan
    if len(fvalues) &gt; 0:
        fRunAvg = np.mean(fvalues)

    H[&#39;history&#39;][round] = {}
    H[&#39;history&#39;][round].update({ &#34;client_states&#34; : clients,
                                  &#34;grad_sgd_server_l2&#34; : mutils.l2_norm_of_vec(grad_server),
                                  &#34;approximate_f_avg_value&#34; : fRunAvg,
                                  &#34;x_before_round&#34; : mutils.l2_norm_of_vec(paramsPrev)
                               })

    if has_experiment_option(H, &#34;track_distance_to_solution&#34;):
        xSolutionFileName = H[&#34;execution_context&#34;].experimental_options[&#34;x_solution&#34;]
        if &#34;used_x_solution&#34; not in H:
            with open(xSolutionFileName, &#34;rb&#34;) as f:
                import pickle
                obj = pickle.load(f)
                assert len(obj) == 1
                Hsol = obj[0][1]
                used_x_solution = Hsol[&#39;xfinal&#39;]
                H[&#34;used_x_solution&#34;] = used_x_solution.to(dtype = paramsPrev.dtype)

        H[&#39;history&#39;][round][&#34;distance_to_solution&#34;] = mutils.l2_norm_of_vec(paramsPrev - H[&#34;used_x_solution&#34;].to(paramsPrev.device))

    classImpl = getImplClassForAlgo(H[&#34;algorithm&#34;])
    Hnew = classImpl.serverGlobalStateUpdate(clients_responses, clients, model, paramsPrev, grad_server, H)

    # Update server state
    Hnew.update( {&#39;server_state_update_time&#39; : time.strftime(&#34;%d %b %Y %H:%M:%S UTC%z&#34;, time.localtime())} )

    # Move various shifts client information into CPU
    move_client_state_to_host = Hnew[&#39;args&#39;].store_client_state_in_cpu

    if move_client_state_to_host:
        client_states = Hnew[&#39;history&#39;][round][&#34;client_states&#34;]
        for client_id in client_states:
            # Client will be sampled in a next round. It&#39;s not worthwhile to copy state to CPU, because in next round we will need back it to GPU (TODO: Maybe still better to move to CPU)
            # if client_id in sampled_clients_per_next_round:
            #    continue

            cs = client_states[client_id][&#39;client_state&#39;]
            for k,v in cs.items():
                if torch.is_tensor(v):
                    if v.device != &#39;cpu&#39;:
                        v = v.cpu()
                    client_states[client_id][&#39;client_state&#39;][k] = v

    return  Hnew</code></pre>
</details>
</dd>
<dt id="utils.algorithms.serverGradient"><code class="name flex">
<span>def <span class="ident">serverGradient</span></span>(<span>clients_responses: utils.buffer.Buffer, clients: int, model: torch.nn.modules.module.Module, params_current: torch.Tensor, H: dict) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Evalute server gradient via analyzing local shifts from the clients.</p>
<h2 id="args">Args</h2>
<pre><code>clients_responses (Buffer): client responses. Each client transfers at least:
'model' field in their response with last iterate for local model
'optimiser' state of the local optimizer for optimizers with state
'client_id' id of the client
'client_state' state of the client
</code></pre>
<p>clients(int): number of clients in that communication round. Invariant len(clients_responses) == clients
model(torch.nn.Module): model which is locating currently in need server position.
params_current(torch.Tensor): position where currently model is locating
H(dict): server state</p>
<h2 id="returns">Returns</h2>
<p>Flat 1D SGD vector</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def serverGradient(clients_responses : utils.buffer.Buffer,
                   clients : int,
                   model   : torch.nn.Module,
                   params_current : torch.Tensor,
                   H : dict)-&gt;torch.Tensor:
    &#34;&#34;&#34;
    Evalute server gradient via analyzing local shifts from the clients.

    Args:
            clients_responses (Buffer): client responses. Each client transfers at least:
            &#39;model&#39; field in their response with last iterate for local model
            &#39;optimiser&#39; state of the local optimizer for optimizers with state
            &#39;client_id&#39; id of the client
            &#39;client_state&#39; state of the client

        clients(int): number of clients in that communication round. Invariant len(clients_responses) == clients
        model(torch.nn.Module): model which is locating currently in need server position.
        params_current(torch.Tensor): position where currently model is locating
        H(dict): server state

    Returns:
        Flat 1D SGD vector
    &#34;&#34;&#34;
    if clients == 0:
        return torch.zeros_like(params_current)

    classImpl = getImplClassForAlgo(H[&#34;algorithm&#34;])
    gs = classImpl.serverGradient(clients_responses, clients, model, params_current, H)

    # Need change for global optimizer for some time was gs=-gs. Currently no need.

    return gs</code></pre>
</details>
</dd>
<dt id="utils.algorithms.theoreticalStepSize"><code class="name flex">
<span>def <span class="ident">theoreticalStepSize</span></span>(<span>x_cur, grad_server, H, clients_in_round, clients_responses: utils.buffer.Buffer, use_steps_size_for_non_convex_case: bool)</span>
</code></dt>
<dd>
<div class="desc"><p>Experimental method to evaluate theoretical step-size for non-convex L-smooth case</p>
<h2 id="args">Args</h2>
<dl>
<dt>x_cur(torch.Tensor): current iterate</dt>
<dt>grad_server(torch.Tensor): global gradient for which we're finding theoretical step-size</dt>
<dt>H(dict): server state</dt>
<dt>clients_in_round(int): number of clients particiapted in that round</dt>
<dt><strong><code>clients_responses</code></strong></dt>
<dd>responses from clients</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Step size</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses: utils.buffer.Buffer, use_steps_size_for_non_convex_case:bool):
    &#34;&#34;&#34;
    Experimental method to evaluate theoretical step-size for non-convex L-smooth case

    Args:
        x_cur(torch.Tensor): current iterate
        grad_server(torch.Tensor): global gradient for which we&#39;re finding theoretical step-size
        H(dict): server state
        clients_in_round(int): number of clients particiapted in that round
        clients_responses: responses from clients

    Returns:
        Step size
    &#34;&#34;&#34;
    classImpl = getImplClassForAlgo(H[&#34;algorithm&#34;])
    step_size = classImpl.theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case)

    logger = Logger.get(H[&#34;args&#34;].run_id)
    logger.info(f&#39;Computed step size for H[&#34;algorithm&#34;] is {step_size}&#39;)

    return step_size</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="utils.algorithms.COFIG"><code class="flex name class">
<span>class <span class="ident">COFIG</span></span>
</code></dt>
<dd>
<div class="desc"><p>COFIG Algoritm [Haoyu Zhao et al., 2021]: <a href="https://arxiv.org/abs/2112.13097">https://arxiv.org/abs/2112.13097</a>
Assumption: \widetilda{S} = S, i.e. they are the same sets</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class COFIG:
    &#39;&#39;&#39;
    COFIG Algoritm [Haoyu Zhao et al., 2021]: https://arxiv.org/abs/2112.13097
    Assumption: \widetilda{S} = S, i.e. they are the same sets
    &#39;&#39;&#39;

    @staticmethod
    def theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case):
        # Step size for non-convex case
        S = clients_in_round
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])

        w = compressor.getW()
        a = 1 / (1.0 + w)

        if use_steps_size_for_non_convex_case:
            Li_all_clients = getLismoothForClients(H, clients_responses)
            Lmax = max(Li_all_clients)
            step_size_1 = 1.0/( Lmax * 2 )
            step_size_2 = S / ( 5 * Lmax * (1 + w) * (H[&#39;total_clients&#39;] ** (2.0/3.0)) )
            step_size_3 = S / ( 5 * Lmax * ((1 + w)**3.0/2.0) * (H[&#39;total_clients&#39;] ** (0.5)) )

            return min(step_size_1, step_size_2, step_size_3)

        else:
            Li_all_clients = getLismoothForClients(H, clients_responses)
            Lmax = max(Li_all_clients)

            step_size_1 = 1.0/( Lmax * (2 + 8 * (1 + w) / S) )
            step_size_2 = S / ( (1 + w) * (H[&#39;total_clients&#39;] ** 0.5) )

            return min(step_size_1, step_size_2)

    @staticmethod
    def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D:int, total_clients:int, grad_start) -&gt; dict:
        cm = compressors.Compressor()
        cm.makeIdenticalCompressor()

        state = {&#34;compressor_master&#34; : cm,
                 &#34;h0&#34;: get_initial_shift(args, D, grad_start),
                 &#34;h_prev&#34;: get_initial_shift(args, D, grad_start),
                 }

        return state

    @staticmethod
    def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)
        last_hi = findRecentRecordAndRemoveFromHistory(H, clientId, &#39;hi&#39;)
        alpha = 1.0 / (1.0 + compressor.getW())

        if last_hi is None:
            return {&#34;client_compressor&#34; : compressor, &#34;alpha&#34; : alpha, &#34;hi&#34; : H[&#39;h0&#39;].detach().clone().to(device = device, dtype = H[&#34;fl_dtype&#34;])}
        else:
            return {&#34;client_compressor&#34; : compressor, &#34;alpha&#34; : alpha,
                    &#34;hi&#34; : last_hi.to(device = device, dtype = H[&#34;fl_dtype&#34;]) #last_hi.detach().clone().to(device = device)
                   }

    @staticmethod
    def localGradientEvaluation(client_state: dict,
                                model: torch.nn.Module,
                                dataloader: torch.utils.data.dataloader.DataLoader,
                                criterion: torch.nn.modules.loss._Loss,
                                is_rnn: bool,
                                local_iteration_number: tuple) -&gt; torch.Tensor:

        client_id = client_state[&#34;client_id&#34;]
        fApprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function=True)

        # Please select in GUI SGD-US or another estimator
        grad_cur = mutils.get_gradient(model)

        ui = client_state[&#34;client_compressor&#34;].compressVector(grad_cur - client_state[&#39;hi&#39;])
        client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance

        # Update hi (Experiment!)
        client_state[&#39;hi&#39;] = client_state[&#39;hi&#39;] + client_state[&#39;alpha&#39;] * ui
        return fApprox, ui

    @staticmethod
    def serverGradient(clients_responses: utils.buffer.Buffer,
                       clients: int,
                       model: torch.nn.Module,
                       params_current: torch.Tensor,
                       H: dict) -&gt; torch.Tensor:
        clients_responses.waitForItem()
        obtained_model = clients_responses.get(0)
        wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
        gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])

        alpha = obtained_model[&#34;client_state&#34;][&#39;alpha&#39;]

        gs = wi * gi

        w_total = wi

        for i in range(1, clients):
            clients_responses.waitForItem()
            client_model = clients_responses.get(i)
            gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
            wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

            w_total += wi
            gs += wi * gi

        gs = gs / w_total
        u = gs

        h_prev = H[&#39;h_prev&#39;]
        result =  u + h_prev

        multipler_alpha = alpha * (clients/H[&#39;total_clients&#39;])
        H[&#39;u_avg_update&#39;] =  gs
        H[&#39;alpha_update&#39;] = multipler_alpha
        return result

    @staticmethod
    def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
        H[&#39;h_prev&#39;] = H[&#39;h_prev&#39;] + H[&#34;alpha_update&#34;] * H[&#39;u_avg_update&#39;]
        return H</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="utils.algorithms.COFIG.clientState"><code class="name flex">
<span>def <span class="ident">clientState</span></span>(<span>H: dict, clientId: int, client_data_samples: int, device: str) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
    compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
    compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)
    last_hi = findRecentRecordAndRemoveFromHistory(H, clientId, &#39;hi&#39;)
    alpha = 1.0 / (1.0 + compressor.getW())

    if last_hi is None:
        return {&#34;client_compressor&#34; : compressor, &#34;alpha&#34; : alpha, &#34;hi&#34; : H[&#39;h0&#39;].detach().clone().to(device = device, dtype = H[&#34;fl_dtype&#34;])}
    else:
        return {&#34;client_compressor&#34; : compressor, &#34;alpha&#34; : alpha,
                &#34;hi&#34; : last_hi.to(device = device, dtype = H[&#34;fl_dtype&#34;]) #last_hi.detach().clone().to(device = device)
               }</code></pre>
</details>
</dd>
<dt id="utils.algorithms.COFIG.initializeServerState"><code class="name flex">
<span>def <span class="ident">initializeServerState</span></span>(<span>args: argparse.Namespace, model: torch.nn.modules.module.Module, D: int, total_clients: int, grad_start) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D:int, total_clients:int, grad_start) -&gt; dict:
    cm = compressors.Compressor()
    cm.makeIdenticalCompressor()

    state = {&#34;compressor_master&#34; : cm,
             &#34;h0&#34;: get_initial_shift(args, D, grad_start),
             &#34;h_prev&#34;: get_initial_shift(args, D, grad_start),
             }

    return state</code></pre>
</details>
</dd>
<dt id="utils.algorithms.COFIG.localGradientEvaluation"><code class="name flex">
<span>def <span class="ident">localGradientEvaluation</span></span>(<span>client_state: dict, model: torch.nn.modules.module.Module, dataloader: torch.utils.data.dataloader.DataLoader, criterion: torch.nn.modules.loss._Loss, is_rnn: bool, local_iteration_number: tuple) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def localGradientEvaluation(client_state: dict,
                            model: torch.nn.Module,
                            dataloader: torch.utils.data.dataloader.DataLoader,
                            criterion: torch.nn.modules.loss._Loss,
                            is_rnn: bool,
                            local_iteration_number: tuple) -&gt; torch.Tensor:

    client_id = client_state[&#34;client_id&#34;]
    fApprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function=True)

    # Please select in GUI SGD-US or another estimator
    grad_cur = mutils.get_gradient(model)

    ui = client_state[&#34;client_compressor&#34;].compressVector(grad_cur - client_state[&#39;hi&#39;])
    client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance

    # Update hi (Experiment!)
    client_state[&#39;hi&#39;] = client_state[&#39;hi&#39;] + client_state[&#39;alpha&#39;] * ui
    return fApprox, ui</code></pre>
</details>
</dd>
<dt id="utils.algorithms.COFIG.serverGlobalStateUpdate"><code class="name flex">
<span>def <span class="ident">serverGlobalStateUpdate</span></span>(<span>clients_responses: utils.buffer.Buffer, clients: dict, model: torch.nn.modules.module.Module, paramsPrev: torch.Tensor, grad_server: torch.Tensor, H: dict) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
    H[&#39;h_prev&#39;] = H[&#39;h_prev&#39;] + H[&#34;alpha_update&#34;] * H[&#39;u_avg_update&#39;]
    return H</code></pre>
</details>
</dd>
<dt id="utils.algorithms.COFIG.serverGradient"><code class="name flex">
<span>def <span class="ident">serverGradient</span></span>(<span>clients_responses: utils.buffer.Buffer, clients: int, model: torch.nn.modules.module.Module, params_current: torch.Tensor, H: dict) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def serverGradient(clients_responses: utils.buffer.Buffer,
                   clients: int,
                   model: torch.nn.Module,
                   params_current: torch.Tensor,
                   H: dict) -&gt; torch.Tensor:
    clients_responses.waitForItem()
    obtained_model = clients_responses.get(0)
    wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
    gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])

    alpha = obtained_model[&#34;client_state&#34;][&#39;alpha&#39;]

    gs = wi * gi

    w_total = wi

    for i in range(1, clients):
        clients_responses.waitForItem()
        client_model = clients_responses.get(i)
        gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

        w_total += wi
        gs += wi * gi

    gs = gs / w_total
    u = gs

    h_prev = H[&#39;h_prev&#39;]
    result =  u + h_prev

    multipler_alpha = alpha * (clients/H[&#39;total_clients&#39;])
    H[&#39;u_avg_update&#39;] =  gs
    H[&#39;alpha_update&#39;] = multipler_alpha
    return result</code></pre>
</details>
</dd>
<dt id="utils.algorithms.COFIG.theoreticalStepSize"><code class="name flex">
<span>def <span class="ident">theoreticalStepSize</span></span>(<span>x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case):
    # Step size for non-convex case
    S = clients_in_round
    compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])

    w = compressor.getW()
    a = 1 / (1.0 + w)

    if use_steps_size_for_non_convex_case:
        Li_all_clients = getLismoothForClients(H, clients_responses)
        Lmax = max(Li_all_clients)
        step_size_1 = 1.0/( Lmax * 2 )
        step_size_2 = S / ( 5 * Lmax * (1 + w) * (H[&#39;total_clients&#39;] ** (2.0/3.0)) )
        step_size_3 = S / ( 5 * Lmax * ((1 + w)**3.0/2.0) * (H[&#39;total_clients&#39;] ** (0.5)) )

        return min(step_size_1, step_size_2, step_size_3)

    else:
        Li_all_clients = getLismoothForClients(H, clients_responses)
        Lmax = max(Li_all_clients)

        step_size_1 = 1.0/( Lmax * (2 + 8 * (1 + w) / S) )
        step_size_2 = S / ( (1 + w) * (H[&#39;total_clients&#39;] ** 0.5) )

        return min(step_size_1, step_size_2)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="utils.algorithms.DCGD"><code class="flex name class">
<span>class <span class="ident">DCGD</span></span>
</code></dt>
<dd>
<div class="desc"><p>Distributed Compressed Gradient Descent Algoritm [Alistarh et al., 2017, Khirirat et al., 2018, Horvath et al., 2019]: <a href="https://arxiv.org/abs/1610.02132,">https://arxiv.org/abs/1610.02132,</a> <a href="https://arxiv.org/abs/1806.06573,">https://arxiv.org/abs/1806.06573,</a> <a href="https://arxiv.org/abs/1905.10988">https://arxiv.org/abs/1905.10988</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DCGD:
    &#39;&#39;&#39;
    Distributed Compressed Gradient Descent Algoritm [Alistarh et al., 2017, Khirirat et al., 2018, Horvath et al., 2019]: https://arxiv.org/abs/1610.02132, https://arxiv.org/abs/1806.06573, https://arxiv.org/abs/1905.10988
    &#39;&#39;&#39;
    @staticmethod
    def theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case):
        # Step size for convex case
        workers = H[&#39;total_clients&#39;]
        Li_all_clients = getLismoothForClients(H, clients_responses)
        L = getLsmoothGlobal(H, clients_responses)
        w = H[&#34;w&#34;]
        wM = H[&#34;compressor_master&#34;].getW()

        A = L + 2 * (wM + 1) * max(Li_all_clients * w/workers) + L * wM
        step_size = 1.0/A
        return step_size

    @staticmethod
    def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D: int, total_clients:int, grad_start:torch.Tensor) -&gt; dict:
        cm = compressors.Compressor()
        cm.makeIdenticalCompressor()

        c = compressors.initCompressor(args.client_compressor, D)
        state = {&#34;compressor_master&#34; : cm}

        if c.isUnbiasedCompressor():
            state[&#34;w&#34;] = c.getW()

        elif c.isContractionCompressor():
            # Not applicable for DCGD
            state[&#34;alpha&#34;] = c.getAlphaContraction()

        return state

    @staticmethod
    def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)
        return {&#34;client_compressor&#34; : compressor}

    @staticmethod
    def localGradientEvaluation(client_state: dict,
                                model: torch.nn.Module,
                                dataloader: torch.utils.data.dataloader.DataLoader,
                                criterion: torch.nn.modules.loss._Loss,
                                is_rnn: bool,
                                local_iteration_number: tuple) -&gt; torch.Tensor:
        fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
        grad_cur = mutils.get_gradient(model)
        grad_compress = client_state[&#34;client_compressor&#34;].compressVector(grad_cur)
        client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance
        return fAprox, grad_compress

    @staticmethod
    def serverGradient(clients_responses: utils.buffer.Buffer,
                       clients: int,
                       model: torch.nn.Module,
                       params_current: torch.Tensor,
                       H: dict) -&gt; torch.Tensor:
        clients_responses.waitForItem()
        obtained_model = clients_responses.get(0)
        wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
        gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        gs = wi * gi
        w_total = wi

        for i in range(1, clients):
            clients_responses.waitForItem()
            client_model = clients_responses.get(i)
            gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
            wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

            w_total += wi
            gs += wi * gi
        gs = gs / w_total
        grad_compress = H[&#34;compressor_master&#34;].compressVector(gs)
        return grad_compress

    @staticmethod
    def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
        compressor = H[&#34;compressor_master&#34;]
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, paramsPrev.device, -1, H)
        return H</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="utils.algorithms.DCGD.clientState"><code class="name flex">
<span>def <span class="ident">clientState</span></span>(<span>H: dict, clientId: int, client_data_samples: int, device: str) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
    compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
    compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)
    return {&#34;client_compressor&#34; : compressor}</code></pre>
</details>
</dd>
<dt id="utils.algorithms.DCGD.initializeServerState"><code class="name flex">
<span>def <span class="ident">initializeServerState</span></span>(<span>args: argparse.Namespace, model: torch.nn.modules.module.Module, D: int, total_clients: int, grad_start: torch.Tensor) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D: int, total_clients:int, grad_start:torch.Tensor) -&gt; dict:
    cm = compressors.Compressor()
    cm.makeIdenticalCompressor()

    c = compressors.initCompressor(args.client_compressor, D)
    state = {&#34;compressor_master&#34; : cm}

    if c.isUnbiasedCompressor():
        state[&#34;w&#34;] = c.getW()

    elif c.isContractionCompressor():
        # Not applicable for DCGD
        state[&#34;alpha&#34;] = c.getAlphaContraction()

    return state</code></pre>
</details>
</dd>
<dt id="utils.algorithms.DCGD.localGradientEvaluation"><code class="name flex">
<span>def <span class="ident">localGradientEvaluation</span></span>(<span>client_state: dict, model: torch.nn.modules.module.Module, dataloader: torch.utils.data.dataloader.DataLoader, criterion: torch.nn.modules.loss._Loss, is_rnn: bool, local_iteration_number: tuple) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def localGradientEvaluation(client_state: dict,
                            model: torch.nn.Module,
                            dataloader: torch.utils.data.dataloader.DataLoader,
                            criterion: torch.nn.modules.loss._Loss,
                            is_rnn: bool,
                            local_iteration_number: tuple) -&gt; torch.Tensor:
    fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
    grad_cur = mutils.get_gradient(model)
    grad_compress = client_state[&#34;client_compressor&#34;].compressVector(grad_cur)
    client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance
    return fAprox, grad_compress</code></pre>
</details>
</dd>
<dt id="utils.algorithms.DCGD.serverGlobalStateUpdate"><code class="name flex">
<span>def <span class="ident">serverGlobalStateUpdate</span></span>(<span>clients_responses: utils.buffer.Buffer, clients: dict, model: torch.nn.modules.module.Module, paramsPrev: torch.Tensor, grad_server: torch.Tensor, H: dict) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
    compressor = H[&#34;compressor_master&#34;]
    compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, paramsPrev.device, -1, H)
    return H</code></pre>
</details>
</dd>
<dt id="utils.algorithms.DCGD.serverGradient"><code class="name flex">
<span>def <span class="ident">serverGradient</span></span>(<span>clients_responses: utils.buffer.Buffer, clients: int, model: torch.nn.modules.module.Module, params_current: torch.Tensor, H: dict) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def serverGradient(clients_responses: utils.buffer.Buffer,
                   clients: int,
                   model: torch.nn.Module,
                   params_current: torch.Tensor,
                   H: dict) -&gt; torch.Tensor:
    clients_responses.waitForItem()
    obtained_model = clients_responses.get(0)
    wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
    gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
    gs = wi * gi
    w_total = wi

    for i in range(1, clients):
        clients_responses.waitForItem()
        client_model = clients_responses.get(i)
        gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

        w_total += wi
        gs += wi * gi
    gs = gs / w_total
    grad_compress = H[&#34;compressor_master&#34;].compressVector(gs)
    return grad_compress</code></pre>
</details>
</dd>
<dt id="utils.algorithms.DCGD.theoreticalStepSize"><code class="name flex">
<span>def <span class="ident">theoreticalStepSize</span></span>(<span>x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case):
    # Step size for convex case
    workers = H[&#39;total_clients&#39;]
    Li_all_clients = getLismoothForClients(H, clients_responses)
    L = getLsmoothGlobal(H, clients_responses)
    w = H[&#34;w&#34;]
    wM = H[&#34;compressor_master&#34;].getW()

    A = L + 2 * (wM + 1) * max(Li_all_clients * w/workers) + L * wM
    step_size = 1.0/A
    return step_size</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="utils.algorithms.DIANA"><code class="flex name class">
<span>class <span class="ident">DIANA</span></span>
</code></dt>
<dd>
<div class="desc"><p>DIANA Algoritm [Mishchenko et al., 2019]: <a href="https://arxiv.org/abs/1901.09269,">https://arxiv.org/abs/1901.09269,</a> <a href="https://arxiv.org/pdf/1904.05115.pdf">https://arxiv.org/pdf/1904.05115.pdf</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DIANA:
    &#39;&#39;&#39;
    DIANA Algoritm [Mishchenko et al., 2019]: https://arxiv.org/abs/1901.09269, https://arxiv.org/pdf/1904.05115.pdf
    &#39;&#39;&#39;
    @staticmethod
    def theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case):
        # DIANA for non-convex case
        if use_steps_size_for_non_convex_case:
            # For non-convex case
            m = 1.0
            workers_per_round = clients_in_round
            workers = H[&#39;total_clients&#39;]
            Ltask = getLsmoothGlobal(H, clients_responses)
            step_size = 1.0/(10*Ltask*(1 + H[&#34;w&#34;]/workers)**0.5 * (m**(2.0/3.0) + H[&#34;w&#34;] + 1))  # Th.4 of https://arxiv.org/pdf/1904.05115.pdf
            return step_size
        else:
            # For convex case
            compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
            w = compressor.getW()
            a = 1 / (1.0 + w)
            Li_all_clients = getLismoothForClients(H, clients_responses)
            Lmax = max(Li_all_clients)
            step_size = 1.0 / (Lmax * (1 + 4*w/clients_in_round))                    # SGD-CTRL analysis for strongly-covnex case
            return step_size

    @staticmethod
    def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D:int, total_clients:int, grad_start:torch.Tensor) -&gt; dict:
        c = compressors.initCompressor(args.client_compressor, D)
        w = c.getW()
        alpha = 1.0 / (1.0 + w)

        state = {&#34;h0&#34;    : get_initial_shift(args, D, grad_start),
                 &#34;h&#34;     : get_initial_shift(args, D, grad_start),
                 &#34;alpha&#34; : alpha,
                 &#34;w&#34;     :w
                }
        return state

    @staticmethod
    def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)

        last_hi = findRecentRecordAndRemoveFromHistory(H, clientId, &#39;hi&#39;)

        if last_hi is None:
            return {&#34;client_compressor&#34; : compressor,
                    &#34;hi&#34; : H[&#39;h0&#39;].detach().clone().to(device = device, dtype = H[&#34;fl_dtype&#34;])}
        else:
            return {&#34;client_compressor&#34; : compressor,
                    &#34;hi&#34; : last_hi.to(device = device, dtype = H[&#34;fl_dtype&#34;])          #last_hi.detach().clone().to(device = device, dtype = H[&#34;fl_dtype&#34;])
                   }

    @staticmethod
    def localGradientEvaluation(client_state: dict,
                                model: torch.nn.Module,
                                dataloader: torch.utils.data.dataloader.DataLoader,
                                criterion: torch.nn.modules.loss._Loss,
                                is_rnn: bool,
                                local_iteration_number: tuple) -&gt; torch.Tensor:
        # In theory it&#39;s possible to perform compute without accessing &#34;h&#34; from master
        h = client_state[&#39;hi&#39;]
        fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
        grad_cur = mutils.get_gradient(model)
        m_i = client_state[&#34;client_compressor&#34;].compressVector(grad_cur - h)

        # Comments: server needs only obtain m_i
        client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance

        client_state[&#39;hi&#39;] = client_state[&#39;hi&#39;] + client_state[&#39;H&#39;][&#39;alpha&#39;] * m_i
        return fAprox, m_i

    @staticmethod
    def serverGradient(clients_responses: utils.buffer.Buffer,
                       clients: int,
                       model: torch.nn.Module,
                       params_current: torch.Tensor,
                       H: dict) -&gt; torch.Tensor:
        clients_responses.waitForItem()
        obtained_model = clients_responses.get(0)
        wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
        gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])

        gs = wi * gi
        w_total = wi

        for i in range(1, clients):
            clients_responses.waitForItem()
            client_model = clients_responses.get(i)
            gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
            wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

            w_total += wi
            gs += wi * gi
        gs = gs / w_total


        # Here gs is final gradient estimator without shift
        H[&#39;m&#39;] = gs
        h = H[&#39;h&#39;]
        return h + gs

    @staticmethod
    def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
        mk = H[&#39;m&#39;]
        H[&#39;h&#39;] = H[&#39;h&#39;] + H[&#39;alpha&#39;] * mk
        return H</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="utils.algorithms.DIANA.clientState"><code class="name flex">
<span>def <span class="ident">clientState</span></span>(<span>H: dict, clientId: int, client_data_samples: int, device: str) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
    compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
    compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)

    last_hi = findRecentRecordAndRemoveFromHistory(H, clientId, &#39;hi&#39;)

    if last_hi is None:
        return {&#34;client_compressor&#34; : compressor,
                &#34;hi&#34; : H[&#39;h0&#39;].detach().clone().to(device = device, dtype = H[&#34;fl_dtype&#34;])}
    else:
        return {&#34;client_compressor&#34; : compressor,
                &#34;hi&#34; : last_hi.to(device = device, dtype = H[&#34;fl_dtype&#34;])          #last_hi.detach().clone().to(device = device, dtype = H[&#34;fl_dtype&#34;])
               }</code></pre>
</details>
</dd>
<dt id="utils.algorithms.DIANA.initializeServerState"><code class="name flex">
<span>def <span class="ident">initializeServerState</span></span>(<span>args: argparse.Namespace, model: torch.nn.modules.module.Module, D: int, total_clients: int, grad_start: torch.Tensor) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D:int, total_clients:int, grad_start:torch.Tensor) -&gt; dict:
    c = compressors.initCompressor(args.client_compressor, D)
    w = c.getW()
    alpha = 1.0 / (1.0 + w)

    state = {&#34;h0&#34;    : get_initial_shift(args, D, grad_start),
             &#34;h&#34;     : get_initial_shift(args, D, grad_start),
             &#34;alpha&#34; : alpha,
             &#34;w&#34;     :w
            }
    return state</code></pre>
</details>
</dd>
<dt id="utils.algorithms.DIANA.localGradientEvaluation"><code class="name flex">
<span>def <span class="ident">localGradientEvaluation</span></span>(<span>client_state: dict, model: torch.nn.modules.module.Module, dataloader: torch.utils.data.dataloader.DataLoader, criterion: torch.nn.modules.loss._Loss, is_rnn: bool, local_iteration_number: tuple) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def localGradientEvaluation(client_state: dict,
                            model: torch.nn.Module,
                            dataloader: torch.utils.data.dataloader.DataLoader,
                            criterion: torch.nn.modules.loss._Loss,
                            is_rnn: bool,
                            local_iteration_number: tuple) -&gt; torch.Tensor:
    # In theory it&#39;s possible to perform compute without accessing &#34;h&#34; from master
    h = client_state[&#39;hi&#39;]
    fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
    grad_cur = mutils.get_gradient(model)
    m_i = client_state[&#34;client_compressor&#34;].compressVector(grad_cur - h)

    # Comments: server needs only obtain m_i
    client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance

    client_state[&#39;hi&#39;] = client_state[&#39;hi&#39;] + client_state[&#39;H&#39;][&#39;alpha&#39;] * m_i
    return fAprox, m_i</code></pre>
</details>
</dd>
<dt id="utils.algorithms.DIANA.serverGlobalStateUpdate"><code class="name flex">
<span>def <span class="ident">serverGlobalStateUpdate</span></span>(<span>clients_responses: utils.buffer.Buffer, clients: dict, model: torch.nn.modules.module.Module, paramsPrev: torch.Tensor, grad_server: torch.Tensor, H: dict) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
    mk = H[&#39;m&#39;]
    H[&#39;h&#39;] = H[&#39;h&#39;] + H[&#39;alpha&#39;] * mk
    return H</code></pre>
</details>
</dd>
<dt id="utils.algorithms.DIANA.serverGradient"><code class="name flex">
<span>def <span class="ident">serverGradient</span></span>(<span>clients_responses: utils.buffer.Buffer, clients: int, model: torch.nn.modules.module.Module, params_current: torch.Tensor, H: dict) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def serverGradient(clients_responses: utils.buffer.Buffer,
                   clients: int,
                   model: torch.nn.Module,
                   params_current: torch.Tensor,
                   H: dict) -&gt; torch.Tensor:
    clients_responses.waitForItem()
    obtained_model = clients_responses.get(0)
    wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
    gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])

    gs = wi * gi
    w_total = wi

    for i in range(1, clients):
        clients_responses.waitForItem()
        client_model = clients_responses.get(i)
        gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

        w_total += wi
        gs += wi * gi
    gs = gs / w_total


    # Here gs is final gradient estimator without shift
    H[&#39;m&#39;] = gs
    h = H[&#39;h&#39;]
    return h + gs</code></pre>
</details>
</dd>
<dt id="utils.algorithms.DIANA.theoreticalStepSize"><code class="name flex">
<span>def <span class="ident">theoreticalStepSize</span></span>(<span>x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case):
    # DIANA for non-convex case
    if use_steps_size_for_non_convex_case:
        # For non-convex case
        m = 1.0
        workers_per_round = clients_in_round
        workers = H[&#39;total_clients&#39;]
        Ltask = getLsmoothGlobal(H, clients_responses)
        step_size = 1.0/(10*Ltask*(1 + H[&#34;w&#34;]/workers)**0.5 * (m**(2.0/3.0) + H[&#34;w&#34;] + 1))  # Th.4 of https://arxiv.org/pdf/1904.05115.pdf
        return step_size
    else:
        # For convex case
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        w = compressor.getW()
        a = 1 / (1.0 + w)
        Li_all_clients = getLismoothForClients(H, clients_responses)
        Lmax = max(Li_all_clients)
        step_size = 1.0 / (Lmax * (1 + 4*w/clients_in_round))                    # SGD-CTRL analysis for strongly-covnex case
        return step_size</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="utils.algorithms.EF21"><code class="flex name class">
<span>class <span class="ident">EF21</span></span>
</code></dt>
<dd>
<div class="desc"><p>EF21 Algoritm: "EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback", <a href="https://arxiv.org/abs/2106.05203">https://arxiv.org/abs/2106.05203</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EF21:
    &#39;&#39;&#39;
    EF21 Algoritm: &#34;EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback&#34;, https://arxiv.org/abs/2106.05203
    &#39;&#39;&#39;
    @staticmethod
    def theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case):
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        if compressor.isContractionCompressor():
            a = compressor.getAlphaContraction() # use alpha for contraction compressor
        elif compressor.isUnbiasedCompressor():
            a = 1/(1.0 + compressor.getW())      # use w for scaled unbiased compressor

        Li = getLismoothForClients(H, clients_responses)
        Ltask = getLsmoothGlobal(H, clients_responses)
        Ltilda = np.mean(Li**2)**0.5

        theta = 1 - (1 - a)**0.5
        beta = (1.0 - a) / (1 - (1 - a)**0.5)
        gamma = 1.0 / (Ltask + Ltilda * (beta/theta)**0.5)

        if has_experiment_option(H, &#39;stepsize_multiplier&#39;):
            gamma = gamma * get_experiment_option_f(H, &#39;stepsize_multiplier&#39;)

        return gamma # Th.1, p.40 from EF21

    @staticmethod
    def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D:int, total_clients:int, grad_start:torch.Tensor) -&gt; dict:
        cm = compressors.Compressor()
        cm.makeIdenticalCompressor()
        state = {&#34;compressor_master&#34; : cm,
                 &#34;x0&#34; : mutils.get_params(model),
                 &#34;request_use_full_list_of_clients&#34;: True
                 }
        return state

    @staticmethod
    def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)
        last_g_prev = findRecentRecordAndRemoveFromHistory(H, clientId, &#39;g_prev&#39;)

        if last_g_prev is None:
            return {&#34;client_compressor&#34; : compressor,
                    &#34;g_prev&#34; : None
                    }
        else:
            return {&#34;client_compressor&#34; : compressor,
                    &#34;g_prev&#34; : last_g_prev.to(device = device, dtype = H[&#34;fl_dtype&#34;]) #last_g_prev.detach().clone().to(device = device)
                    }

    @staticmethod
    def localGradientEvaluation(client_state: dict,
                                model: torch.nn.Module,
                                dataloader: torch.utils.data.dataloader.DataLoader,
                                criterion: torch.nn.modules.loss._Loss,
                                is_rnn: bool,
                                local_iteration_number: tuple) -&gt; torch.Tensor:
        # Compute g0 for a first iteration
        g_prev = client_state[&#39;g_prev&#39;]
        if g_prev is None:
            fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function=True)
            grad_cur = mutils.get_gradient(model)
            client_state[&#39;g_prev&#39;] = grad_cur
            # Not take into account communication at first round
            return fAprox, grad_cur
        else:
            # In theory it&#39;s possible to perform compute without accessing &#34;h&#34; from master
            fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
            grad_cur = mutils.get_gradient(model)

            g_prev = client_state[&#39;g_prev&#39;]

            compressor_multiplier = 1.0
            if not client_state[&#34;client_compressor&#34;].isContractionCompressor():
                compressor_multiplier = 1.0/(1.0 + client_state[&#34;client_compressor&#34;].getW())

            g_next = g_prev + client_state[&#34;client_compressor&#34;].compressVector(grad_cur - g_prev) * compressor_multiplier
            # In algorithm really we need only to send compressed difference between new gradient and previous gradient estimator
            client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance
            client_state[&#39;g_prev&#39;] = g_next
            return fAprox, g_next

    @staticmethod
    def serverGradient(clients_responses: utils.buffer.Buffer,
                       clients: int,
                       model: torch.nn.Module,
                       params_current: torch.Tensor,
                       H: dict) -&gt; torch.Tensor:
        # We compute it straightford.
        # In the paper the master uses g^t on server side and combine that with avg. of c_i^t

        clients_responses.waitForItem()
        obtained_model = clients_responses.get(0)
        wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
        gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        gs = wi * gi
        w_total = wi

        for i in range(1, clients):
            clients_responses.waitForItem()
            client_model = clients_responses.get(i)
            gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
            wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

            w_total += wi
            gs += wi * gi
        gs = gs / w_total
        grad_compress = H[&#34;compressor_master&#34;].compressVector(gs)
        return grad_compress

    @staticmethod
    def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
        compressor = H[&#34;compressor_master&#34;]
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, paramsPrev.device, -1, H)
        H[&#34;request_use_full_list_of_clients&#34;] = False
        return H</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="utils.algorithms.EF21.clientState"><code class="name flex">
<span>def <span class="ident">clientState</span></span>(<span>H: dict, clientId: int, client_data_samples: int, device: str) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
    compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
    compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)
    last_g_prev = findRecentRecordAndRemoveFromHistory(H, clientId, &#39;g_prev&#39;)

    if last_g_prev is None:
        return {&#34;client_compressor&#34; : compressor,
                &#34;g_prev&#34; : None
                }
    else:
        return {&#34;client_compressor&#34; : compressor,
                &#34;g_prev&#34; : last_g_prev.to(device = device, dtype = H[&#34;fl_dtype&#34;]) #last_g_prev.detach().clone().to(device = device)
                }</code></pre>
</details>
</dd>
<dt id="utils.algorithms.EF21.initializeServerState"><code class="name flex">
<span>def <span class="ident">initializeServerState</span></span>(<span>args: argparse.Namespace, model: torch.nn.modules.module.Module, D: int, total_clients: int, grad_start: torch.Tensor) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D:int, total_clients:int, grad_start:torch.Tensor) -&gt; dict:
    cm = compressors.Compressor()
    cm.makeIdenticalCompressor()
    state = {&#34;compressor_master&#34; : cm,
             &#34;x0&#34; : mutils.get_params(model),
             &#34;request_use_full_list_of_clients&#34;: True
             }
    return state</code></pre>
</details>
</dd>
<dt id="utils.algorithms.EF21.localGradientEvaluation"><code class="name flex">
<span>def <span class="ident">localGradientEvaluation</span></span>(<span>client_state: dict, model: torch.nn.modules.module.Module, dataloader: torch.utils.data.dataloader.DataLoader, criterion: torch.nn.modules.loss._Loss, is_rnn: bool, local_iteration_number: tuple) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def localGradientEvaluation(client_state: dict,
                            model: torch.nn.Module,
                            dataloader: torch.utils.data.dataloader.DataLoader,
                            criterion: torch.nn.modules.loss._Loss,
                            is_rnn: bool,
                            local_iteration_number: tuple) -&gt; torch.Tensor:
    # Compute g0 for a first iteration
    g_prev = client_state[&#39;g_prev&#39;]
    if g_prev is None:
        fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function=True)
        grad_cur = mutils.get_gradient(model)
        client_state[&#39;g_prev&#39;] = grad_cur
        # Not take into account communication at first round
        return fAprox, grad_cur
    else:
        # In theory it&#39;s possible to perform compute without accessing &#34;h&#34; from master
        fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
        grad_cur = mutils.get_gradient(model)

        g_prev = client_state[&#39;g_prev&#39;]

        compressor_multiplier = 1.0
        if not client_state[&#34;client_compressor&#34;].isContractionCompressor():
            compressor_multiplier = 1.0/(1.0 + client_state[&#34;client_compressor&#34;].getW())

        g_next = g_prev + client_state[&#34;client_compressor&#34;].compressVector(grad_cur - g_prev) * compressor_multiplier
        # In algorithm really we need only to send compressed difference between new gradient and previous gradient estimator
        client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance
        client_state[&#39;g_prev&#39;] = g_next
        return fAprox, g_next</code></pre>
</details>
</dd>
<dt id="utils.algorithms.EF21.serverGlobalStateUpdate"><code class="name flex">
<span>def <span class="ident">serverGlobalStateUpdate</span></span>(<span>clients_responses: utils.buffer.Buffer, clients: dict, model: torch.nn.modules.module.Module, paramsPrev: torch.Tensor, grad_server: torch.Tensor, H: dict) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
    compressor = H[&#34;compressor_master&#34;]
    compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, paramsPrev.device, -1, H)
    H[&#34;request_use_full_list_of_clients&#34;] = False
    return H</code></pre>
</details>
</dd>
<dt id="utils.algorithms.EF21.serverGradient"><code class="name flex">
<span>def <span class="ident">serverGradient</span></span>(<span>clients_responses: utils.buffer.Buffer, clients: int, model: torch.nn.modules.module.Module, params_current: torch.Tensor, H: dict) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def serverGradient(clients_responses: utils.buffer.Buffer,
                   clients: int,
                   model: torch.nn.Module,
                   params_current: torch.Tensor,
                   H: dict) -&gt; torch.Tensor:
    # We compute it straightford.
    # In the paper the master uses g^t on server side and combine that with avg. of c_i^t

    clients_responses.waitForItem()
    obtained_model = clients_responses.get(0)
    wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
    gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
    gs = wi * gi
    w_total = wi

    for i in range(1, clients):
        clients_responses.waitForItem()
        client_model = clients_responses.get(i)
        gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

        w_total += wi
        gs += wi * gi
    gs = gs / w_total
    grad_compress = H[&#34;compressor_master&#34;].compressVector(gs)
    return grad_compress</code></pre>
</details>
</dd>
<dt id="utils.algorithms.EF21.theoreticalStepSize"><code class="name flex">
<span>def <span class="ident">theoreticalStepSize</span></span>(<span>x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case):
    compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
    if compressor.isContractionCompressor():
        a = compressor.getAlphaContraction() # use alpha for contraction compressor
    elif compressor.isUnbiasedCompressor():
        a = 1/(1.0 + compressor.getW())      # use w for scaled unbiased compressor

    Li = getLismoothForClients(H, clients_responses)
    Ltask = getLsmoothGlobal(H, clients_responses)
    Ltilda = np.mean(Li**2)**0.5

    theta = 1 - (1 - a)**0.5
    beta = (1.0 - a) / (1 - (1 - a)**0.5)
    gamma = 1.0 / (Ltask + Ltilda * (beta/theta)**0.5)

    if has_experiment_option(H, &#39;stepsize_multiplier&#39;):
        gamma = gamma * get_experiment_option_f(H, &#39;stepsize_multiplier&#39;)

    return gamma # Th.1, p.40 from EF21</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="utils.algorithms.EF21PP"><code class="flex name class">
<span>class <span class="ident">EF21PP</span></span>
</code></dt>
<dd>
<div class="desc"><p>EF21PP: "EF21 with Bells &amp; Whistles: Practical Algorithmic Extensions of Modern Error Feedback" <a href="https://arxiv.org/abs/2110.03294,">https://arxiv.org/abs/2110.03294,</a> PP with Poisson sampling</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EF21PP:
    &#39;&#39;&#39;
    EF21PP: &#34;EF21 with Bells &amp; Whistles: Practical Algorithmic Extensions of Modern Error Feedback&#34; https://arxiv.org/abs/2110.03294, PP with Poisson sampling
    &#39;&#39;&#39;
    @staticmethod
    def theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case):
        # Theoretical steps size for non-convex case
        p = H[&#39;args&#39;].client_sampling_poisson
        assert p &gt; 0.0

        pmax = p
        pmin = p

        rho = 1e-3
        s = 1e-3

        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        if compressor.isContractionCompressor():
            a = compressor.getAlphaContraction() # use alpha for contraction compressor
        elif compressor.isUnbiasedCompressor():
            a = 1/(1.0 + compressor.getW())      # use w for scaled unbiased compressor

        theta = 1 - (1 + s)*(1 - a)
        beta = (1.0 + 1.0/s) * (1 - a)
        thetap = rho * pmin + theta * pmax - rho - (pmax - pmin)
        Li = getLismoothForClients(H, clients_responses)

        B = (beta * p + (1 + 1.0/rho) * (1 - p)) * (np.mean(Li**2) )

        Ltask = getLsmoothGlobal(H, clients_responses)

        return 1.0/(Ltask + (B/thetap)**0.5) # Th.7, p.47 from EF21-PP

    @staticmethod
    def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D:int, total_clients:int, grad_start:torch.Tensor) -&gt; dict:
        cm = compressors.Compressor()
        cm.makeIdenticalCompressor()
        state = {&#34;compressor_master&#34; : cm,
                 &#34;x0&#34; : mutils.get_params(model),
                 &#34;request_use_full_list_of_clients&#34;: True
                 }
        return state

    @staticmethod
    def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)
        last_g_prev = findRecentRecordAndRemoveFromHistory(H, clientId, &#39;g_prev&#39;)

        if last_g_prev is None:
            return {&#34;client_compressor&#34; : compressor,
                    &#34;g_prev&#34; : None
                    }
        else:
            return {&#34;client_compressor&#34; : compressor,
                    &#34;g_prev&#34; : last_g_prev.to(device = device, dtype = H[&#34;fl_dtype&#34;]) #last_g_prev.detach().clone().to(device = device, dtype = H[&#34;fl_dtype&#34;])
                    }

    @staticmethod
    def localGradientEvaluation(client_state: dict,
                                model: torch.nn.Module,
                                dataloader: torch.utils.data.dataloader.DataLoader,
                                criterion: torch.nn.modules.loss._Loss,
                                is_rnn: bool,
                                local_iteration_number: tuple) -&gt; torch.Tensor:
        # Compute g0 for a first iteration
        g_prev = client_state[&#39;g_prev&#39;]
        if g_prev is None:
            fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function=True)
            grad_cur = mutils.get_gradient(model)
            client_state[&#39;g_prev&#39;] = grad_cur
            # Not take into account communication at first round
            return fAprox, grad_cur
        else:
            # In theory it&#39;s possible to perform compute without accessing &#34;h&#34; from master
            fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
            grad_cur = mutils.get_gradient(model)

            g_prev = client_state[&#39;g_prev&#39;]

            compressor_multiplier = 1.0
            if not client_state[&#34;client_compressor&#34;].isContractionCompressor():
                compressor_multiplier = 1.0/(1.0 + client_state[&#34;client_compressor&#34;].getW())

            g_next = g_prev + client_state[&#34;client_compressor&#34;].compressVector(grad_cur - g_prev) * compressor_multiplier
            client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance
            client_state[&#39;g_prev&#39;] = g_next
            return fAprox, g_next

    @staticmethod
    def serverGradient(clients_responses: utils.buffer.Buffer,
                       clients: int,
                       model: torch.nn.Module,
                       params_current: torch.Tensor,
                       H: dict) -&gt; torch.Tensor:
        # We compute it straightford.
        # In the paper the master uses g^t on server side and combine that with avg. of c_i^t

        clients_responses.waitForItem()
        obtained_model = clients_responses.get(0)
        wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
        gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        gs = wi * gi
        w_total = wi

        for i in range(1, clients):
            clients_responses.waitForItem()
            client_model = clients_responses.get(i)
            gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
            wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

            w_total += wi
            gs += wi * gi
        gs = gs / w_total
        grad_compress = H[&#34;compressor_master&#34;].compressVector(gs)
        return grad_compress

    @staticmethod
    def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
        compressor = H[&#34;compressor_master&#34;]
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, paramsPrev.device, -1, H)
        H[&#34;request_use_full_list_of_clients&#34;] = False
        return H</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="utils.algorithms.EF21PP.clientState"><code class="name flex">
<span>def <span class="ident">clientState</span></span>(<span>H: dict, clientId: int, client_data_samples: int, device: str) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
    compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
    compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)
    last_g_prev = findRecentRecordAndRemoveFromHistory(H, clientId, &#39;g_prev&#39;)

    if last_g_prev is None:
        return {&#34;client_compressor&#34; : compressor,
                &#34;g_prev&#34; : None
                }
    else:
        return {&#34;client_compressor&#34; : compressor,
                &#34;g_prev&#34; : last_g_prev.to(device = device, dtype = H[&#34;fl_dtype&#34;]) #last_g_prev.detach().clone().to(device = device, dtype = H[&#34;fl_dtype&#34;])
                }</code></pre>
</details>
</dd>
<dt id="utils.algorithms.EF21PP.initializeServerState"><code class="name flex">
<span>def <span class="ident">initializeServerState</span></span>(<span>args: argparse.Namespace, model: torch.nn.modules.module.Module, D: int, total_clients: int, grad_start: torch.Tensor) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D:int, total_clients:int, grad_start:torch.Tensor) -&gt; dict:
    cm = compressors.Compressor()
    cm.makeIdenticalCompressor()
    state = {&#34;compressor_master&#34; : cm,
             &#34;x0&#34; : mutils.get_params(model),
             &#34;request_use_full_list_of_clients&#34;: True
             }
    return state</code></pre>
</details>
</dd>
<dt id="utils.algorithms.EF21PP.localGradientEvaluation"><code class="name flex">
<span>def <span class="ident">localGradientEvaluation</span></span>(<span>client_state: dict, model: torch.nn.modules.module.Module, dataloader: torch.utils.data.dataloader.DataLoader, criterion: torch.nn.modules.loss._Loss, is_rnn: bool, local_iteration_number: tuple) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def localGradientEvaluation(client_state: dict,
                            model: torch.nn.Module,
                            dataloader: torch.utils.data.dataloader.DataLoader,
                            criterion: torch.nn.modules.loss._Loss,
                            is_rnn: bool,
                            local_iteration_number: tuple) -&gt; torch.Tensor:
    # Compute g0 for a first iteration
    g_prev = client_state[&#39;g_prev&#39;]
    if g_prev is None:
        fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function=True)
        grad_cur = mutils.get_gradient(model)
        client_state[&#39;g_prev&#39;] = grad_cur
        # Not take into account communication at first round
        return fAprox, grad_cur
    else:
        # In theory it&#39;s possible to perform compute without accessing &#34;h&#34; from master
        fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
        grad_cur = mutils.get_gradient(model)

        g_prev = client_state[&#39;g_prev&#39;]

        compressor_multiplier = 1.0
        if not client_state[&#34;client_compressor&#34;].isContractionCompressor():
            compressor_multiplier = 1.0/(1.0 + client_state[&#34;client_compressor&#34;].getW())

        g_next = g_prev + client_state[&#34;client_compressor&#34;].compressVector(grad_cur - g_prev) * compressor_multiplier
        client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance
        client_state[&#39;g_prev&#39;] = g_next
        return fAprox, g_next</code></pre>
</details>
</dd>
<dt id="utils.algorithms.EF21PP.serverGlobalStateUpdate"><code class="name flex">
<span>def <span class="ident">serverGlobalStateUpdate</span></span>(<span>clients_responses: utils.buffer.Buffer, clients: dict, model: torch.nn.modules.module.Module, paramsPrev: torch.Tensor, grad_server: torch.Tensor, H: dict) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
    compressor = H[&#34;compressor_master&#34;]
    compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, paramsPrev.device, -1, H)
    H[&#34;request_use_full_list_of_clients&#34;] = False
    return H</code></pre>
</details>
</dd>
<dt id="utils.algorithms.EF21PP.serverGradient"><code class="name flex">
<span>def <span class="ident">serverGradient</span></span>(<span>clients_responses: utils.buffer.Buffer, clients: int, model: torch.nn.modules.module.Module, params_current: torch.Tensor, H: dict) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def serverGradient(clients_responses: utils.buffer.Buffer,
                   clients: int,
                   model: torch.nn.Module,
                   params_current: torch.Tensor,
                   H: dict) -&gt; torch.Tensor:
    # We compute it straightford.
    # In the paper the master uses g^t on server side and combine that with avg. of c_i^t

    clients_responses.waitForItem()
    obtained_model = clients_responses.get(0)
    wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
    gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
    gs = wi * gi
    w_total = wi

    for i in range(1, clients):
        clients_responses.waitForItem()
        client_model = clients_responses.get(i)
        gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

        w_total += wi
        gs += wi * gi
    gs = gs / w_total
    grad_compress = H[&#34;compressor_master&#34;].compressVector(gs)
    return grad_compress</code></pre>
</details>
</dd>
<dt id="utils.algorithms.EF21PP.theoreticalStepSize"><code class="name flex">
<span>def <span class="ident">theoreticalStepSize</span></span>(<span>x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case):
    # Theoretical steps size for non-convex case
    p = H[&#39;args&#39;].client_sampling_poisson
    assert p &gt; 0.0

    pmax = p
    pmin = p

    rho = 1e-3
    s = 1e-3

    compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
    if compressor.isContractionCompressor():
        a = compressor.getAlphaContraction() # use alpha for contraction compressor
    elif compressor.isUnbiasedCompressor():
        a = 1/(1.0 + compressor.getW())      # use w for scaled unbiased compressor

    theta = 1 - (1 + s)*(1 - a)
    beta = (1.0 + 1.0/s) * (1 - a)
    thetap = rho * pmin + theta * pmax - rho - (pmax - pmin)
    Li = getLismoothForClients(H, clients_responses)

    B = (beta * p + (1 + 1.0/rho) * (1 - p)) * (np.mean(Li**2) )

    Ltask = getLsmoothGlobal(H, clients_responses)

    return 1.0/(Ltask + (B/thetap)**0.5) # Th.7, p.47 from EF21-PP</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="utils.algorithms.FRECON"><code class="flex name class">
<span>class <span class="ident">FRECON</span></span>
</code></dt>
<dd>
<div class="desc"><p>FRECON Algoritm [Haoyu Zhao et al., 2021]: <a href="https://arxiv.org/abs/2112.13097">https://arxiv.org/abs/2112.13097</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FRECON:
    &#39;&#39;&#39;
    FRECON Algoritm [Haoyu Zhao et al., 2021]: https://arxiv.org/abs/2112.13097
    &#39;&#39;&#39;
    @staticmethod
    def theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case):
        # FRECON in non-convex case
        S = clients_in_round
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        w = compressor.getW()
        a = 1 / (1.0 + w)
        n = H[&#39;total_clients&#39;]
        Li_all_clients = getLismoothForClients(H, clients_responses)
        Lmax = max(Li_all_clients)
        step_size = 1.0/(Lmax * (1 + (10*(1+w)*(1+w)*n/S/S)**0.5))
        return step_size

    @staticmethod
    def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D:int, total_clients:int, grad_start) -&gt; dict:

        state = {&#34;h0&#34;: get_initial_shift(args, D, grad_start),
                 &#34;g0&#34;: grad_start.detach().clone().to(device = args.device, dtype = model.fl_dtype),
                 &#34;h_prev&#34;: get_initial_shift(args, D, grad_start),
                 &#34;g_server_prev&#34;: grad_start.detach().clone().to(device = args.device, dtype = model.fl_dtype),
                 &#34;x_prev&#34;: mutils.get_params(model)
                 }

        return state

    @staticmethod
    def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)
        last_hi = findRecentRecordAndRemoveFromHistory(H, clientId, &#39;hi&#39;)

        #last_qi = findRecentRecordAndRemoveFromHistory(H, clientId, &#39;qi&#39;)
        # Drop qi
        #last_qi = None

        w = compressor.getW()
        alpha = 1 / (1.0 + w)

        if last_hi is None:
            return {&#34;client_compressor&#34; : compressor, &#34;alpha&#34; : alpha, &#34;hi&#34; : H[&#39;h0&#39;].detach().clone().to(device = device, dtype = H[&#34;fl_dtype&#34;])}
        else:
            return {&#34;client_compressor&#34; : compressor, &#34;alpha&#34; : alpha,
                    &#34;hi&#34; : last_hi.to(device = device, dtype = H[&#34;fl_dtype&#34;])  #last_hi.detach().clone().to(device = device, dtype = H[&#34;fl_dtype&#34;])
                   }

    @staticmethod
    def localGradientEvaluation(client_state: dict,
                                model: torch.nn.Module,
                                dataloader: torch.utils.data.dataloader.DataLoader,
                                criterion: torch.nn.modules.loss._Loss,
                                is_rnn: bool,
                                local_iteration_number: tuple) -&gt; torch.Tensor:

        client_id = client_state[&#34;client_id&#34;]
        fApprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function=True)

        # Please select in GUI SGD-US or another estimator
        grad_cur = mutils.get_gradient(model)
        ui = client_state[&#34;client_compressor&#34;].compressVector(grad_cur - client_state[&#39;hi&#39;])
        #if client_state[&#34;H&#34;][&#34;current_round&#34;] != 0:
        client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance
        client_state[&#39;hi&#39;] = client_state[&#39;hi&#39;] + client_state[&#39;alpha&#39;] * ui

        # ==============================================================================================================
        reconstruct_params = mutils.get_params(model)
        mutils.set_params(model, client_state[&#34;H&#34;][&#34;x_prev&#34;])
        evaluateSgd(client_state, model, dataloader, criterion, is_rnn)
        grad_prev = mutils.get_gradient(model)
        mutils.set_params(model, reconstruct_params)
        # ==============================================================================================================
        qi = client_state[&#34;client_compressor&#34;].compressVector(grad_cur - grad_prev)
        #if client_state[&#34;H&#34;][&#34;current_round&#34;] != 0:
        client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance
        client_state[&#39;qi&#39;] = qi
        # =============================================================================================================
        return fApprox, ui

    @staticmethod
    def serverGradient(clients_responses: utils.buffer.Buffer,
                       clients: int,
                       model: torch.nn.Module,
                       params_current: torch.Tensor,
                       H: dict) -&gt; torch.Tensor:
        clients_responses.waitForItem()
        obtained_model = clients_responses.get(0)
        wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
        gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        alpha = obtained_model[&#34;client_state&#34;][&#39;alpha&#39;]

        gs = wi * gi
        q_avg = wi * obtained_model[&#39;client_state&#39;][&#39;qi&#39;]
        w_total = wi

        del obtained_model[&#39;client_state&#39;][&#39;qi&#39;]

        for i in range(1, clients):
            clients_responses.waitForItem()
            client_model = clients_responses.get(i)
            gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
            wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]
            q_avg += wi * client_model[&#39;client_state&#39;][&#39;qi&#39;]

            w_total += wi
            gs += wi * gi

            del client_model[&#39;client_state&#39;][&#39;qi&#39;]

        gs = gs / w_total
        q_avg = q_avg / w_total
        u = gs

        h_prev = H[&#39;h_prev&#39;]

        # ===============================================================================================================
        if has_experiment_option(H, &#34;lambda_&#34;):
            lambda_ = get_experiment_option_f(H, &#34;lambda_&#34;)
        elif has_experiment_option(H, &#34;th_stepsize_noncvx&#34;) or has_experiment_option(H, &#34;th_stepsize_cvx&#34;):
            S = clients
            compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
            w = compressor.getW()
            n = H[&#39;total_clients&#39;]
            H[&#34;lambda_th&#34;] = S / (2 * (1 + w) * n)
            lambda_ = S/( 2*(1+w) * n )
            get_logger(H).info(f&#34;Used lambda is {lambda_}&#34;)
        # ==============================================================================================================
        result = q_avg + (1.0 - lambda_) * H[&#34;g_server_prev&#34;] + lambda_ * (u + h_prev)

        multipler_alpha = alpha * (clients/H[&#39;total_clients&#39;])
        H[&#39;u_avg_update&#39;] =  u
        H[&#39;alpha_update&#39;] = multipler_alpha
        return result

    @staticmethod
    def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
        H[&#39;h_prev&#39;] = H[&#39;h_prev&#39;] + H[&#34;alpha_update&#34;] * H[&#39;u_avg_update&#39;]
        H[&#34;x_prev&#34;] = paramsPrev
        H[&#34;g_server_prev&#34;] = grad_server
        return H</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="utils.algorithms.FRECON.clientState"><code class="name flex">
<span>def <span class="ident">clientState</span></span>(<span>H: dict, clientId: int, client_data_samples: int, device: str) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
    compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
    compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)
    last_hi = findRecentRecordAndRemoveFromHistory(H, clientId, &#39;hi&#39;)

    #last_qi = findRecentRecordAndRemoveFromHistory(H, clientId, &#39;qi&#39;)
    # Drop qi
    #last_qi = None

    w = compressor.getW()
    alpha = 1 / (1.0 + w)

    if last_hi is None:
        return {&#34;client_compressor&#34; : compressor, &#34;alpha&#34; : alpha, &#34;hi&#34; : H[&#39;h0&#39;].detach().clone().to(device = device, dtype = H[&#34;fl_dtype&#34;])}
    else:
        return {&#34;client_compressor&#34; : compressor, &#34;alpha&#34; : alpha,
                &#34;hi&#34; : last_hi.to(device = device, dtype = H[&#34;fl_dtype&#34;])  #last_hi.detach().clone().to(device = device, dtype = H[&#34;fl_dtype&#34;])
               }</code></pre>
</details>
</dd>
<dt id="utils.algorithms.FRECON.initializeServerState"><code class="name flex">
<span>def <span class="ident">initializeServerState</span></span>(<span>args: argparse.Namespace, model: torch.nn.modules.module.Module, D: int, total_clients: int, grad_start) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D:int, total_clients:int, grad_start) -&gt; dict:

    state = {&#34;h0&#34;: get_initial_shift(args, D, grad_start),
             &#34;g0&#34;: grad_start.detach().clone().to(device = args.device, dtype = model.fl_dtype),
             &#34;h_prev&#34;: get_initial_shift(args, D, grad_start),
             &#34;g_server_prev&#34;: grad_start.detach().clone().to(device = args.device, dtype = model.fl_dtype),
             &#34;x_prev&#34;: mutils.get_params(model)
             }

    return state</code></pre>
</details>
</dd>
<dt id="utils.algorithms.FRECON.localGradientEvaluation"><code class="name flex">
<span>def <span class="ident">localGradientEvaluation</span></span>(<span>client_state: dict, model: torch.nn.modules.module.Module, dataloader: torch.utils.data.dataloader.DataLoader, criterion: torch.nn.modules.loss._Loss, is_rnn: bool, local_iteration_number: tuple) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def localGradientEvaluation(client_state: dict,
                            model: torch.nn.Module,
                            dataloader: torch.utils.data.dataloader.DataLoader,
                            criterion: torch.nn.modules.loss._Loss,
                            is_rnn: bool,
                            local_iteration_number: tuple) -&gt; torch.Tensor:

    client_id = client_state[&#34;client_id&#34;]
    fApprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function=True)

    # Please select in GUI SGD-US or another estimator
    grad_cur = mutils.get_gradient(model)
    ui = client_state[&#34;client_compressor&#34;].compressVector(grad_cur - client_state[&#39;hi&#39;])
    #if client_state[&#34;H&#34;][&#34;current_round&#34;] != 0:
    client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance
    client_state[&#39;hi&#39;] = client_state[&#39;hi&#39;] + client_state[&#39;alpha&#39;] * ui

    # ==============================================================================================================
    reconstruct_params = mutils.get_params(model)
    mutils.set_params(model, client_state[&#34;H&#34;][&#34;x_prev&#34;])
    evaluateSgd(client_state, model, dataloader, criterion, is_rnn)
    grad_prev = mutils.get_gradient(model)
    mutils.set_params(model, reconstruct_params)
    # ==============================================================================================================
    qi = client_state[&#34;client_compressor&#34;].compressVector(grad_cur - grad_prev)
    #if client_state[&#34;H&#34;][&#34;current_round&#34;] != 0:
    client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance
    client_state[&#39;qi&#39;] = qi
    # =============================================================================================================
    return fApprox, ui</code></pre>
</details>
</dd>
<dt id="utils.algorithms.FRECON.serverGlobalStateUpdate"><code class="name flex">
<span>def <span class="ident">serverGlobalStateUpdate</span></span>(<span>clients_responses: utils.buffer.Buffer, clients: dict, model: torch.nn.modules.module.Module, paramsPrev: torch.Tensor, grad_server: torch.Tensor, H: dict) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
    H[&#39;h_prev&#39;] = H[&#39;h_prev&#39;] + H[&#34;alpha_update&#34;] * H[&#39;u_avg_update&#39;]
    H[&#34;x_prev&#34;] = paramsPrev
    H[&#34;g_server_prev&#34;] = grad_server
    return H</code></pre>
</details>
</dd>
<dt id="utils.algorithms.FRECON.serverGradient"><code class="name flex">
<span>def <span class="ident">serverGradient</span></span>(<span>clients_responses: utils.buffer.Buffer, clients: int, model: torch.nn.modules.module.Module, params_current: torch.Tensor, H: dict) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def serverGradient(clients_responses: utils.buffer.Buffer,
                   clients: int,
                   model: torch.nn.Module,
                   params_current: torch.Tensor,
                   H: dict) -&gt; torch.Tensor:
    clients_responses.waitForItem()
    obtained_model = clients_responses.get(0)
    wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
    gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
    alpha = obtained_model[&#34;client_state&#34;][&#39;alpha&#39;]

    gs = wi * gi
    q_avg = wi * obtained_model[&#39;client_state&#39;][&#39;qi&#39;]
    w_total = wi

    del obtained_model[&#39;client_state&#39;][&#39;qi&#39;]

    for i in range(1, clients):
        clients_responses.waitForItem()
        client_model = clients_responses.get(i)
        gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]
        q_avg += wi * client_model[&#39;client_state&#39;][&#39;qi&#39;]

        w_total += wi
        gs += wi * gi

        del client_model[&#39;client_state&#39;][&#39;qi&#39;]

    gs = gs / w_total
    q_avg = q_avg / w_total
    u = gs

    h_prev = H[&#39;h_prev&#39;]

    # ===============================================================================================================
    if has_experiment_option(H, &#34;lambda_&#34;):
        lambda_ = get_experiment_option_f(H, &#34;lambda_&#34;)
    elif has_experiment_option(H, &#34;th_stepsize_noncvx&#34;) or has_experiment_option(H, &#34;th_stepsize_cvx&#34;):
        S = clients
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        w = compressor.getW()
        n = H[&#39;total_clients&#39;]
        H[&#34;lambda_th&#34;] = S / (2 * (1 + w) * n)
        lambda_ = S/( 2*(1+w) * n )
        get_logger(H).info(f&#34;Used lambda is {lambda_}&#34;)
    # ==============================================================================================================
    result = q_avg + (1.0 - lambda_) * H[&#34;g_server_prev&#34;] + lambda_ * (u + h_prev)

    multipler_alpha = alpha * (clients/H[&#39;total_clients&#39;])
    H[&#39;u_avg_update&#39;] =  u
    H[&#39;alpha_update&#39;] = multipler_alpha
    return result</code></pre>
</details>
</dd>
<dt id="utils.algorithms.FRECON.theoreticalStepSize"><code class="name flex">
<span>def <span class="ident">theoreticalStepSize</span></span>(<span>x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case):
    # FRECON in non-convex case
    S = clients_in_round
    compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
    w = compressor.getW()
    a = 1 / (1.0 + w)
    n = H[&#39;total_clients&#39;]
    Li_all_clients = getLismoothForClients(H, clients_responses)
    Lmax = max(Li_all_clients)
    step_size = 1.0/(Lmax * (1 + (10*(1+w)*(1+w)*n/S/S)**0.5))
    return step_size</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="utils.algorithms.FedAvg"><code class="flex name class">
<span>class <span class="ident">FedAvg</span></span>
</code></dt>
<dd>
<div class="desc"><p>Algorithm FedAVG [McMahan et al., 2017]: <a href="https://arxiv.org/abs/1602.05629">https://arxiv.org/abs/1602.05629</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FedAvg:
    &#39;&#39;&#39;
    Algorithm FedAVG [McMahan et al., 2017]: https://arxiv.org/abs/1602.05629 
    &#39;&#39;&#39;
    @staticmethod
    def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D:int, total_clients:int, grad_start:torch.Tensor) -&gt; dict:
        state = {}
        return state

    @staticmethod
    def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
        return {}

    @staticmethod
    def localGradientEvaluation(client_state: dict,
                                model: torch.nn.Module,
                                dataloader: torch.utils.data.dataloader.DataLoader,
                                criterion: torch.nn.modules.loss._Loss,
                                is_rnn: bool,
                                local_iteration_number: tuple) -&gt; torch.Tensor:
        fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
        grad_cur = mutils.get_gradient(model)
        client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += grad_cur.numel()

        return fAprox, grad_cur

    @staticmethod
    def serverGradient(clients_responses: utils.buffer.Buffer,
                       clients: int,
                       model: torch.nn.Module,
                       params_current: torch.Tensor,
                       H: dict) -&gt; torch.Tensor:
        clients_responses.waitForItem()
        obtained_model = clients_responses.get(0)
        wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
        gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        gs = wi * gi
        w_total = wi

        for i in range(1, clients):
            clients_responses.waitForItem()
            client_model = clients_responses.get(i)
            gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
            wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

            w_total += wi
            gs += wi * gi
        gs = gs / w_total

        return gs

    @staticmethod
    def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
        return H</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="utils.algorithms.FedAvg.clientState"><code class="name flex">
<span>def <span class="ident">clientState</span></span>(<span>H: dict, clientId: int, client_data_samples: int, device: str) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
    return {}</code></pre>
</details>
</dd>
<dt id="utils.algorithms.FedAvg.initializeServerState"><code class="name flex">
<span>def <span class="ident">initializeServerState</span></span>(<span>args: argparse.Namespace, model: torch.nn.modules.module.Module, D: int, total_clients: int, grad_start: torch.Tensor) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D:int, total_clients:int, grad_start:torch.Tensor) -&gt; dict:
    state = {}
    return state</code></pre>
</details>
</dd>
<dt id="utils.algorithms.FedAvg.localGradientEvaluation"><code class="name flex">
<span>def <span class="ident">localGradientEvaluation</span></span>(<span>client_state: dict, model: torch.nn.modules.module.Module, dataloader: torch.utils.data.dataloader.DataLoader, criterion: torch.nn.modules.loss._Loss, is_rnn: bool, local_iteration_number: tuple) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def localGradientEvaluation(client_state: dict,
                            model: torch.nn.Module,
                            dataloader: torch.utils.data.dataloader.DataLoader,
                            criterion: torch.nn.modules.loss._Loss,
                            is_rnn: bool,
                            local_iteration_number: tuple) -&gt; torch.Tensor:
    fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
    grad_cur = mutils.get_gradient(model)
    client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += grad_cur.numel()

    return fAprox, grad_cur</code></pre>
</details>
</dd>
<dt id="utils.algorithms.FedAvg.serverGlobalStateUpdate"><code class="name flex">
<span>def <span class="ident">serverGlobalStateUpdate</span></span>(<span>clients_responses: utils.buffer.Buffer, clients: dict, model: torch.nn.modules.module.Module, paramsPrev: torch.Tensor, grad_server: torch.Tensor, H: dict) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
    return H</code></pre>
</details>
</dd>
<dt id="utils.algorithms.FedAvg.serverGradient"><code class="name flex">
<span>def <span class="ident">serverGradient</span></span>(<span>clients_responses: utils.buffer.Buffer, clients: int, model: torch.nn.modules.module.Module, params_current: torch.Tensor, H: dict) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def serverGradient(clients_responses: utils.buffer.Buffer,
                   clients: int,
                   model: torch.nn.Module,
                   params_current: torch.Tensor,
                   H: dict) -&gt; torch.Tensor:
    clients_responses.waitForItem()
    obtained_model = clients_responses.get(0)
    wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
    gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
    gs = wi * gi
    w_total = wi

    for i in range(1, clients):
        clients_responses.waitForItem()
        client_model = clients_responses.get(i)
        gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

        w_total += wi
        gs += wi * gi
    gs = gs / w_total

    return gs</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="utils.algorithms.FedProx"><code class="flex name class">
<span>class <span class="ident">FedProx</span></span>
</code></dt>
<dd>
<div class="desc"><p>Algorithm FedProx
[Li et al., 2018]: <a href="https://arxiv.org/abs/1812.06127">https://arxiv.org/abs/1812.06127</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FedProx:
    &#39;&#39;&#39;
    Algorithm FedProx  [Li et al., 2018]: https://arxiv.org/abs/1812.06127
    &#39;&#39;&#39;
    @staticmethod
    def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D:int, total_clients:int, grad_start:torch.Tensor) -&gt; dict:
        state = {&#39;wt&#39;: mutils.get_params(model)}
        return state

    @staticmethod
    def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)
        return {&#34;client_compressor&#34;: compressor}

    @staticmethod
    def localGradientEvaluation(client_state: dict,
                                model: torch.nn.Module,
                                dataloader: torch.utils.data.dataloader.DataLoader,
                                criterion: torch.nn.modules.loss._Loss,
                                is_rnn: bool,
                                local_iteration_number: tuple) -&gt; torch.Tensor:
        fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
        grad_cur = mutils.get_gradient(model)

        opts = client_state[&#39;H&#39;][&#39;execution_context&#39;].experimental_options
        mu_prox = 1.0
        if &#34;mu_prox&#34; in opts:
            mu_prox = float(opts[&#39;mu_prox&#39;])

        x_cur = mutils.get_params(model)
        wt = client_state[&#39;H&#39;][&#39;wt&#39;].to(device = client_state[&#34;device&#34;], dtype = model.fl_dtype)

        grad_cur += mu_prox * (x_cur - wt)

        client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += grad_cur.numel()
        # assume sending &#39;wt&#39; from master to clients is for free

        grad_cur = client_state[&#34;client_compressor&#34;].compressVector(grad_cur)

        return fAprox, grad_cur

    @staticmethod
    def serverGradient(clients_responses: utils.buffer.Buffer,
                       clients: int,
                       model: torch.nn.Module,
                       params_current: torch.Tensor,
                       H: dict) -&gt; torch.Tensor:
        clients_responses.waitForItem()
        obtained_model = clients_responses.get(0)
        wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
        gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        gs = wi * gi
        w_total = wi

        for i in range(1, clients):
            clients_responses.waitForItem()
            client_model = clients_responses.get(i)
            gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
            wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

            w_total += wi
            gs += wi * gi
        gs = gs / w_total

        return gs

    @staticmethod
    def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
        H[&#39;wt&#39;] = mutils.get_params(model)
        return H</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="utils.algorithms.FedProx.clientState"><code class="name flex">
<span>def <span class="ident">clientState</span></span>(<span>H: dict, clientId: int, client_data_samples: int, device: str) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
    compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
    compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)
    return {&#34;client_compressor&#34;: compressor}</code></pre>
</details>
</dd>
<dt id="utils.algorithms.FedProx.initializeServerState"><code class="name flex">
<span>def <span class="ident">initializeServerState</span></span>(<span>args: argparse.Namespace, model: torch.nn.modules.module.Module, D: int, total_clients: int, grad_start: torch.Tensor) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D:int, total_clients:int, grad_start:torch.Tensor) -&gt; dict:
    state = {&#39;wt&#39;: mutils.get_params(model)}
    return state</code></pre>
</details>
</dd>
<dt id="utils.algorithms.FedProx.localGradientEvaluation"><code class="name flex">
<span>def <span class="ident">localGradientEvaluation</span></span>(<span>client_state: dict, model: torch.nn.modules.module.Module, dataloader: torch.utils.data.dataloader.DataLoader, criterion: torch.nn.modules.loss._Loss, is_rnn: bool, local_iteration_number: tuple) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def localGradientEvaluation(client_state: dict,
                            model: torch.nn.Module,
                            dataloader: torch.utils.data.dataloader.DataLoader,
                            criterion: torch.nn.modules.loss._Loss,
                            is_rnn: bool,
                            local_iteration_number: tuple) -&gt; torch.Tensor:
    fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
    grad_cur = mutils.get_gradient(model)

    opts = client_state[&#39;H&#39;][&#39;execution_context&#39;].experimental_options
    mu_prox = 1.0
    if &#34;mu_prox&#34; in opts:
        mu_prox = float(opts[&#39;mu_prox&#39;])

    x_cur = mutils.get_params(model)
    wt = client_state[&#39;H&#39;][&#39;wt&#39;].to(device = client_state[&#34;device&#34;], dtype = model.fl_dtype)

    grad_cur += mu_prox * (x_cur - wt)

    client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += grad_cur.numel()
    # assume sending &#39;wt&#39; from master to clients is for free

    grad_cur = client_state[&#34;client_compressor&#34;].compressVector(grad_cur)

    return fAprox, grad_cur</code></pre>
</details>
</dd>
<dt id="utils.algorithms.FedProx.serverGlobalStateUpdate"><code class="name flex">
<span>def <span class="ident">serverGlobalStateUpdate</span></span>(<span>clients_responses: utils.buffer.Buffer, clients: dict, model: torch.nn.modules.module.Module, paramsPrev: torch.Tensor, grad_server: torch.Tensor, H: dict) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
    H[&#39;wt&#39;] = mutils.get_params(model)
    return H</code></pre>
</details>
</dd>
<dt id="utils.algorithms.FedProx.serverGradient"><code class="name flex">
<span>def <span class="ident">serverGradient</span></span>(<span>clients_responses: utils.buffer.Buffer, clients: int, model: torch.nn.modules.module.Module, params_current: torch.Tensor, H: dict) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def serverGradient(clients_responses: utils.buffer.Buffer,
                   clients: int,
                   model: torch.nn.Module,
                   params_current: torch.Tensor,
                   H: dict) -&gt; torch.Tensor:
    clients_responses.waitForItem()
    obtained_model = clients_responses.get(0)
    wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
    gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
    gs = wi * gi
    w_total = wi

    for i in range(1, clients):
        clients_responses.waitForItem()
        client_model = clients_responses.get(i)
        gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

        w_total += wi
        gs += wi * gi
    gs = gs / w_total

    return gs</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="utils.algorithms.MarinaAlgorithm"><code class="flex name class">
<span>class <span class="ident">MarinaAlgorithm</span></span>
</code></dt>
<dd>
<div class="desc"><p>MARINA Algoritm [Gorbunov et al., 2021]: <a href="https://arxiv.org/abs/2102.07845">https://arxiv.org/abs/2102.07845</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MarinaAlgorithm:
    &#39;&#39;&#39;
    MARINA Algoritm [Gorbunov et al., 2021]: https://arxiv.org/abs/2102.07845
    &#39;&#39;&#39;
    @staticmethod
    def initializeServerState(args:argparse.Namespace, model:torch.nn.Module, D:int, total_clients:int, grad_start:torch.Tensor)-&gt;dict:
        state = {&#34;x_prev&#34; : mutils.get_params(model),             # previous iterate
                 &#34;test_ber_rv&#34; : 0.0                              # test_ber_rv = 0.0 will force fisrt iteration be a full gradient evaluation
                 }
        return state

    @staticmethod
    def clientState(H:dict, clientId:int, client_data_samples:int, device:str)-&gt;dict:
        logger = Logger.get(H[&#34;run_id&#34;])

        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)

        p = 1.0 / (1.0 + compressor.getW())

        state = {}
        if H[&#34;test_ber_rv&#34;] &lt;= p:
            state.update({&#34;p&#34; : p, &#34;ck&#34;: 1, &#34;client_compressor&#34; : compressor})
        else:
            state.update({&#34;p&#34; : p, &#34;ck&#34;: 0, &#34;client_compressor&#34;: compressor})

        return state

    @staticmethod
    def localGradientEvaluation(client_state: dict,
                                model: torch.nn.Module,
                                dataloader: torch.utils.data.dataloader.DataLoader,
                                criterion: torch.nn.modules.loss._Loss,
                                is_rnn: bool,
                                local_iteration_number: tuple) -&gt; torch.Tensor:
        if client_state[&#34;ck&#34;] == 1:
            fApprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
            grad_cur = mutils.get_gradient(model)
            client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += grad_cur.numel()
            return fApprox, grad_cur
        else:
            client_id = client_state[&#34;client_id&#34;]
            fApprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
            grad_cur = mutils.get_gradient(model)

            reconstruct_params = mutils.get_params(model)
            mutils.set_params(model, client_state[&#34;H&#34;][&#34;x_prev&#34;])
            evaluateSgd(client_state, model, dataloader, criterion, is_rnn)
            grad_prev = mutils.get_gradient(model)
            mutils.set_params(model, reconstruct_params)

            g_prev = client_state[&#34;H&#34;][&#34;g_prev&#34;].to(device = client_state[&#34;device&#34;], dtype = model.fl_dtype)
            g_next = g_prev + client_state[&#34;client_compressor&#34;].compressVector(grad_cur - grad_prev)

            # Comments: server knows g_prev
            client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance
            return fApprox, g_next

    @staticmethod
    def serverGradient(clients_responses: utils.buffer.Buffer, clients: int, model: torch.nn.Module, params_current: torch.Tensor, H: dict) -&gt; torch.Tensor:
        clients_responses.waitForItem()
        obtained_model = clients_responses.get(0)
        wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
        gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        gs = wi * gi
        w_total = wi

        for i in range(1, clients):
            clients_responses.waitForItem()
            client_model = clients_responses.get(i)
            gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
            wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

            w_total += wi
            gs += wi * gi
        gs = gs / w_total
        return gs

    @staticmethod
    def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
        H[&#34;g_prev&#34;] = grad_server
        H[&#34;x_prev&#34;] = mutils.get_params(model)

        H[&#34;test_ber_rv&#34;] = H[&#39;execution_context&#39;].np_random.random()
        return H</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="utils.algorithms.MarinaAlgorithm.clientState"><code class="name flex">
<span>def <span class="ident">clientState</span></span>(<span>H: dict, clientId: int, client_data_samples: int, device: str) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def clientState(H:dict, clientId:int, client_data_samples:int, device:str)-&gt;dict:
    logger = Logger.get(H[&#34;run_id&#34;])

    compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
    compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)

    p = 1.0 / (1.0 + compressor.getW())

    state = {}
    if H[&#34;test_ber_rv&#34;] &lt;= p:
        state.update({&#34;p&#34; : p, &#34;ck&#34;: 1, &#34;client_compressor&#34; : compressor})
    else:
        state.update({&#34;p&#34; : p, &#34;ck&#34;: 0, &#34;client_compressor&#34;: compressor})

    return state</code></pre>
</details>
</dd>
<dt id="utils.algorithms.MarinaAlgorithm.initializeServerState"><code class="name flex">
<span>def <span class="ident">initializeServerState</span></span>(<span>args: argparse.Namespace, model: torch.nn.modules.module.Module, D: int, total_clients: int, grad_start: torch.Tensor) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def initializeServerState(args:argparse.Namespace, model:torch.nn.Module, D:int, total_clients:int, grad_start:torch.Tensor)-&gt;dict:
    state = {&#34;x_prev&#34; : mutils.get_params(model),             # previous iterate
             &#34;test_ber_rv&#34; : 0.0                              # test_ber_rv = 0.0 will force fisrt iteration be a full gradient evaluation
             }
    return state</code></pre>
</details>
</dd>
<dt id="utils.algorithms.MarinaAlgorithm.localGradientEvaluation"><code class="name flex">
<span>def <span class="ident">localGradientEvaluation</span></span>(<span>client_state: dict, model: torch.nn.modules.module.Module, dataloader: torch.utils.data.dataloader.DataLoader, criterion: torch.nn.modules.loss._Loss, is_rnn: bool, local_iteration_number: tuple) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def localGradientEvaluation(client_state: dict,
                            model: torch.nn.Module,
                            dataloader: torch.utils.data.dataloader.DataLoader,
                            criterion: torch.nn.modules.loss._Loss,
                            is_rnn: bool,
                            local_iteration_number: tuple) -&gt; torch.Tensor:
    if client_state[&#34;ck&#34;] == 1:
        fApprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
        grad_cur = mutils.get_gradient(model)
        client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += grad_cur.numel()
        return fApprox, grad_cur
    else:
        client_id = client_state[&#34;client_id&#34;]
        fApprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
        grad_cur = mutils.get_gradient(model)

        reconstruct_params = mutils.get_params(model)
        mutils.set_params(model, client_state[&#34;H&#34;][&#34;x_prev&#34;])
        evaluateSgd(client_state, model, dataloader, criterion, is_rnn)
        grad_prev = mutils.get_gradient(model)
        mutils.set_params(model, reconstruct_params)

        g_prev = client_state[&#34;H&#34;][&#34;g_prev&#34;].to(device = client_state[&#34;device&#34;], dtype = model.fl_dtype)
        g_next = g_prev + client_state[&#34;client_compressor&#34;].compressVector(grad_cur - grad_prev)

        # Comments: server knows g_prev
        client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance
        return fApprox, g_next</code></pre>
</details>
</dd>
<dt id="utils.algorithms.MarinaAlgorithm.serverGlobalStateUpdate"><code class="name flex">
<span>def <span class="ident">serverGlobalStateUpdate</span></span>(<span>clients_responses: utils.buffer.Buffer, clients: dict, model: torch.nn.modules.module.Module, paramsPrev: torch.Tensor, grad_server: torch.Tensor, H: dict) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
    H[&#34;g_prev&#34;] = grad_server
    H[&#34;x_prev&#34;] = mutils.get_params(model)

    H[&#34;test_ber_rv&#34;] = H[&#39;execution_context&#39;].np_random.random()
    return H</code></pre>
</details>
</dd>
<dt id="utils.algorithms.MarinaAlgorithm.serverGradient"><code class="name flex">
<span>def <span class="ident">serverGradient</span></span>(<span>clients_responses: utils.buffer.Buffer, clients: int, model: torch.nn.modules.module.Module, params_current: torch.Tensor, H: dict) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def serverGradient(clients_responses: utils.buffer.Buffer, clients: int, model: torch.nn.Module, params_current: torch.Tensor, H: dict) -&gt; torch.Tensor:
    clients_responses.waitForItem()
    obtained_model = clients_responses.get(0)
    wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
    gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
    gs = wi * gi
    w_total = wi

    for i in range(1, clients):
        clients_responses.waitForItem()
        client_model = clients_responses.get(i)
        gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

        w_total += wi
        gs += wi * gi
    gs = gs / w_total
    return gs</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="utils.algorithms.MarinaAlgorithmPP"><code class="flex name class">
<span>class <span class="ident">MarinaAlgorithmPP</span></span>
</code></dt>
<dd>
<div class="desc"><p>MARINA Algoritm [Gorbunov et al., 2021]: <a href="https://arxiv.org/abs/2102.07845">https://arxiv.org/abs/2102.07845</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MarinaAlgorithmPP:
    &#39;&#39;&#39;
    MARINA Algoritm [Gorbunov et al., 2021]: https://arxiv.org/abs/2102.07845
    &#39;&#39;&#39;
    @staticmethod
    def algorithmDescription():
        return { &#34;paper&#34; : &#34;https://arxiv.org/abs/2102.07845&#34; }

    @staticmethod
    def theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case):
        # Step size for non-convex case
        m = 1.0
        workers_per_round = clients_in_round
        workers = H[&#39;total_clients&#39;]
        Li_all_clients = getLismoothForClients(H, clients_responses)

        # Ltask = (np.mean( (Li_all_clients) **2) )**0.5
        Ltask = (np.mean( max(Li_all_clients) **2) )**0.5 # Maybe hack by /2
        w = H[&#34;w&#34;]
        p = (workers_per_round/workers)*1.0/(1+w)            # For RAND-K compressor
        r = workers_per_round

        step_1 = ((1 + 4*(1-p)*(1+w)/(p*workers))**0.5 - 1) / (2* (1-p) * (1+w)/(p*workers) * Ltask)
        step_2 = (-(1 + 4*(1-p)*(1+w)/(p*workers))**0.5 - 1) / (2* (1-p) * (1+w)/(p*workers) * Ltask)
        step_3 = 1.0 / (Ltask * (1 + ((1 - p) * (1 + w) / (p * workers_per_round)) ** 0.5))  # Theorem 4.1, p.37

        return step_3

    @staticmethod
    def initializeServerState(args:argparse.Namespace, model:torch.nn.Module, D:int, total_clients:int, grad_start:torch.Tensor)-&gt;dict:
        compressor = compressors.initCompressor(args.client_compressor, D)

        state = {&#34;x_prev&#34; : mutils.get_params(model),             # previous iterate
                 &#34;test_ber_rv&#34; : 0.0,                             # test_ber_rv = 0.0 will force fisrt iteration be a full gradient evaluation
                 &#34;num_clients_per_round&#34; : args.num_clients_per_round,
                 &#34;total_clients&#34; : total_clients,
                 &#34;w&#34; : compressor.getW()
                 }

        p = 1.0 / (1.0 + compressor.getW())
        p = p * args.num_clients_per_round / total_clients
        state.update({&#34;p&#34; : p})

        if state[&#34;test_ber_rv&#34;] &lt;= p:
            state[&#34;ck&#34;] = 1
            state[&#34;request_use_full_list_of_clients&#34;] = True
        else:
            state[&#34;ck&#34;] = 0
            state[&#34;request_use_full_list_of_clients&#34;] = False

        return state

    @staticmethod
    def clientState(H:dict, clientId:int, client_data_samples:int, device:str)-&gt;dict:
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)
        state = {&#34;p&#34; : H[&#34;p&#34;], &#34;ck&#34;: H[&#34;ck&#34;], &#34;client_compressor&#34; : compressor}
        return state

    @staticmethod
    def localGradientEvaluation(client_state: dict,
                                model: torch.nn.Module,
                                dataloader: torch.utils.data.dataloader.DataLoader,
                                criterion: torch.nn.modules.loss._Loss,
                                is_rnn: bool,
                                local_iteration_number: tuple) -&gt; torch.Tensor:
        if client_state[&#34;ck&#34;] == 1:
            fApprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
            grad_cur = mutils.get_gradient(model)
            client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += grad_cur.numel()
            return fApprox, grad_cur
        else:
            client_id = client_state[&#34;client_id&#34;]
            fApprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
            grad_cur = mutils.get_gradient(model)

            reconstruct_params = mutils.get_params(model)
            mutils.set_params(model, client_state[&#34;H&#34;][&#34;x_prev&#34;])
            evaluateSgd(client_state, model, dataloader, criterion, is_rnn)
            grad_prev = mutils.get_gradient(model)
            mutils.set_params(model, reconstruct_params)

            g_prev = client_state[&#34;H&#34;][&#34;g_prev&#34;].to(device = client_state[&#34;device&#34;], dtype = model.fl_dtype)
            g_next = g_prev + client_state[&#34;client_compressor&#34;].compressVector(grad_cur - grad_prev)

            # Comments: server knows g_prev
            client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance
            return fApprox, g_next

    @staticmethod
    def serverGradient(clients_responses: utils.buffer.Buffer, clients: int, model: torch.nn.Module, params_current: torch.Tensor, H: dict) -&gt; torch.Tensor:
        clients_responses.waitForItem()
        obtained_model = clients_responses.get(0)
        wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
        gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        gs = wi * gi
        w_total = wi

        for i in range(1, clients):
            clients_responses.waitForItem()
            client_model = clients_responses.get(i)
            gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
            wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

            w_total += wi
            gs += wi * gi
        gs = gs / w_total
        return gs

    @staticmethod
    def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
        H[&#34;g_prev&#34;] = grad_server
        H[&#34;x_prev&#34;] = mutils.get_params(model)

        H[&#34;test_ber_rv&#34;] = H[&#39;execution_context&#39;].np_random.random()
        if H[&#34;test_ber_rv&#34;] &lt;= H[&#34;p&#34;]:
            H[&#34;ck&#34;] = 1
            H[&#34;request_use_full_list_of_clients&#34;] = True
        else:
            H[&#34;ck&#34;] = 0
            H[&#34;request_use_full_list_of_clients&#34;] = False

        return H</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="utils.algorithms.MarinaAlgorithmPP.algorithmDescription"><code class="name flex">
<span>def <span class="ident">algorithmDescription</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def algorithmDescription():
    return { &#34;paper&#34; : &#34;https://arxiv.org/abs/2102.07845&#34; }</code></pre>
</details>
</dd>
<dt id="utils.algorithms.MarinaAlgorithmPP.clientState"><code class="name flex">
<span>def <span class="ident">clientState</span></span>(<span>H: dict, clientId: int, client_data_samples: int, device: str) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def clientState(H:dict, clientId:int, client_data_samples:int, device:str)-&gt;dict:
    compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
    compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)
    state = {&#34;p&#34; : H[&#34;p&#34;], &#34;ck&#34;: H[&#34;ck&#34;], &#34;client_compressor&#34; : compressor}
    return state</code></pre>
</details>
</dd>
<dt id="utils.algorithms.MarinaAlgorithmPP.initializeServerState"><code class="name flex">
<span>def <span class="ident">initializeServerState</span></span>(<span>args: argparse.Namespace, model: torch.nn.modules.module.Module, D: int, total_clients: int, grad_start: torch.Tensor) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def initializeServerState(args:argparse.Namespace, model:torch.nn.Module, D:int, total_clients:int, grad_start:torch.Tensor)-&gt;dict:
    compressor = compressors.initCompressor(args.client_compressor, D)

    state = {&#34;x_prev&#34; : mutils.get_params(model),             # previous iterate
             &#34;test_ber_rv&#34; : 0.0,                             # test_ber_rv = 0.0 will force fisrt iteration be a full gradient evaluation
             &#34;num_clients_per_round&#34; : args.num_clients_per_round,
             &#34;total_clients&#34; : total_clients,
             &#34;w&#34; : compressor.getW()
             }

    p = 1.0 / (1.0 + compressor.getW())
    p = p * args.num_clients_per_round / total_clients
    state.update({&#34;p&#34; : p})

    if state[&#34;test_ber_rv&#34;] &lt;= p:
        state[&#34;ck&#34;] = 1
        state[&#34;request_use_full_list_of_clients&#34;] = True
    else:
        state[&#34;ck&#34;] = 0
        state[&#34;request_use_full_list_of_clients&#34;] = False

    return state</code></pre>
</details>
</dd>
<dt id="utils.algorithms.MarinaAlgorithmPP.localGradientEvaluation"><code class="name flex">
<span>def <span class="ident">localGradientEvaluation</span></span>(<span>client_state: dict, model: torch.nn.modules.module.Module, dataloader: torch.utils.data.dataloader.DataLoader, criterion: torch.nn.modules.loss._Loss, is_rnn: bool, local_iteration_number: tuple) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def localGradientEvaluation(client_state: dict,
                            model: torch.nn.Module,
                            dataloader: torch.utils.data.dataloader.DataLoader,
                            criterion: torch.nn.modules.loss._Loss,
                            is_rnn: bool,
                            local_iteration_number: tuple) -&gt; torch.Tensor:
    if client_state[&#34;ck&#34;] == 1:
        fApprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
        grad_cur = mutils.get_gradient(model)
        client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += grad_cur.numel()
        return fApprox, grad_cur
    else:
        client_id = client_state[&#34;client_id&#34;]
        fApprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
        grad_cur = mutils.get_gradient(model)

        reconstruct_params = mutils.get_params(model)
        mutils.set_params(model, client_state[&#34;H&#34;][&#34;x_prev&#34;])
        evaluateSgd(client_state, model, dataloader, criterion, is_rnn)
        grad_prev = mutils.get_gradient(model)
        mutils.set_params(model, reconstruct_params)

        g_prev = client_state[&#34;H&#34;][&#34;g_prev&#34;].to(device = client_state[&#34;device&#34;], dtype = model.fl_dtype)
        g_next = g_prev + client_state[&#34;client_compressor&#34;].compressVector(grad_cur - grad_prev)

        # Comments: server knows g_prev
        client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance
        return fApprox, g_next</code></pre>
</details>
</dd>
<dt id="utils.algorithms.MarinaAlgorithmPP.serverGlobalStateUpdate"><code class="name flex">
<span>def <span class="ident">serverGlobalStateUpdate</span></span>(<span>clients_responses: utils.buffer.Buffer, clients: dict, model: torch.nn.modules.module.Module, paramsPrev: torch.Tensor, grad_server: torch.Tensor, H: dict) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
    H[&#34;g_prev&#34;] = grad_server
    H[&#34;x_prev&#34;] = mutils.get_params(model)

    H[&#34;test_ber_rv&#34;] = H[&#39;execution_context&#39;].np_random.random()
    if H[&#34;test_ber_rv&#34;] &lt;= H[&#34;p&#34;]:
        H[&#34;ck&#34;] = 1
        H[&#34;request_use_full_list_of_clients&#34;] = True
    else:
        H[&#34;ck&#34;] = 0
        H[&#34;request_use_full_list_of_clients&#34;] = False

    return H</code></pre>
</details>
</dd>
<dt id="utils.algorithms.MarinaAlgorithmPP.serverGradient"><code class="name flex">
<span>def <span class="ident">serverGradient</span></span>(<span>clients_responses: utils.buffer.Buffer, clients: int, model: torch.nn.modules.module.Module, params_current: torch.Tensor, H: dict) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def serverGradient(clients_responses: utils.buffer.Buffer, clients: int, model: torch.nn.Module, params_current: torch.Tensor, H: dict) -&gt; torch.Tensor:
    clients_responses.waitForItem()
    obtained_model = clients_responses.get(0)
    wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
    gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
    gs = wi * gi
    w_total = wi

    for i in range(1, clients):
        clients_responses.waitForItem()
        client_model = clients_responses.get(i)
        gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

        w_total += wi
        gs += wi * gi
    gs = gs / w_total
    return gs</code></pre>
</details>
</dd>
<dt id="utils.algorithms.MarinaAlgorithmPP.theoreticalStepSize"><code class="name flex">
<span>def <span class="ident">theoreticalStepSize</span></span>(<span>x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def theoreticalStepSize(x_cur, grad_server, H, clients_in_round, clients_responses, use_steps_size_for_non_convex_case):
    # Step size for non-convex case
    m = 1.0
    workers_per_round = clients_in_round
    workers = H[&#39;total_clients&#39;]
    Li_all_clients = getLismoothForClients(H, clients_responses)

    # Ltask = (np.mean( (Li_all_clients) **2) )**0.5
    Ltask = (np.mean( max(Li_all_clients) **2) )**0.5 # Maybe hack by /2
    w = H[&#34;w&#34;]
    p = (workers_per_round/workers)*1.0/(1+w)            # For RAND-K compressor
    r = workers_per_round

    step_1 = ((1 + 4*(1-p)*(1+w)/(p*workers))**0.5 - 1) / (2* (1-p) * (1+w)/(p*workers) * Ltask)
    step_2 = (-(1 + 4*(1-p)*(1+w)/(p*workers))**0.5 - 1) / (2* (1-p) * (1+w)/(p*workers) * Ltask)
    step_3 = 1.0 / (Ltask * (1 + ((1 - p) * (1 + w) / (p * workers_per_round)) ** 0.5))  # Theorem 4.1, p.37

    return step_3</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="utils.algorithms.SCAFFOLD"><code class="flex name class">
<span>class <span class="ident">SCAFFOLD</span></span>
</code></dt>
<dd>
<div class="desc"><p>SCAFFOLD Algoritm [Karimireddy et al., 2020]: <a href="https://arxiv.org/abs/1910.06378">https://arxiv.org/abs/1910.06378</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SCAFFOLD:
    &#39;&#39;&#39;
    SCAFFOLD Algoritm [Karimireddy et al., 2020]: https://arxiv.org/abs/1910.06378
    &#39;&#39;&#39;
    @staticmethod
    def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D:int, total_clients:int, grad_start:torch.Tensor) -&gt; dict:
        state = {&#34;c&#34;  : torch.zeros(D).to(device = args.device, dtype = model.fl_dtype),
                 &#34;c0&#34; : torch.zeros(D).to(device = args.device, dtype = model.fl_dtype)}

        return state

    @staticmethod
    def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
        # Compressors are not part of SCAFFOLD
        compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
        compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)
        last_ci = findRecentRecordAndRemoveFromHistory(H, clientId, &#39;ci&#39;)

        if last_ci is None:
            return {&#34;ci&#34; : H[&#39;c0&#39;].detach().clone().to(device = device, dtype = H[&#34;fl_dtype&#34;]),
                    &#34;client_compressor&#34;: compressor}
        else:
            return {&#34;ci&#34; : last_ci.to(device = device, dtype = H[&#34;fl_dtype&#34;]),                  #last_ci.detach().clone().to(device, dtype = H[&#34;fl_dtype&#34;]),
                    &#34;client_compressor&#34;: compressor}

    @staticmethod
    def localGradientEvaluation(client_state: dict,
                                model: torch.nn.Module,
                                dataloader: torch.utils.data.dataloader.DataLoader,
                                criterion: torch.nn.modules.loss._Loss,
                                is_rnn: bool,
                                local_iteration_number: tuple) -&gt; torch.Tensor:
        c  = client_state[&#34;H&#34;][&#39;c&#39;].to(device = client_state[&#34;device&#34;], dtype = model.fl_dtype)
        ci = client_state[&#39;ci&#39;]

        if local_iteration_number[0] == 0:
            evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = False)
            c_plus = mutils.get_gradient(model)
            client_state[&#39;delta_c&#39;] = client_state[&#34;client_compressor&#34;].compressVector(c_plus - c)

            # send delta_c and delta_x for model which has the same dimension
            client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#39;delta_c&#39;].numel() # send change iterates
            client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance

        fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
        grad_cur = mutils.get_gradient(model)
        dy = grad_cur - ci + c

        return fAprox, dy

    @staticmethod
    def serverGradient(clients_responses: utils.buffer.Buffer,
                       clients: int,
                       model: torch.nn.Module,
                       params_current: torch.Tensor,
                       H: dict) -&gt; torch.Tensor:
        clients_responses.waitForItem()
        obtained_model = clients_responses.get(0)
        wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
        gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        gs = wi * gi
        w_total = wi

        for i in range(1, clients):
            clients_responses.waitForItem()
            client_model = clients_responses.get(i)
            gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
            wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

            w_total += wi
            gs += wi * gi
        gs = gs / w_total

        return gs

    @staticmethod
    def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
        # x is updates as a part of general logic
        # here we will update c = c + sum(dc) * |S|/N

        obtained_model = clients_responses.get(0)
        dc = obtained_model[&#39;client_state&#39;][&#39;delta_c&#39;].to(device = paramsPrev.device, dtype = H[&#34;fl_dtype&#34;])
        clients_num_in_round = len(clients_responses)

        for i in range(1, clients_num_in_round):
            client_model = clients_responses.get(i)
            dc += client_model[&#39;client_state&#39;][&#39;delta_c&#39;].to(device = paramsPrev.device, dtype = H[&#34;fl_dtype&#34;])

        # Make dc is average of detla_c
        dc = dc / clients_num_in_round

        # Construct final delta step for update &#34;c&#34;
        dc = dc * float(clients_num_in_round)/float(H[&#34;total_clients&#34;])
        H[&#34;c&#34;] += dc
        return H</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="utils.algorithms.SCAFFOLD.clientState"><code class="name flex">
<span>def <span class="ident">clientState</span></span>(<span>H: dict, clientId: int, client_data_samples: int, device: str) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def clientState(H: dict, clientId: int, client_data_samples:int, device:str) -&gt; dict:
    # Compressors are not part of SCAFFOLD
    compressor = compressors.initCompressor(H[&#34;client_compressor&#34;], H[&#34;D&#34;])
    compressor.generateCompressPattern(H[&#39;execution_context&#39;].np_random, device, clientId, H)
    last_ci = findRecentRecordAndRemoveFromHistory(H, clientId, &#39;ci&#39;)

    if last_ci is None:
        return {&#34;ci&#34; : H[&#39;c0&#39;].detach().clone().to(device = device, dtype = H[&#34;fl_dtype&#34;]),
                &#34;client_compressor&#34;: compressor}
    else:
        return {&#34;ci&#34; : last_ci.to(device = device, dtype = H[&#34;fl_dtype&#34;]),                  #last_ci.detach().clone().to(device, dtype = H[&#34;fl_dtype&#34;]),
                &#34;client_compressor&#34;: compressor}</code></pre>
</details>
</dd>
<dt id="utils.algorithms.SCAFFOLD.initializeServerState"><code class="name flex">
<span>def <span class="ident">initializeServerState</span></span>(<span>args: argparse.Namespace, model: torch.nn.modules.module.Module, D: int, total_clients: int, grad_start: torch.Tensor) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def initializeServerState(args: argparse.Namespace, model: torch.nn.Module, D:int, total_clients:int, grad_start:torch.Tensor) -&gt; dict:
    state = {&#34;c&#34;  : torch.zeros(D).to(device = args.device, dtype = model.fl_dtype),
             &#34;c0&#34; : torch.zeros(D).to(device = args.device, dtype = model.fl_dtype)}

    return state</code></pre>
</details>
</dd>
<dt id="utils.algorithms.SCAFFOLD.localGradientEvaluation"><code class="name flex">
<span>def <span class="ident">localGradientEvaluation</span></span>(<span>client_state: dict, model: torch.nn.modules.module.Module, dataloader: torch.utils.data.dataloader.DataLoader, criterion: torch.nn.modules.loss._Loss, is_rnn: bool, local_iteration_number: tuple) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def localGradientEvaluation(client_state: dict,
                            model: torch.nn.Module,
                            dataloader: torch.utils.data.dataloader.DataLoader,
                            criterion: torch.nn.modules.loss._Loss,
                            is_rnn: bool,
                            local_iteration_number: tuple) -&gt; torch.Tensor:
    c  = client_state[&#34;H&#34;][&#39;c&#39;].to(device = client_state[&#34;device&#34;], dtype = model.fl_dtype)
    ci = client_state[&#39;ci&#39;]

    if local_iteration_number[0] == 0:
        evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = False)
        c_plus = mutils.get_gradient(model)
        client_state[&#39;delta_c&#39;] = client_state[&#34;client_compressor&#34;].compressVector(c_plus - c)

        # send delta_c and delta_x for model which has the same dimension
        client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#39;delta_c&#39;].numel() # send change iterates
        client_state[&#39;stats&#39;][&#39;send_scalars_to_master&#39;] += client_state[&#34;client_compressor&#34;].last_need_to_send_advance

    fAprox = evaluateSgd(client_state, model, dataloader, criterion, is_rnn, evaluate_function = True)
    grad_cur = mutils.get_gradient(model)
    dy = grad_cur - ci + c

    return fAprox, dy</code></pre>
</details>
</dd>
<dt id="utils.algorithms.SCAFFOLD.serverGlobalStateUpdate"><code class="name flex">
<span>def <span class="ident">serverGlobalStateUpdate</span></span>(<span>clients_responses: utils.buffer.Buffer, clients: dict, model: torch.nn.modules.module.Module, paramsPrev: torch.Tensor, grad_server: torch.Tensor, H: dict) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def serverGlobalStateUpdate(clients_responses:utils.buffer.Buffer, clients:dict, model:torch.nn.Module, paramsPrev:torch.Tensor, grad_server:torch.Tensor, H:dict)-&gt;dict:
    # x is updates as a part of general logic
    # here we will update c = c + sum(dc) * |S|/N

    obtained_model = clients_responses.get(0)
    dc = obtained_model[&#39;client_state&#39;][&#39;delta_c&#39;].to(device = paramsPrev.device, dtype = H[&#34;fl_dtype&#34;])
    clients_num_in_round = len(clients_responses)

    for i in range(1, clients_num_in_round):
        client_model = clients_responses.get(i)
        dc += client_model[&#39;client_state&#39;][&#39;delta_c&#39;].to(device = paramsPrev.device, dtype = H[&#34;fl_dtype&#34;])

    # Make dc is average of detla_c
    dc = dc / clients_num_in_round

    # Construct final delta step for update &#34;c&#34;
    dc = dc * float(clients_num_in_round)/float(H[&#34;total_clients&#34;])
    H[&#34;c&#34;] += dc
    return H</code></pre>
</details>
</dd>
<dt id="utils.algorithms.SCAFFOLD.serverGradient"><code class="name flex">
<span>def <span class="ident">serverGradient</span></span>(<span>clients_responses: utils.buffer.Buffer, clients: int, model: torch.nn.modules.module.Module, params_current: torch.Tensor, H: dict) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def serverGradient(clients_responses: utils.buffer.Buffer,
                   clients: int,
                   model: torch.nn.Module,
                   params_current: torch.Tensor,
                   H: dict) -&gt; torch.Tensor:
    clients_responses.waitForItem()
    obtained_model = clients_responses.get(0)
    wi = obtained_model[&#39;client_state&#39;][&#39;weight&#39;]
    gi = params_current - obtained_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
    gs = wi * gi
    w_total = wi

    for i in range(1, clients):
        clients_responses.waitForItem()
        client_model = clients_responses.get(i)
        gi = params_current - client_model[&#34;model&#34;].to(device = params_current.device, dtype = H[&#34;fl_dtype&#34;])
        wi = client_model[&#39;client_state&#39;][&#39;weight&#39;]

        w_total += wi
        gs += wi * gi
    gs = gs / w_total

    return gs</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="utils.algorithms.clientState" href="#utils.algorithms.clientState">clientState</a></code></li>
<li><code><a title="utils.algorithms.evaluateFunction" href="#utils.algorithms.evaluateFunction">evaluateFunction</a></code></li>
<li><code><a title="utils.algorithms.evaluateGradient" href="#utils.algorithms.evaluateGradient">evaluateGradient</a></code></li>
<li><code><a title="utils.algorithms.evaluateSgd" href="#utils.algorithms.evaluateSgd">evaluateSgd</a></code></li>
<li><code><a title="utils.algorithms.findRecentRecord" href="#utils.algorithms.findRecentRecord">findRecentRecord</a></code></li>
<li><code><a title="utils.algorithms.findRecentRecordAndRemoveFromHistory" href="#utils.algorithms.findRecentRecordAndRemoveFromHistory">findRecentRecordAndRemoveFromHistory</a></code></li>
<li><code><a title="utils.algorithms.getAlgorithmsList" href="#utils.algorithms.getAlgorithmsList">getAlgorithmsList</a></code></li>
<li><code><a title="utils.algorithms.getImplClassForAlgo" href="#utils.algorithms.getImplClassForAlgo">getImplClassForAlgo</a></code></li>
<li><code><a title="utils.algorithms.getLismoothForClients" href="#utils.algorithms.getLismoothForClients">getLismoothForClients</a></code></li>
<li><code><a title="utils.algorithms.getLsmoothGlobal" href="#utils.algorithms.getLsmoothGlobal">getLsmoothGlobal</a></code></li>
<li><code><a title="utils.algorithms.get_experiment_option_f" href="#utils.algorithms.get_experiment_option_f">get_experiment_option_f</a></code></li>
<li><code><a title="utils.algorithms.get_experiment_option_int" href="#utils.algorithms.get_experiment_option_int">get_experiment_option_int</a></code></li>
<li><code><a title="utils.algorithms.get_experiment_option_str" href="#utils.algorithms.get_experiment_option_str">get_experiment_option_str</a></code></li>
<li><code><a title="utils.algorithms.get_initial_shift" href="#utils.algorithms.get_initial_shift">get_initial_shift</a></code></li>
<li><code><a title="utils.algorithms.get_logger" href="#utils.algorithms.get_logger">get_logger</a></code></li>
<li><code><a title="utils.algorithms.has_experiment_option" href="#utils.algorithms.has_experiment_option">has_experiment_option</a></code></li>
<li><code><a title="utils.algorithms.initializeServerState" href="#utils.algorithms.initializeServerState">initializeServerState</a></code></li>
<li><code><a title="utils.algorithms.localGradientEvaluation" href="#utils.algorithms.localGradientEvaluation">localGradientEvaluation</a></code></li>
<li><code><a title="utils.algorithms.serverGlobalStateUpdate" href="#utils.algorithms.serverGlobalStateUpdate">serverGlobalStateUpdate</a></code></li>
<li><code><a title="utils.algorithms.serverGradient" href="#utils.algorithms.serverGradient">serverGradient</a></code></li>
<li><code><a title="utils.algorithms.theoreticalStepSize" href="#utils.algorithms.theoreticalStepSize">theoreticalStepSize</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="utils.algorithms.COFIG" href="#utils.algorithms.COFIG">COFIG</a></code></h4>
<ul class="">
<li><code><a title="utils.algorithms.COFIG.clientState" href="#utils.algorithms.COFIG.clientState">clientState</a></code></li>
<li><code><a title="utils.algorithms.COFIG.initializeServerState" href="#utils.algorithms.COFIG.initializeServerState">initializeServerState</a></code></li>
<li><code><a title="utils.algorithms.COFIG.localGradientEvaluation" href="#utils.algorithms.COFIG.localGradientEvaluation">localGradientEvaluation</a></code></li>
<li><code><a title="utils.algorithms.COFIG.serverGlobalStateUpdate" href="#utils.algorithms.COFIG.serverGlobalStateUpdate">serverGlobalStateUpdate</a></code></li>
<li><code><a title="utils.algorithms.COFIG.serverGradient" href="#utils.algorithms.COFIG.serverGradient">serverGradient</a></code></li>
<li><code><a title="utils.algorithms.COFIG.theoreticalStepSize" href="#utils.algorithms.COFIG.theoreticalStepSize">theoreticalStepSize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="utils.algorithms.DCGD" href="#utils.algorithms.DCGD">DCGD</a></code></h4>
<ul class="">
<li><code><a title="utils.algorithms.DCGD.clientState" href="#utils.algorithms.DCGD.clientState">clientState</a></code></li>
<li><code><a title="utils.algorithms.DCGD.initializeServerState" href="#utils.algorithms.DCGD.initializeServerState">initializeServerState</a></code></li>
<li><code><a title="utils.algorithms.DCGD.localGradientEvaluation" href="#utils.algorithms.DCGD.localGradientEvaluation">localGradientEvaluation</a></code></li>
<li><code><a title="utils.algorithms.DCGD.serverGlobalStateUpdate" href="#utils.algorithms.DCGD.serverGlobalStateUpdate">serverGlobalStateUpdate</a></code></li>
<li><code><a title="utils.algorithms.DCGD.serverGradient" href="#utils.algorithms.DCGD.serverGradient">serverGradient</a></code></li>
<li><code><a title="utils.algorithms.DCGD.theoreticalStepSize" href="#utils.algorithms.DCGD.theoreticalStepSize">theoreticalStepSize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="utils.algorithms.DIANA" href="#utils.algorithms.DIANA">DIANA</a></code></h4>
<ul class="">
<li><code><a title="utils.algorithms.DIANA.clientState" href="#utils.algorithms.DIANA.clientState">clientState</a></code></li>
<li><code><a title="utils.algorithms.DIANA.initializeServerState" href="#utils.algorithms.DIANA.initializeServerState">initializeServerState</a></code></li>
<li><code><a title="utils.algorithms.DIANA.localGradientEvaluation" href="#utils.algorithms.DIANA.localGradientEvaluation">localGradientEvaluation</a></code></li>
<li><code><a title="utils.algorithms.DIANA.serverGlobalStateUpdate" href="#utils.algorithms.DIANA.serverGlobalStateUpdate">serverGlobalStateUpdate</a></code></li>
<li><code><a title="utils.algorithms.DIANA.serverGradient" href="#utils.algorithms.DIANA.serverGradient">serverGradient</a></code></li>
<li><code><a title="utils.algorithms.DIANA.theoreticalStepSize" href="#utils.algorithms.DIANA.theoreticalStepSize">theoreticalStepSize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="utils.algorithms.EF21" href="#utils.algorithms.EF21">EF21</a></code></h4>
<ul class="">
<li><code><a title="utils.algorithms.EF21.clientState" href="#utils.algorithms.EF21.clientState">clientState</a></code></li>
<li><code><a title="utils.algorithms.EF21.initializeServerState" href="#utils.algorithms.EF21.initializeServerState">initializeServerState</a></code></li>
<li><code><a title="utils.algorithms.EF21.localGradientEvaluation" href="#utils.algorithms.EF21.localGradientEvaluation">localGradientEvaluation</a></code></li>
<li><code><a title="utils.algorithms.EF21.serverGlobalStateUpdate" href="#utils.algorithms.EF21.serverGlobalStateUpdate">serverGlobalStateUpdate</a></code></li>
<li><code><a title="utils.algorithms.EF21.serverGradient" href="#utils.algorithms.EF21.serverGradient">serverGradient</a></code></li>
<li><code><a title="utils.algorithms.EF21.theoreticalStepSize" href="#utils.algorithms.EF21.theoreticalStepSize">theoreticalStepSize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="utils.algorithms.EF21PP" href="#utils.algorithms.EF21PP">EF21PP</a></code></h4>
<ul class="">
<li><code><a title="utils.algorithms.EF21PP.clientState" href="#utils.algorithms.EF21PP.clientState">clientState</a></code></li>
<li><code><a title="utils.algorithms.EF21PP.initializeServerState" href="#utils.algorithms.EF21PP.initializeServerState">initializeServerState</a></code></li>
<li><code><a title="utils.algorithms.EF21PP.localGradientEvaluation" href="#utils.algorithms.EF21PP.localGradientEvaluation">localGradientEvaluation</a></code></li>
<li><code><a title="utils.algorithms.EF21PP.serverGlobalStateUpdate" href="#utils.algorithms.EF21PP.serverGlobalStateUpdate">serverGlobalStateUpdate</a></code></li>
<li><code><a title="utils.algorithms.EF21PP.serverGradient" href="#utils.algorithms.EF21PP.serverGradient">serverGradient</a></code></li>
<li><code><a title="utils.algorithms.EF21PP.theoreticalStepSize" href="#utils.algorithms.EF21PP.theoreticalStepSize">theoreticalStepSize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="utils.algorithms.FRECON" href="#utils.algorithms.FRECON">FRECON</a></code></h4>
<ul class="">
<li><code><a title="utils.algorithms.FRECON.clientState" href="#utils.algorithms.FRECON.clientState">clientState</a></code></li>
<li><code><a title="utils.algorithms.FRECON.initializeServerState" href="#utils.algorithms.FRECON.initializeServerState">initializeServerState</a></code></li>
<li><code><a title="utils.algorithms.FRECON.localGradientEvaluation" href="#utils.algorithms.FRECON.localGradientEvaluation">localGradientEvaluation</a></code></li>
<li><code><a title="utils.algorithms.FRECON.serverGlobalStateUpdate" href="#utils.algorithms.FRECON.serverGlobalStateUpdate">serverGlobalStateUpdate</a></code></li>
<li><code><a title="utils.algorithms.FRECON.serverGradient" href="#utils.algorithms.FRECON.serverGradient">serverGradient</a></code></li>
<li><code><a title="utils.algorithms.FRECON.theoreticalStepSize" href="#utils.algorithms.FRECON.theoreticalStepSize">theoreticalStepSize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="utils.algorithms.FedAvg" href="#utils.algorithms.FedAvg">FedAvg</a></code></h4>
<ul class="">
<li><code><a title="utils.algorithms.FedAvg.clientState" href="#utils.algorithms.FedAvg.clientState">clientState</a></code></li>
<li><code><a title="utils.algorithms.FedAvg.initializeServerState" href="#utils.algorithms.FedAvg.initializeServerState">initializeServerState</a></code></li>
<li><code><a title="utils.algorithms.FedAvg.localGradientEvaluation" href="#utils.algorithms.FedAvg.localGradientEvaluation">localGradientEvaluation</a></code></li>
<li><code><a title="utils.algorithms.FedAvg.serverGlobalStateUpdate" href="#utils.algorithms.FedAvg.serverGlobalStateUpdate">serverGlobalStateUpdate</a></code></li>
<li><code><a title="utils.algorithms.FedAvg.serverGradient" href="#utils.algorithms.FedAvg.serverGradient">serverGradient</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="utils.algorithms.FedProx" href="#utils.algorithms.FedProx">FedProx</a></code></h4>
<ul class="">
<li><code><a title="utils.algorithms.FedProx.clientState" href="#utils.algorithms.FedProx.clientState">clientState</a></code></li>
<li><code><a title="utils.algorithms.FedProx.initializeServerState" href="#utils.algorithms.FedProx.initializeServerState">initializeServerState</a></code></li>
<li><code><a title="utils.algorithms.FedProx.localGradientEvaluation" href="#utils.algorithms.FedProx.localGradientEvaluation">localGradientEvaluation</a></code></li>
<li><code><a title="utils.algorithms.FedProx.serverGlobalStateUpdate" href="#utils.algorithms.FedProx.serverGlobalStateUpdate">serverGlobalStateUpdate</a></code></li>
<li><code><a title="utils.algorithms.FedProx.serverGradient" href="#utils.algorithms.FedProx.serverGradient">serverGradient</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="utils.algorithms.MarinaAlgorithm" href="#utils.algorithms.MarinaAlgorithm">MarinaAlgorithm</a></code></h4>
<ul class="">
<li><code><a title="utils.algorithms.MarinaAlgorithm.clientState" href="#utils.algorithms.MarinaAlgorithm.clientState">clientState</a></code></li>
<li><code><a title="utils.algorithms.MarinaAlgorithm.initializeServerState" href="#utils.algorithms.MarinaAlgorithm.initializeServerState">initializeServerState</a></code></li>
<li><code><a title="utils.algorithms.MarinaAlgorithm.localGradientEvaluation" href="#utils.algorithms.MarinaAlgorithm.localGradientEvaluation">localGradientEvaluation</a></code></li>
<li><code><a title="utils.algorithms.MarinaAlgorithm.serverGlobalStateUpdate" href="#utils.algorithms.MarinaAlgorithm.serverGlobalStateUpdate">serverGlobalStateUpdate</a></code></li>
<li><code><a title="utils.algorithms.MarinaAlgorithm.serverGradient" href="#utils.algorithms.MarinaAlgorithm.serverGradient">serverGradient</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="utils.algorithms.MarinaAlgorithmPP" href="#utils.algorithms.MarinaAlgorithmPP">MarinaAlgorithmPP</a></code></h4>
<ul class="">
<li><code><a title="utils.algorithms.MarinaAlgorithmPP.algorithmDescription" href="#utils.algorithms.MarinaAlgorithmPP.algorithmDescription">algorithmDescription</a></code></li>
<li><code><a title="utils.algorithms.MarinaAlgorithmPP.clientState" href="#utils.algorithms.MarinaAlgorithmPP.clientState">clientState</a></code></li>
<li><code><a title="utils.algorithms.MarinaAlgorithmPP.initializeServerState" href="#utils.algorithms.MarinaAlgorithmPP.initializeServerState">initializeServerState</a></code></li>
<li><code><a title="utils.algorithms.MarinaAlgorithmPP.localGradientEvaluation" href="#utils.algorithms.MarinaAlgorithmPP.localGradientEvaluation">localGradientEvaluation</a></code></li>
<li><code><a title="utils.algorithms.MarinaAlgorithmPP.serverGlobalStateUpdate" href="#utils.algorithms.MarinaAlgorithmPP.serverGlobalStateUpdate">serverGlobalStateUpdate</a></code></li>
<li><code><a title="utils.algorithms.MarinaAlgorithmPP.serverGradient" href="#utils.algorithms.MarinaAlgorithmPP.serverGradient">serverGradient</a></code></li>
<li><code><a title="utils.algorithms.MarinaAlgorithmPP.theoreticalStepSize" href="#utils.algorithms.MarinaAlgorithmPP.theoreticalStepSize">theoreticalStepSize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="utils.algorithms.SCAFFOLD" href="#utils.algorithms.SCAFFOLD">SCAFFOLD</a></code></h4>
<ul class="">
<li><code><a title="utils.algorithms.SCAFFOLD.clientState" href="#utils.algorithms.SCAFFOLD.clientState">clientState</a></code></li>
<li><code><a title="utils.algorithms.SCAFFOLD.initializeServerState" href="#utils.algorithms.SCAFFOLD.initializeServerState">initializeServerState</a></code></li>
<li><code><a title="utils.algorithms.SCAFFOLD.localGradientEvaluation" href="#utils.algorithms.SCAFFOLD.localGradientEvaluation">localGradientEvaluation</a></code></li>
<li><code><a title="utils.algorithms.SCAFFOLD.serverGlobalStateUpdate" href="#utils.algorithms.SCAFFOLD.serverGlobalStateUpdate">serverGlobalStateUpdate</a></code></li>
<li><code><a title="utils.algorithms.SCAFFOLD.serverGradient" href="#utils.algorithms.SCAFFOLD.serverGradient">serverGradient</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
FL_PyTorch. The document generated 09-July-2022 23:03:20.
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>